[{"comment_by": "Frankie ***", "comment_text": "Hideously relevant rn", "comment_date": "2020-05-19T01:43:34Z", "likes_count": 7}, {"comment_by": "byongcheol Ko", "comment_text": "<a href=\"https://www.youtube.com/watch?v=gV0_raKR2UQ&amp;t=2m25s\">2:25</a> is there anybody knows the title of article?", "comment_date": "2020-04-09T03:51:12Z", "likes_count": 0}, {"comment_by": "StarCoinHero", "comment_text": "<a href=\"https://www.youtube.com/watch?v=gV0_raKR2UQ&amp;t=6m52s\">6:52</a> This is why you don\u2019t leave John Green Bot alone online. John Green Bot just saying the n word every 5 seconds.", "comment_date": "2020-02-20T16:45:57Z", "likes_count": 3}, {"comment_by": "H Lam", "comment_text": "Algorithms like when YouTube give some video an \u274c to demonetized the video when it is talking about the truth.", "comment_date": "2020-02-14T14:01:17Z", "likes_count": 2}, {"comment_by": "Mark_till Till", "comment_text": "AI has the power to destroy peoples lives. It has no conscience.", "comment_date": "2020-02-13T23:20:15Z", "likes_count": 0}, {"comment_by": "Athena Caesura", "comment_text": "Love the nonbinary inclusion remark. PBS on the whole isn&#39;t great with gender inclusive language and I didn&#39;t expect crash course AI to be the pioneer, but I sure appreciate it.", "comment_date": "2019-12-31T00:03:25Z", "likes_count": 10}, {"comment_by": "Saul Galloway", "comment_text": "6 agreements and disagreements.<br><br>1. Nurses are 90% female. Programmers are 80% male. Of course you&#39;re going to have far more images on average of the dominate sex in those fields. But, sure. Get it to say THEY.<br><br>2. The only value understanding gender has is significant behavioral predictions. Algorithm doesn&#39;t care about your social Yugioh game to feel special. It&#39;s tackling reality.<br><br>3. Lack of data on the racial bit. For sure we need greater data samples there.<br><br>4. We&#39;re gonna&#39; ignore uncomfortable crime stats? Ok.<br><br>5. Yes. The kids who are shown to do well often are at a much lesser risk of becoming shitty. Reality sure is complicated.<br><br>6. Yes. You can&#39;t discriminate when it comes to loans and jobs. Even if there&#39;s a significant racial, sex, whatever difference. Things can&#39;t change for the better if you force them out and skillful/valuable individuals that aren&#39;t part of the problem within&#39; these groups would suffer.", "comment_date": "2019-12-28T02:31:05Z", "likes_count": 5}, {"comment_by": "Ramon Luque", "comment_text": "Trash,  algorithms tell facts, they ones who are biassed are people with that &quot;equlity&quot; ideology. Races are not equal, peopple from defferent ages have different midnsets, there is only men and women and they also have different nature. The AI shows it by its results.", "comment_date": "2019-12-24T23:21:42Z", "likes_count": 0}, {"comment_by": "Mr. Wallet", "comment_text": "OK, I was kind of dreading this one because I expected a bunch of woke drivel - but I gotta be honest, you folks pretty much nailed it. This was informative, and probably as even-handed as Crash Course has <i>ever been</i> on such a sensitive topic. I am impressed.", "comment_date": "2019-12-18T07:30:49Z", "likes_count": 4}, {"comment_by": "D Murphy", "comment_text": "Many people are missing the point to the Google analogy.   AI hiring systems will learn associated characteristics of a nurse or programmer or what have you from similar datasets.  That&#39;s not so much the problem- it&#39;s what happens next.  It discriminates against people who don&#39;t meet the average characteristics.  The AI system may throw out a resume for a nursing position that has the words &quot;Boy Scout troop leader&quot; because that&#39;s not something associated with the average nurse.  It may throw out qualified programmer resumes from people who attended HBCUs, because most programmers haven&#39;t.  If you don&#39;t quite get this, please look up the scrapped Amazon AI hiring program.  It downgraded resumes from applicants who attended women&#39;s colleges.", "comment_date": "2019-12-18T01:48:26Z", "likes_count": 12}, {"comment_by": "gunsandcarsandstuff", "comment_text": "Should a program be faulted for showing mostly female nurses? 91% of nurses are female. Should it be faulted for recognizing more white people? The United States is 72% Caucasian. It seems silly that we try to tell computers lies, so that their results don\u2019t hurt anyone\u2019s feelers.", "comment_date": "2019-12-17T10:32:17Z", "likes_count": 1}, {"comment_by": "Shukla Maths Academy", "comment_text": "Very nice crash course.\ud83d\udc4d\ud83c\udffb", "comment_date": "2019-12-17T10:04:55Z", "likes_count": 0}, {"comment_by": "Hambone", "comment_text": "Please do a complete course on climate science!!", "comment_date": "2019-12-16T23:48:55Z", "likes_count": 1}, {"comment_by": "Karl Ramstedt", "comment_text": "<a href=\"https://www.youtube.com/watch?v=gV0_raKR2UQ&amp;t=3m28s\">3:28</a> omg, that&#39;s the jealous girlfriend from the stock-photo meme.", "comment_date": "2019-12-16T14:38:20Z", "likes_count": 15}, {"comment_by": "Cody Uhi", "comment_text": "Are there really deep learning models that implement a person&#39;s name as a factor to extrapolate their personality traits or compatibility for a job?  Are there any studies that show that a person&#39;s given name has a significant correlation to their personality?", "comment_date": "2019-12-16T13:12:23Z", "likes_count": 0}, {"comment_by": "\u0141i\u0142 B\u012b\u0142\u0142", "comment_text": "HOW IS HIS HAT THAT BIG?!?!", "comment_date": "2019-12-16T05:32:18Z", "likes_count": 0}, {"comment_by": "Susan Maddison", "comment_text": "A fundamentally dishonest video.  No mention of the ideological bias that is almost unanimous in silicon valley, obviously is going to infect the algorithms. and is the prima facie cause of the discrimination so many conservatives have noticed in social media and on youtube right here. No mention of the obstinate denials of this obvious reality by the tech companies, rather than trying seriously to deal with it by hiring enough non-leftists who would be able to recognize it and help them police it. No mention of the refusal of these companies and their personnel to acknowledge their own bias, the first step toward policing. No mention of their firing of people who point out the problem. The failure here to mention this, on a supposedly scientific video here, is itself a confirmation of how serious the problem is.", "comment_date": "2019-12-15T02:55:04Z", "likes_count": 0}, {"comment_by": "Jibril Harris", "comment_text": "bruh my name is Jibril", "comment_date": "2019-12-15T02:07:40Z", "likes_count": 2}, {"comment_by": "Leen Jabri", "comment_text": "THIS IS FANTASTIC! YOU ARE ENCOURAGING ME TO KEEP WORKING ON MY CHANNEL!. <br>GUYS I WOULD APPRECIATE YOUR HELP IF YOU COME AND CHECKED OUT MY CHANNEL!!!.", "comment_date": "2019-12-15T01:57:54Z", "likes_count": 0}, {"comment_by": "Ruma Sen Gupta", "comment_text": "what does the term capitalism and laissez faire mean. Help i donot understand", "comment_date": "2019-12-15T01:47:25Z", "likes_count": 0}, {"comment_by": "Hetare King", "comment_text": "The point of the Google image search example isn&#39;t to accuse Google of some grave injustice, it&#39;s just an easy to understand example of how just because a computer is generating it doesn&#39;t mean its output isn&#39;t biased. The society it&#39;s getting its data from is biased in favour of female nurses, so it will return mostly pictures of female nurses even when the user is just looking for &quot;nurse&quot; without specifying gender. Once you understand that, it&#39;s easy to understand how that can become a problem when the situation is more complicated, the stakes are higher, which is the whole point of the episode.<br><br><br>Let&#39;s say there&#39;s 10 male nurses in the world and 90 female nurses. Out of those 100 nurses, one man and two women have committed the same misdemeanour on the job. Given that, would it be fair to make decisions on who to to employ as nurse based on the idea that 10% of men have committed this misdemeanour but only ~2% of women have? An AI trained with this data might. Worse yet, you don&#39;t even know it&#39;s doing this because its decision-making process is more or less a black box.", "comment_date": "2019-12-14T22:57:50Z", "likes_count": 3}, {"comment_by": "BatteryJuicy", "comment_text": "it&#39;s so weird to see Jabrils move his mouth!", "comment_date": "2019-12-14T19:39:53Z", "likes_count": 6}, {"comment_by": "Eli Estephan", "comment_text": "&quot;sexual orientation is strongly correlated with certain characteristics of a social media profile photo&quot;<br>which characteristics? how do i algorithmically optimize the gayness of my profile??", "comment_date": "2019-12-14T19:32:07Z", "likes_count": 12}, {"comment_by": "basilwhite", "comment_text": "Outstandingly clear and engaging.\ud83d\udc4d", "comment_date": "2019-12-14T18:06:31Z", "likes_count": 0}, {"comment_by": "Gravity", "comment_text": "Have you guys covered politics much? I know it can be touchy, but I&#39;d like someone in a good position to do so. To explain the issues with things like the party system and gerrymandering, and what you can legally do to change it instead of letting things go until the levy breaks.", "comment_date": "2019-12-14T17:44:15Z", "likes_count": 0}, {"comment_by": "Carson Cunningham", "comment_text": "Why his shirts always wrinkled?", "comment_date": "2019-12-14T15:01:34Z", "likes_count": 0}, {"comment_by": "artiphology", "comment_text": "&quot;Do you pledge the axiom?&quot; &quot;Only in my reality class&quot;<br>(got distracted reading the babel)", "comment_date": "2019-12-14T14:40:42Z", "likes_count": 3}, {"comment_by": "Reid Merrill", "comment_text": "<a href=\"https://www.youtube.com/watch?v=gV0_raKR2UQ&amp;t=1m30s\">1:30</a> <br>Not bias. Most nurses are women.", "comment_date": "2019-12-14T13:58:16Z", "likes_count": 5}, {"comment_by": "nantukoprime", "comment_text": "Did a short stint working on an algorithm that looked for potential pickpockets, trained on video of actual incidents that led to arrest.<br><br>Was moved to another project after I kept bringing up the fact that the algorithm was biased as the data set was generally representative of a subset of pickpockets, the ones who get caught. My request for video of successful pickpockets that were not arrested to train the algorithm was not viewed favorably.", "comment_date": "2019-12-14T13:56:00Z", "likes_count": 77}, {"comment_by": "Boomer & Zoomer Figure it out", "comment_text": "White progressives are biased against white people", "comment_date": "2019-12-14T12:30:34Z", "likes_count": 1}, {"comment_by": "TopCommenter", "comment_text": "Most nurses are female, and most programmers are male. So if you Google a nurse, you see a typical nurse (which is female). The example between minute one and two is not about our biases, its simply what is actually there.", "comment_date": "2019-12-14T12:02:03Z", "likes_count": 5}, {"comment_by": "l1222214 jnv111", "comment_text": "Good bye crash course. I loved your videos but go woke go broke", "comment_date": "2019-12-14T10:49:52Z", "likes_count": 2}, {"comment_by": "Enigmo", "comment_text": "\u201cNon-binary people doing both of these things\u201d<br>What would google images be showing to present this?", "comment_date": "2019-12-14T10:05:45Z", "likes_count": 5}, {"comment_by": "intboom", "comment_text": "Machine Learning creates Bias Machines; things capable of making a snap judgement based on minimal input. They&#39;re better at discovering patterns in empirical reality than humans are. If you don&#39;t like that, then just make an Expert System instead.", "comment_date": "2019-12-14T08:47:42Z", "likes_count": 7}, {"comment_by": "FuCkthEnSa", "comment_text": "Propaganda", "comment_date": "2019-12-14T08:29:50Z", "likes_count": 1}, {"comment_by": "TapIntoTheEssence", "comment_text": "During the debate that followed ProPublia&#39;s accusations of the COMPAS-algorithm being discriminatory against black people, Kleinberg, Mullainathan and Raghavan showed that there are inherent trade-offs between different notions of fairness.<br><br>In the case of COMPAS, for example, the algorithm was &quot;well-calobrated among groups&quot;, which means that, independent of skin colour, a group of people classified as, say, 70% to recidive, actually had 70% of people that would recidive.<br><br>However, ProPublia objected, that the algorithm produced more false positive predictions for blacks (meaning that blacks were labeled more often wrongly as high risk) and more false negative predictions for whites (meaning that whites were more often labeled wrongly as low risk).<br><br>In their paper, the authors showed that these notions of fairness, namely &quot;well balanced among groups&quot;, &quot;balance for the negative class&quot; and &quot;balance for the positive class&quot; are mathematically incompatible and exclude each other. One can&#39;t have the one and the other at the same time.<br><br>So yes, AI-systems will be biased, as insisted upon in the video. But it raises questions about what kind of fairness we want to be implemented and what we&#39;re willing to give up.", "comment_date": "2019-12-14T08:02:47Z", "likes_count": 38}, {"comment_by": "L3GITME", "comment_text": "These features are going to come to pass, the only hope is in salvation by faith through Jesus Christ, to obtain the Holy Spirit which will lead you to a relationship with God", "comment_date": "2019-12-14T05:21:51Z", "likes_count": 0}, {"comment_by": "Qilin", "comment_text": "An interesting paper published in the journal of Psychological Science in 2018 looked at cross-cultural differences between international databases of achievement in STEM programs, and found that the lower Gender Gap Index of a country, the more likely it is to have &quot;equal&quot; rate of women to men among STEM graduates of universities. That is to say in countries with high Global Gender Gap Index like Finland, Norway, and Sweden, they tend to have significantly lower rates of women graduates of STEM programs (~20-25%), whereas in some of the countries with the lowest Global Gender Gap Index like UAE, Turkey, Algeria, they have some of the highest rates of women graduates of STEM programs (~36-41%).<br><br>The paper is titled &quot;The Gender-Equality Paradox in Science, Technology, Engineering, and Mathematics Education&quot; DOI: 10.1177/0956797617741719<br><br>In the Nursing and Programmer example, you mention data reflecting hidden biases in society, and certainly there must be some hidden biases that influence this population distribution. But it would be apt to also note that bias can exist in the way the data is presented to. This bias is called &quot;Algorithmic Fairness&quot; and is used by google, and ties in with data manipulation mentioned in section 5, though arguably it&#39;s not &quot;malicious&quot;. <br><br>At its core, algorithmic fairness manipulates data to over-represent groups. There are examples of this that anybody can test out, where results on image searches produce nearly <a href=\"https://www.youtube.com/watch?v=gV0_raKR2UQ&amp;t=50m50s\">50:50</a> results between two subpopulations, despite their actual ratio in reality not being <a href=\"https://www.youtube.com/watch?v=gV0_raKR2UQ&amp;t=50m50s\">50:50</a>. This isn&#39;t malicious, but it could be harmful all the same. In an ideal world where there is true equality, we can pursue whatever career we want without worrying about the statistics of who makes up what job. And in the most &quot;equal&quot; societies, we find that there are fewer women in STEM. While using algorithmic fairness to show even pictures of men and women might make women in STEM feel better, it might also make women who don&#39;t wish to pursue STEM feel bad for not contributing to that equality. And this may sound silly, but we see the consequences of this in the &quot;STEAM&quot; movement where they try to include Arts into STEM to be more inclusive of women. <br><br>Ultimately this boils down into the problem of equality of outcome, versus equality of opportunity. We know that in the most equal societies, they have close to equality of opportunity but not enough equality of outcome. And we know that in the least equal society, they have close to equality of outcome, but nowhere near enough equality of opportunity. The key question, then, is &quot;Should algorithms reflect the actual data even if it&#39;s biased, or should algorithmic fairness be implemented to to makeup for biases hidden in society?&quot; because there are arguments for both sides. If we truly believe that more equal societies are better, I think there&#39;s merit in accepting disproportionate gender representation.", "comment_date": "2019-12-14T04:38:46Z", "likes_count": 8}, {"comment_by": "Christopher Nielsen", "comment_text": "More Jabril please!", "comment_date": "2019-12-14T04:23:17Z", "likes_count": 0}, {"comment_by": "Clare Kuehn", "comment_text": "And some are deliberately biased. This is how cultural manipulation through government black projects is done here: let private corporations censor unspecified classes and it&#39;s not illegal. Only governments can be called illegal for suppressing speech on line. Nice trick!", "comment_date": "2019-12-14T04:22:08Z", "likes_count": 3}, {"comment_by": "Ricky", "comment_text": "As a society, we live in a society.", "comment_date": "2019-12-14T03:49:12Z", "likes_count": 32}, {"comment_by": "wolflink9000", "comment_text": "Prioritizing resources to areas where statistically in the past there is more likely to be issues makes 100% perfect sense.", "comment_date": "2019-12-14T03:05:24Z", "likes_count": 1}, {"comment_by": "james staggs", "comment_text": "Yeah when you complain about the AI making things &quot;a little more difficult&quot; or &quot;frustrating&quot; then you&#39;ve really got nothing to complain about. So Google image shows pictures of nurses as women and programmers as men. More women are nurses and more programmers are men. Nobody is keeping anyone from being a programmer if they&#39;re female or being a nurse if they&#39;re male. I&#39;m sorry that&#39;s just a non-issue. We don&#39;t need to try and ensure that every single vocation has a perfect balance or race and/or gender. All we need to do is make sure that nobody is barred from any career path based only on their gender or race. Thinking like this should just be called &quot;too many straight white men over there&quot; because that seems to be the only group anybody is interested in making sure there aren&#39;t too many of in a given area. This just about is never applied to any other group.", "comment_date": "2019-12-14T02:59:06Z", "likes_count": 4}, {"comment_by": "ListerTunes", "comment_text": "I&#39;m reminded of the resume-screening AI that taught itself that the best candidates were named Trevor and played high school lacrosse. Biases in culture introduce biases into data, which just replicates the bias.", "comment_date": "2019-12-14T01:34:42Z", "likes_count": 13}, {"comment_by": "Hamstray", "comment_text": "&quot;Algorithms are\u00a0unambiguous\u00a0specifications for performing\u00a0calculation,\u00a0data processing,\u00a0automated reasoning, and other tasks.&quot;, AI (neural networks) are not unambiguous and don&#39;t qualify as algorithms. In neural networks biases may emerge spontaneously regardless of the training data.", "comment_date": "2019-12-14T01:25:57Z", "likes_count": 20}, {"comment_by": "Michael Hughes", "comment_text": "If you gave the algorithm more data for protected classes, wouldn&#39;t that just bias it towards them? It seems that any learning data would necessarily contain some kind of pre-selected bias to even make a choice.", "comment_date": "2019-12-14T01:14:58Z", "likes_count": 33}, {"comment_by": "Henry P", "comment_text": "Do you mean the way Google has biased towards conservatives?", "comment_date": "2019-12-14T00:29:02Z", "likes_count": 7}, {"comment_by": "Tribal 868", "comment_text": "Do a crash course music production please.", "comment_date": "2019-12-14T00:17:19Z", "likes_count": 2}, {"comment_by": "El Mahdi EL MHAMDI", "comment_text": "Hi Jabril, I&#39;m a machine learning (a.k.a AI) researcher with a PhD, and an upcoming book* in the field, I was super excited to see this series (as a long-time fan of CrashCourse from World History with John Green to Sociology with Nicole Sweeney ). Now I&#39;m even more excited to see you tackle this very very hard question in my field, AI. Unsurprisingly, I see a setback in the comments saying that this is becoming a social justice and not a science channel. So let me address some of these concerns.<br>1) Algorithmic fairness is a highly scientific, highly &quot;technical&quot; topic, involving the state of the art knowledge we have in statistics and computer science today.<br>2) Historically, the very word of &quot;algorithm&quot; originate from the 9th century book on &quot;algebra&quot;, by alkhawarizmi (latinised to algorithmi), which itself is a book on *justice*, written by a lawyer for other lawyers so that they can also solve complex inheritance cases. More than half of that book uses indistinguishably the concept of &quot;judgement&quot; and &quot;computation&quot; (Hissab: \u062d\u0633\u0627\u0628).<br>Just to say: the very birth of &quot;algorithms&quot; was about making better judgements. Today, as we are automating these judgements, we ought to come up with better <b>scientifically robust</b> notions of fairness, which is what a whole community of more specialised researchers than myself are doing.<br>3) Some of the reactions I saw in the comments are not uncommon even in discussions with top scientists (when they are from other specialities and are not aware of the impressive research being done in algorithmic fairness).<br><br>best of luck, and keep up the very good job you and the rest of the channel&#39;s team are doing.<br><br><br>*: (French version already available, English version due for June 2020)", "comment_date": "2019-12-13T23:57:55Z", "likes_count": 25}, {"comment_by": "Christopher Walsh", "comment_text": "&quot;bias is wrong&quot; says the most stereotypical looking black guy they could find", "comment_date": "2019-12-13T23:56:09Z", "likes_count": 6}, {"comment_by": "B K", "comment_text": "I guess you could make the whole second season of CC Ecology about all the stuff that lives under that hat.", "comment_date": "2019-12-13T23:52:44Z", "likes_count": 4}, {"comment_by": "Non Sense Speaks", "comment_text": "Non binary?<br>- me a programmer confused", "comment_date": "2019-12-13T23:51:12Z", "likes_count": 36}, {"comment_by": "Motorola Android", "comment_text": "I see the book Weapons of Math Destruction.", "comment_date": "2019-12-13T23:47:26Z", "likes_count": 6}, {"comment_by": "Caper. here", "comment_text": "Cambridge Analytics.", "comment_date": "2019-12-13T23:47:22Z", "likes_count": 8}, {"comment_by": "darkblood626", "comment_text": "Algorithm doesn&#39;t privilege not whites or women&#39; -leftists &quot;But that&#39;s not fair&quot;", "comment_date": "2019-12-13T23:08:45Z", "likes_count": 3}, {"comment_by": "Argacyan", "comment_text": "I&#39;ll imagine at least a couple of people will be upset to hear that things like what data is put into their algorithm will bias the outcome which is a very easy concept to grasp, like the chemicals in a reaction will narrow down what products you possibly get or how different fuels to a fire can impact the heat generated by the fire - like I could only imagine disagreement there if getting biased results while claiming there was none would be the intention to begin with...", "comment_date": "2019-12-13T23:07:45Z", "likes_count": 27}, {"comment_by": "Holger Kraus", "comment_text": "That&#39;s why all big companies that use AI should establish AI ethics boards - let&#39;s keep our AI fair and unprejudiced! :)", "comment_date": "2019-12-13T23:00:30Z", "likes_count": 6}, {"comment_by": "RedShipley", "comment_text": "I dont get people are upset when AI cant recognize their face. I&#39;d be thrilled to not be recognized. They wouldnt be able to use recognition software on me.", "comment_date": "2019-12-13T22:55:04Z", "likes_count": 8}, {"comment_by": "Led Vegan", "comment_text": "Omg, im like 3 min in and its already dumb. So a google search shows more women pictures than men for nurses, and more men than women for programs. What the ratio of pictures featuring women to men for nursing, and men to women for programing? Im guessing the same as Google images shows.", "comment_date": "2019-12-13T22:49:40Z", "likes_count": 18}, {"comment_by": "Oscar O. Rosas", "comment_text": "#315 here", "comment_date": "2019-12-13T22:47:33Z", "likes_count": 0}, {"comment_by": "TaxPayingContributor", "comment_text": "Gonna say it again:<br>(Al Gore Rhythms)", "comment_date": "2019-12-13T22:43:15Z", "likes_count": 5}, {"comment_by": "THE KING OF BAKERSFIELD", "comment_text": "Very informative,  great work.", "comment_date": "2019-12-13T22:42:51Z", "likes_count": 6}, {"comment_by": "RedShipley", "comment_text": "You say all those nurses were women... some could have been men or non binary. You just showing your bias in how women look.", "comment_date": "2019-12-13T22:41:03Z", "likes_count": 13}, {"comment_by": "Dr. Masi", "comment_text": "Hi good job take a look to my channel I make tutorial videos", "comment_date": "2019-12-13T22:35:21Z", "likes_count": 2}, {"comment_by": "Dankley", "comment_text": "Had to throw that &quot;and non binary people&quot; bit in there", "comment_date": "2019-12-13T22:35:11Z", "likes_count": 7}, {"comment_by": "Mikhail Semenchenko", "comment_text": "Wow, this video is newborn", "comment_date": "2019-12-13T22:35:07Z", "likes_count": 3}, {"comment_by": "Demoman", "comment_text": "noice video", "comment_date": "2019-12-13T22:33:33Z", "likes_count": 1}, {"comment_by": "Setarko", "comment_text": "Early squad here?", "comment_date": "2019-12-13T22:32:19Z", "likes_count": 4}]