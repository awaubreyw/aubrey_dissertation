[{"comment_by": "Shoha S", "comment_text": "wow, I am surprise anyone knows this sophisticated concepts to explain them clearly, and yet you also accompany them by amazing visuals which I believe not easier to do. Thank you!", "comment_date": "2022-08-22T15:49:13Z", "likes_count": 0}, {"comment_by": "Sarthak chauhan", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m21s\">4:21</a>", "comment_date": "2022-08-18T12:58:04Z", "likes_count": 0}, {"comment_by": "Trevin Small", "comment_text": "All I can say is thank you! Every 3b1b video that I have watched is simply incredible. The chronology of ideas introduced throughout a video makes complex topics easy to follow, and the animations are BEAUTIFUL. I have never been able to visualize math so well before. You are an amazing teacher and have contributed invaluable knowledge to society! I&#39;m still in shock that educational resources this profound are available for free. I truly appreciate the work you are doing!", "comment_date": "2022-08-14T00:52:24Z", "likes_count": 0}, {"comment_by": "Bhavik Patel", "comment_text": "Insert &quot;Society if everything and everyone was taught concepts this nicely&quot; meme.", "comment_date": "2022-08-10T06:01:33Z", "likes_count": 0}, {"comment_by": "\u043a\u0438\u0440\u0438\u043b\u043b \u043f\u0430\u043d\u043e\u0432\u0438\u0446\u0438\u043d", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m30s\">4:30</a> Why sum of probabilitieas don&#39;t equal 1?", "comment_date": "2022-08-02T13:56:07Z", "likes_count": 0}, {"comment_by": "Ron Spain", "comment_text": "But what are cheenjes?", "comment_date": "2022-07-29T19:43:35Z", "likes_count": 0}, {"comment_by": "jake is the coolest", "comment_text": "One tip: don&#39;t make your voice so calming, I fell asleep....", "comment_date": "2022-07-28T03:46:15Z", "likes_count": 0}, {"comment_by": "Lorenzo Battini", "comment_text": "I didn&#39;t get when i must choose learning using biases or by weights", "comment_date": "2022-07-23T18:18:55Z", "likes_count": 0}, {"comment_by": "Mahdi Amrollahi", "comment_text": "Hello, just have a question: in dnn, sometimes we have a complicated estimated function at the end including many multiplication and applying activation functions. How do we know that is differentiable or not? We just add layers and changing activation fn without knowing it.", "comment_date": "2022-07-23T16:11:24Z", "likes_count": 0}, {"comment_by": "Pilotamericano", "comment_text": "thanks for reminding in S3 E4 that I might have not watched the previous episode :D", "comment_date": "2022-07-20T20:39:07Z", "likes_count": 0}, {"comment_by": "Luciano Ropero", "comment_text": "Math \u2764\ufe0f", "comment_date": "2022-07-18T11:26:19Z", "likes_count": 0}, {"comment_by": "Sahana Sachindra Mallya", "comment_text": "Very very bad . Poverty will come with this tech because this is evil", "comment_date": "2022-07-15T20:00:32Z", "likes_count": 0}, {"comment_by": "Luke Turton", "comment_text": "I&#39;m an MSc student studying Maths w/ Data Science (I won&#39;t say where), and I&#39;m currently working on a CNN project for my final submission. I&#39;m finding your videos much more informative and understandable than any of my lecturer&#39;s methods of teaching the subject. I would just like to say thankyou very much, as you&#39;re really helping me get to grips with all sorts of Data Science related knowlege which I would have to wait days for to get off of my lecturers via email. I plan to donate as soon as my next pay cheque comes in, so once again; thankyou so much, and keep up the great content!", "comment_date": "2022-07-15T09:35:02Z", "likes_count": 0}, {"comment_by": "Max Niemi", "comment_text": "WHY IS HIS VOICE SO CALMING???", "comment_date": "2022-07-13T22:13:52Z", "likes_count": 0}, {"comment_by": "Mohammad Zandsalimy", "comment_text": "I&#39;m not sure about the &quot;songbird of our generation&quot; tbh.", "comment_date": "2022-07-02T09:35:04Z", "likes_count": 0}, {"comment_by": "\u0421\u0430\u0448\u0430 \u0417\u0430\u0439\u043a\u0430", "comment_text": "\u043f\u043e \u0440\u0443\u0441\u0441\u043a\u0438 \u0431\u044b", "comment_date": "2022-06-29T22:12:15Z", "likes_count": 0}, {"comment_by": "Jp Silver", "comment_text": "Dude, your videos are amazing. Thank you for sharing your infinite knowledge about math!", "comment_date": "2022-06-29T07:46:16Z", "likes_count": 0}, {"comment_by": "\u039d\u03b9\u03ba\u03b7\u03c6\u03cc\u03c1\u03bf\u03c2 \u0392\u03bb\u03ac\u03c7\u03bf\u03c2", "comment_text": "I&#39;ll keep this short because I see a lot of long comments with fancy words describing just how incredible your work on this channel is. So lets me just sum it up, in one word, for you. Magic.", "comment_date": "2022-06-27T14:40:40Z", "likes_count": 0}, {"comment_by": "Jiaqin T", "comment_text": "This is very helpful.", "comment_date": "2022-06-23T12:27:51Z", "likes_count": 0}, {"comment_by": "Kellen", "comment_text": "Why do we sum the squares of the differences to get the cost function, rather than the differences?", "comment_date": "2022-06-19T21:55:00Z", "likes_count": 0}, {"comment_by": "\u0645\u062c\u062a\u0628\u0649 \u0627\u0644\u0634\u0645\u0633", "comment_text": "Really great video with very clear explanation!<br>Thank you and all of the people who contributed to this video.<br><br>I would love, if you are planning to add more videos in this series, to know more about stochastic gradient decent as it is very common to be used in practice.", "comment_date": "2022-06-14T07:42:06Z", "likes_count": 0}, {"comment_by": "Mr. Fahrenheit", "comment_text": "What\u2019s meant by increasing weights in proportion to activation and activation in proportion to weights. Isn\u2019t it better to have a change thats unrestricted?", "comment_date": "2022-06-14T01:07:22Z", "likes_count": 0}, {"comment_by": "Pankaj Sharma", "comment_text": "You are the best!!! Thank you so so so much!!! &lt;3", "comment_date": "2022-06-13T19:58:36Z", "likes_count": 0}, {"comment_by": "anonymous name", "comment_text": "This makes me wonder, could you download a pack of lots of fonts, and write a program to generate an image of a random character from a random font, and then train a NN off of that data?", "comment_date": "2022-06-12T01:57:21Z", "likes_count": 0}, {"comment_by": "\u7530\u4e2dTanaka\u5c1a\u5fd7Naoshi", "comment_text": "Good video. Deeplearning and gradient vectors work like a dialectic. We know the right answer must be logical, but we don&#39;t know how to get from a to b.", "comment_date": "2022-06-04T08:54:00Z", "likes_count": 0}, {"comment_by": "oscar katzinski", "comment_text": "Amazing series", "comment_date": "2022-06-02T22:50:22Z", "likes_count": 0}, {"comment_by": "mysillyusername", "comment_text": "crowdflower page not found...", "comment_date": "2022-06-02T05:09:55Z", "likes_count": 0}, {"comment_by": "Mr Muschnik", "comment_text": "Einfach super.", "comment_date": "2022-05-24T12:50:17Z", "likes_count": 0}, {"comment_by": "alex isakksen", "comment_text": "This is so well explained, it&#39;s ridiculous. My god.", "comment_date": "2022-05-16T01:36:53Z", "likes_count": 0}, {"comment_by": "Ishaan Singh", "comment_text": "Could you give a brief explanation on L1 and L2 Regularization, Please?", "comment_date": "2022-05-13T06:37:40Z", "likes_count": 0}, {"comment_by": "Mads", "comment_text": "This made me realize how much computational power our computers have - it can do such iterations so fast", "comment_date": "2022-05-10T13:39:27Z", "likes_count": 0}, {"comment_by": "Esmeralda Torres", "comment_text": "Great explanation, thanks!", "comment_date": "2022-05-09T22:32:26Z", "likes_count": 0}, {"comment_by": "fire stick", "comment_text": "You are a Legend .", "comment_date": "2022-05-08T06:14:54Z", "likes_count": 0}, {"comment_by": "Guy Leblanc", "comment_text": "Just wondering... The cost function goes to a minimum. Then the back propagation climbs you up in a new cost function that will seek a new minimum. The learning process. Do we always have convergence for these minimums?", "comment_date": "2022-05-06T07:11:53Z", "likes_count": 0}, {"comment_by": "Life has no purpose", "comment_text": "how does it determine how much the activation should change, i mean the weight have been changed", "comment_date": "2022-04-26T13:30:09Z", "likes_count": 0}, {"comment_by": "Connor CoCo", "comment_text": "I know this is super super late, but I was just curious as to whether this method of reducing cost would potentially lead to overfitting?", "comment_date": "2022-04-14T03:34:17Z", "likes_count": 0}, {"comment_by": "The Game Vortex", "comment_text": "If i would be eally determend on training that, i would make a neural network that makes images that look like numbers because more than a 1000 pictures a day", "comment_date": "2022-04-10T10:38:50Z", "likes_count": 0}, {"comment_by": "Raphael Okai", "comment_text": "The video is soo great", "comment_date": "2022-04-07T22:28:44Z", "likes_count": 0}, {"comment_by": "W L", "comment_text": "well, I mean.. just give me the derivate of energy function, and go through all this calculation.", "comment_date": "2022-04-07T15:31:24Z", "likes_count": 0}, {"comment_by": "Aditya Agarwal", "comment_text": "Its just so mind boggling that our brain can learn so well, with just too less training data....", "comment_date": "2022-03-27T05:46:11Z", "likes_count": 0}, {"comment_by": "Charles Van Noland", "comment_text": "I&#39;ve been trying to wrap my head around this for years. I&#39;ve always understood the gist of backprop but even after watching this I&#39;m still confused about how you get from the 2nd-to-last layer to the 3rd-to-last. I understand how you calculate the error from the output layer to the layer before it but then how do you calculate what to do with the weights leading up to the hidden layer just before the output layer?", "comment_date": "2022-03-20T13:08:12Z", "likes_count": 0}, {"comment_by": "kharnak crux", "comment_text": "What&#39;s mind-blowing... i&#39;m watching this in the context of fragrance recognition.    just one sense alone has hundreds of vector components and inputs at a time.", "comment_date": "2022-03-16T01:39:40Z", "likes_count": 0}, {"comment_by": "Luca Voros", "comment_text": "It\u2019s 12 midnight and I have been reading articles on this all day, thank you for helping me!", "comment_date": "2022-03-09T04:49:33Z", "likes_count": 0}, {"comment_by": "Murkantyle", "comment_text": "i b watching this knowing i scored a 7% on my discrete math exam", "comment_date": "2022-03-07T23:54:05Z", "likes_count": 0}, {"comment_by": "Ferrylane AIBM", "comment_text": "Hi Dave,<br>Thanks for the video!<br><br>Can you do a video on ResNets please.", "comment_date": "2022-03-06T09:12:02Z", "likes_count": 0}, {"comment_by": "Jean Zyx", "comment_text": "you made the exploit of making understand the base of neuronal network to a french whos not speaking english exept for school and who is 14 years old. I wish all teachers are like you", "comment_date": "2022-03-05T20:45:47Z", "likes_count": 0}, {"comment_by": "vikas manav", "comment_text": "They should create  a Nobel prize category for education and give it to you.. You are awesome..", "comment_date": "2022-03-05T15:14:22Z", "likes_count": 0}, {"comment_by": "MattC's YT Channel", "comment_text": "Hi 3Blue1Brown, can you tell us what animation software you use to create your animations? It looks amazing!", "comment_date": "2022-03-05T15:10:00Z", "likes_count": 0}, {"comment_by": "DankquaMan", "comment_text": "But why should the second to last layer change when we already have weights that we can adjust?", "comment_date": "2022-03-02T20:45:03Z", "likes_count": 0}, {"comment_by": "Ammar Arif", "comment_text": "Half the time I&#39;m in awe of the quality of the content. The other half I&#39;m learning. Thank you so much man.", "comment_date": "2022-02-28T09:27:17Z", "likes_count": 0}, {"comment_by": "The Hatted Hedgehog", "comment_text": "still doesn&#39;t make sense, but I guess that&#39;s because I&#39;m a sleepy high-schooler.  maybe one day...", "comment_date": "2022-02-18T17:03:51Z", "likes_count": 0}, {"comment_by": "Marco Tr\u00f6ster", "comment_text": "I love the statement &quot;the most confusing part is the notation&quot;. This is so true \ud83d\ude02 It would be really, really nice if you did a last video on how to program this network in a comprehensive way \ud83d\ude04<br><br>IMO the inventors of DNN chose all those activation functions and matrice multiplications really carefully to make computation feasible. But all this beautiful simplicity is gone once these unnecessarily complicated math formulas come into place.<br><br>Honestly, you did an amazing job with your videos on DNNs and backpropagation. The first time I seemed to understand it.", "comment_date": "2022-02-13T15:36:16Z", "likes_count": 7}, {"comment_by": "Keldor314", "comment_text": "Another important thing with batches in training is that it can help escape local minimums.", "comment_date": "2022-02-08T05:55:31Z", "likes_count": 0}, {"comment_by": "\ub0a8\uc900 \ud669", "comment_text": "\uc778\uc0dd\uc758 \ud68c\uc804\ubaa9\ub9c8", "comment_date": "2022-02-07T11:56:20Z", "likes_count": 0}, {"comment_by": "\ub0a8\uc900 \ud669", "comment_text": "\uc560\uc4f0\uc9c0\ub9c8\uc694^^", "comment_date": "2022-02-05T08:17:49Z", "likes_count": 0}, {"comment_by": "Abhinav Garg", "comment_text": "I have watched first 3 videos of this series and i have gained a lot of insight about what basically happens within the black box. Thanks a lot 3Blue 1Brown for these wonderful video illustrations. It took me more than 3.5 hours along with note making. I have still not completely understood NN, but its ok. I am looking forward to video number 4 of these wonderful series. Thanks a ton once again :))))))", "comment_date": "2022-02-03T12:27:11Z", "likes_count": 0}, {"comment_by": "Abdullah Yaqub", "comment_text": "Thanks!", "comment_date": "2022-02-03T01:27:16Z", "likes_count": 0}, {"comment_by": "Abhinav Garg", "comment_text": "I have watched first 3 videos of this series and i have gained a lot of insight about what basically happens within the black box. Thanks a lot 3Blue 1Brown for these wonderful video illustrations. It took me more than 3.5 hours along with note making. I have still not completely understood NN, but its ok. I am looking forward to video number 4 of these wonderful series. Thanks a ton once again :))))))", "comment_date": "2022-02-01T12:28:36Z", "likes_count": 2}, {"comment_by": "NavySturmGewehr", "comment_text": "I&#39;ve been studying your content and a few other peoples content.  I&#39;ve got a somewhat working neural network and I can easily train one neuron but the back propagation part is turning out to be very difficult.  I almost had something working but it seems to cause all the neurons in a layer to behave the same way.", "comment_date": "2022-01-30T19:16:03Z", "likes_count": 0}, {"comment_by": "Eitan Asher", "comment_text": "Man, you&#39;re a real genius.", "comment_date": "2022-01-29T19:36:10Z", "likes_count": 0}, {"comment_by": "Ahmed Engineer", "comment_text": "I really admire your work ... This is the 2nd time I watch this series , and I can say that it makes a little more sense right now \ud83d\ude05<br>But I salute you for your efforts", "comment_date": "2022-01-26T11:20:44Z", "likes_count": 0}, {"comment_by": "Yi Li", "comment_text": "Thank you for saving my life.", "comment_date": "2022-01-25T07:18:39Z", "likes_count": 0}, {"comment_by": "Keshav Bimbraw", "comment_text": "Beautifully explained!", "comment_date": "2022-01-19T16:15:29Z", "likes_count": 0}, {"comment_by": "Mino Levon", "comment_text": "our professor just got his doctore degree and he tells us youtube is where you can get your education and he refers us to this channel. He also said all he got after all these years of studying is DR prefix on his name and thats it.", "comment_date": "2022-01-14T06:34:28Z", "likes_count": 0}, {"comment_by": "Hansana", "comment_text": "I love you", "comment_date": "2022-01-12T16:21:57Z", "likes_count": 0}, {"comment_by": "Eric Xue", "comment_text": "One thing I noticed is that when the batches are made, the network &quot;stumbles&quot; into an even better minima than the local minima it would have originally slid into", "comment_date": "2022-01-09T03:32:52Z", "likes_count": 0}, {"comment_by": "David Dessert", "comment_text": "You have me wondering about modifying the Cost function and if it would perform better if we concede that certain numbers look similar to each other.  <br>For instance, if the input is a \u201c6\u201d, the expectation is that the \u201c6\u201d output would be 1.0 but perhaps the \u201c5\u201d output could be expected to be 0.5 (or similar) because they do look alike. <br>After all, we only really care that the \u201c6\u201d output is greater than all the others, not that all the others are as close to 0.0 as possible.", "comment_date": "2022-01-08T07:05:47Z", "likes_count": 0}, {"comment_by": "Jatin Khanna", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=5m52s\">5:52</a> bang for your buck", "comment_date": "2022-01-08T05:18:48Z", "likes_count": 0}, {"comment_by": "Melanie Tu", "comment_text": "is the way you calculate loss here a form of supervised learning?", "comment_date": "2022-01-07T23:00:37Z", "likes_count": 0}, {"comment_by": "Eduard Rutetskyy", "comment_text": "That feeling when you train AI but still got skammed by a free t shirt. BAD COMPUTER!))))", "comment_date": "2022-01-07T10:13:08Z", "likes_count": 0}, {"comment_by": "J\u00e4tski.fi", "comment_text": "think digital logic instead", "comment_date": "2021-12-30T11:24:19Z", "likes_count": 0}, {"comment_by": "Saikat Chakraborty", "comment_text": "Thanks a lot for making such valuable content and making it available for free", "comment_date": "2021-12-27T15:16:01Z", "likes_count": 0}, {"comment_by": "Matrionix Matx", "comment_text": "Just watched this in college vacation after taking cse engineering as a college course lol", "comment_date": "2021-12-27T05:21:00Z", "likes_count": 0}, {"comment_by": "Py10 Playz", "comment_text": "We are Neural network learning about a neural network", "comment_date": "2021-12-22T09:54:22Z", "likes_count": 0}, {"comment_by": "Kemsekov", "comment_text": "I must say it. I am 100% seriously learnt English just to be able watch your videos. This is the content that will help everyone wandering grow stronger in their favourite subjects. I thank you for your work from the bottom of my heart \u2764\ufe0f\u2764\ufe0f", "comment_date": "2021-12-20T10:26:23Z", "likes_count": 1}, {"comment_by": "Viktor Skarve", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m04s\">8:04</a>, why do we add the derivatives instead of taking the average?", "comment_date": "2021-12-19T19:10:53Z", "likes_count": 1}, {"comment_by": "Shiladitya", "comment_text": "No form of words can express enough what magic you&#39;re creating! I don&#39;t know how much you actually think you impact us.. but let me tell you Grant, your effect on my life is immeasurable! And the fact that you learnt it the hard way, and made it so simple for us, so that we don&#39;t have to go through the same, makes me respect you even more and more every single day.. Thank you so much.. 3B1B is undoubtedly the best channel on YouTube..", "comment_date": "2021-12-18T09:51:31Z", "likes_count": 1}, {"comment_by": "\ucda9\uacfd 33\uae30", "comment_text": "thank you so much for these videos!", "comment_date": "2021-12-16T08:19:05Z", "likes_count": 0}, {"comment_by": "Virkutisss", "comment_text": "Why do we need to minimize cost function in machine learning, what&#39;s the purpose of this? Yeah, I understand that there will be less erorrs etc., but I need to understand it from fundamental perspective. Why don&#39;t we use global maximum for example?", "comment_date": "2021-12-12T21:02:46Z", "likes_count": 0}, {"comment_by": "David DS", "comment_text": "what would happen if instead of taking the avarage cost result from all the training inputs, you instead put them in one huge vector, and then try to minimize this vector to be the 0 vector? i expect this to make the network more accurate but maybe its too much computations?", "comment_date": "2021-12-11T10:26:10Z", "likes_count": 0}, {"comment_by": "mohamed fouad", "comment_text": "someone should do a scientific youtube, filter all the shit ads, distractions  and cats.", "comment_date": "2021-12-09T11:08:07Z", "likes_count": 0}, {"comment_by": "Trigger TheSound", "comment_text": "amazing content", "comment_date": "2021-12-09T01:24:28Z", "likes_count": 0}, {"comment_by": "Qw3rtykid3", "comment_text": "I&#39;m still a little confused by the mini-batches. So is it that the new weights and biases are not applied until after each 100 test examples, of which you calculate an average across those 100 of what each weight and bias should change by?", "comment_date": "2021-12-08T13:12:04Z", "likes_count": 0}, {"comment_by": "Oren Balaban", "comment_text": "Hi!<br>I have a question regarding something I am not sure about. On <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m38s\">7:38</a>, why should we increase the value of the top node on the second-to-last layer in order to decrease the value of the &quot;0&quot; node on the last layer? logically, because the weight connects the nodes is high(blue) we would like to diminish its effect, by decreasing its value contrast to the animation.<br>I would like to know where I am wrong in my perception.<br><br>P.S I LOVE your videos.", "comment_date": "2021-12-06T18:35:53Z", "likes_count": 0}, {"comment_by": "Recidivist", "comment_text": "Everyone praising his explanation <br>Me: watching this in 9th and can barely make anything out<br>No offence or anything its just my ignorance", "comment_date": "2021-11-28T08:15:19Z", "likes_count": 0}, {"comment_by": "Murtaza", "comment_text": "love you and hats off", "comment_date": "2021-11-27T17:56:28Z", "likes_count": 0}, {"comment_by": "Hamed Sharifian", "comment_text": "Clearly Explained with awesome animation. Thanks a lot. :)", "comment_date": "2021-11-26T19:49:38Z", "likes_count": 0}, {"comment_by": "Jacky S", "comment_text": "The squared difference - what is the name of that cost function?", "comment_date": "2021-11-22T22:57:08Z", "likes_count": 0}, {"comment_by": "Robert Knetsch", "comment_text": "This is great! Now I need to know how to take an image file (say, jpeg) and convert it to get the inputs he got. Any suggestions?", "comment_date": "2021-11-21T07:18:37Z", "likes_count": 0}, {"comment_by": "Gabriel", "comment_text": "Thank you 3B1B. The first time I saw these series, I was like: &quot;Yeah, it makes sense&quot;. But now, when I&#39;m coding a NN (thanks to you), I have to rewatch all of them, because, nothing makes sanes:) Such a simple idea, but so hard to implement. Anyway, this is a really intuitive explanation of backprop algorithm", "comment_date": "2021-11-16T14:22:47Z", "likes_count": 0}, {"comment_by": "Pianoista", "comment_text": "Thank you for the awesome explanations! Question, the nudges are acted upon the weights and biases, not the activations, correct? Then how are the activations determined? Thanks in advance.", "comment_date": "2021-11-15T01:46:31Z", "likes_count": 0}, {"comment_by": "karan shete", "comment_text": "Thanks", "comment_date": "2021-11-14T13:21:29Z", "likes_count": 1}, {"comment_by": "\u05d0\u05d5\u05e8 \u05e7\u05dc\u05d9\u05e4\u05d4", "comment_text": "I love how Fermat labeled as &#39;Tease&#39; \ud83d\ude02", "comment_date": "2021-11-11T14:08:43Z", "likes_count": 0}, {"comment_by": "Chaman Gupta", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=10m41s\">10:41</a> summary", "comment_date": "2021-11-10T14:36:45Z", "likes_count": 0}, {"comment_by": "Theerthan Vidya", "comment_text": "the more i see this video the more confused i become", "comment_date": "2021-11-05T04:03:19Z", "likes_count": 0}, {"comment_by": "Mr. Panda", "comment_text": "There is one mistake. We apply Stochastic Gradient Descent per sample, not per batch. In the animation, it is shown as we apply them to the batch which is wrong. To summarize, when we apply SGD, the batch size is equal to 1, the number of mini-batches is equal to number of samples.", "comment_date": "2021-11-04T17:56:32Z", "likes_count": 0}, {"comment_by": "kailasa nischal", "comment_text": "i liked this concept(all 3 vedios).....anyone suggestions of any course i can take to move on in this field", "comment_date": "2021-11-03T07:29:37Z", "likes_count": 1}, {"comment_by": "Courageous Cuber", "comment_text": "Imagine having a neural network being able to train a neural network", "comment_date": "2021-11-03T03:19:19Z", "likes_count": 0}, {"comment_by": "JazevoAudiosurf", "comment_text": "I just wonder how a batch gets computed. Does it calculate the gradient for every example and take the average of that, and then backpropagate that average?", "comment_date": "2021-11-02T20:06:07Z", "likes_count": 0}, {"comment_by": "Simone Truglia", "comment_text": "That&#39;s insanely clearly explained! Thanks", "comment_date": "2021-10-28T20:52:02Z", "likes_count": 0}, {"comment_by": "Phobos", "comment_text": "You&#39;re a wizard Grant! I sometimes have little epiphanies watching these videos which give a little more meaning to the subject I am trying to learn! Please make as much quality videos as you can! cheers", "comment_date": "2021-10-28T19:49:44Z", "likes_count": 0}, {"comment_by": "Shanur Rahman", "comment_text": "If only there was a nobel prize for youtube videos", "comment_date": "2021-10-20T20:40:43Z", "likes_count": 0}, {"comment_by": "Hansha Raj", "comment_text": "Great Video", "comment_date": "2021-10-18T21:56:16Z", "likes_count": 0}, {"comment_by": "Umair Farooq", "comment_text": "Can anyone point me to the link of the code to Backprop", "comment_date": "2021-10-17T14:16:17Z", "likes_count": 0}, {"comment_by": "Mike Bodzio", "comment_text": "What a magnificient LECTURE!", "comment_date": "2021-10-14T14:24:03Z", "likes_count": 0}, {"comment_by": "\uc885\uc6a9 \uacc4\uc815 \ud559", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=5m55s\">5:55</a> hebbian theory", "comment_date": "2021-10-10T11:43:46Z", "likes_count": 0}, {"comment_by": "Max Waller", "comment_text": "<b>\u00a1pondering and wondering at 1:50 am Pacific Daylight Savings Time on Thursday, 7 October 2021 Common Era or CE formerly known as Ano Domini or AD!</b>", "comment_date": "2021-10-07T08:49:58Z", "likes_count": 1}, {"comment_by": "Sasan Dabirian", "comment_text": "how can you be this smart???? lol", "comment_date": "2021-10-06T16:09:32Z", "likes_count": 0}, {"comment_by": "Michael Byrne", "comment_text": "It seems like calculating the total derivative is very expensive when you have a large amount of inputs.  Also considering the large parameter space, how would an optimization strategy like particle swarm do in this case?  One benefit is that it doesn&#39;t get stuck in local minima.", "comment_date": "2021-10-01T19:18:22Z", "likes_count": 0}, {"comment_by": "Prosetar", "comment_text": "I feel future work of \u2018trainer\u2019 who gathers training data and trains neuron networsks", "comment_date": "2021-09-30T16:00:36Z", "likes_count": 0}, {"comment_by": "Andrian Raja Naibaho", "comment_text": "God bless you", "comment_date": "2021-09-29T16:08:59Z", "likes_count": 0}, {"comment_by": "Rembau Times", "comment_text": "Such a nice and well designed videos, complete with cute animations. Will need to watch this several times to fully absorb the content. Great work, a work of art.", "comment_date": "2021-09-29T14:28:39Z", "likes_count": 0}, {"comment_by": "Md Shariful Islam", "comment_text": "Is it just me or is the MNIST handwritten digits database a lot biased or something? I mean, it will give you 90+% accuracy for even the most basic levels of Machine Learning algorithms. But then apply that same model to data of your own (let&#39;s say, your own handwritten digits reduced to 28x28 pixels) and now the model understands jacksh!t.", "comment_date": "2021-09-21T19:06:05Z", "likes_count": 0}, {"comment_by": "zzZzzZtzzZzztztztZ", "comment_text": "What\u00b4s the name of the wonderful piano music at the end :)?", "comment_date": "2021-09-20T22:48:48Z", "likes_count": 0}, {"comment_by": "mariam issa", "comment_text": "wow thanks!", "comment_date": "2021-09-18T11:05:10Z", "likes_count": 0}, {"comment_by": "Kron", "comment_text": "I got an A-grade for neural networks in college and I couldn&#39;t explain how back-propagation works.", "comment_date": "2021-09-11T13:58:05Z", "likes_count": 0}, {"comment_by": "Dee U", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=5m30s\">5:30</a> Why can&#39;t we change the activation function of that node.. from sigmoid to something else, in order to increase the output?", "comment_date": "2021-09-07T15:56:19Z", "likes_count": 0}, {"comment_by": "\u9e2942\u6591", "comment_text": "This is incredible", "comment_date": "2021-09-05T19:18:34Z", "likes_count": 0}, {"comment_by": "S Y", "comment_text": "3blue1brown &gt; Indian guy on youtube &gt;&gt;&gt; my CS teacher", "comment_date": "2021-09-02T23:47:57Z", "likes_count": 2}, {"comment_by": "Macknight Xu", "comment_text": "So, gradient descent is the thoughts,<br>and backpropagation is the method,<br>right?<br>Is there any other method that can implement gradient descent?", "comment_date": "2021-08-26T07:24:19Z", "likes_count": 0}, {"comment_by": "GIZMO NICOL\u00c1S", "comment_text": "Alternative title: how to kill your brain in 14 minutes", "comment_date": "2021-08-20T13:34:06Z", "likes_count": 0}, {"comment_by": "simay kaz\u0131c\u0131", "comment_text": "Incredibly good video. I wish at least a small portion of the budget for B dollar movies could be invested in such important videos to increase the attention to math and science. Who knows maybe one day we will watch these videos like a movie in cinemas, having neurons talking to each other trying to nudge up or down, calculus is an old wise person whom young people ask for &#39;solutions&#39; having the overall goal to solve a complex problem of humans such as climate change....", "comment_date": "2021-08-16T11:51:10Z", "likes_count": 0}, {"comment_by": "Antoine", "comment_text": "Thank you for the series of videos 3Blue1Brown. It proved really helpful in my understanding of neural networks.<br><br>What I do not understand here though, is why you show only a descending gradient path on a single cost surface.<br>To my understanding, the cost surface (or whatever cost object if we talk about 13,000 weights/dimensions) is dependent on the input. So we should have 1 cost surface for every training instance. Isn&#39;t that the case?<br>At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=10m12s\">10:12</a>, you show a descending trajectory on the same surface, as if for each training instance we descend a similar cost function.<br><br>Can somebody clarify that for me please?", "comment_date": "2021-08-16T08:37:23Z", "likes_count": 0}, {"comment_by": "Jenn W.", "comment_text": "Love your videos &lt;3", "comment_date": "2021-08-12T04:03:39Z", "likes_count": 0}, {"comment_by": "Xentox 501", "comment_text": "This is explained soo well, Thanks! On the Sides of my prof, I don&#39;t get it at all but here it makes total sense!", "comment_date": "2021-08-10T19:05:52Z", "likes_count": 0}, {"comment_by": "schnellel\u00f6sungen", "comment_text": "Can you tell me the name of the video-animation software that you use for your videos? I would appreciate it.", "comment_date": "2021-08-10T11:40:03Z", "likes_count": 0}, {"comment_by": "Scott Silver", "comment_text": "Thanks!", "comment_date": "2021-08-03T21:51:04Z", "likes_count": 1}, {"comment_by": "Tedis0n", "comment_text": "I am a sheep and this man is my shepherd", "comment_date": "2021-07-26T20:49:33Z", "likes_count": 0}, {"comment_by": "Nguy\u1ec5n B\u1ea3o Dung", "comment_text": "I love every videos from you! Such high quality content !", "comment_date": "2021-07-18T11:06:47Z", "likes_count": 0}, {"comment_by": "Bits Of Interest", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=11m57s\">11:57</a> if only I had the most accessed website in the world and could arbitrarily challenge you to access it by making you provide me with training data for my model.<br><br>&quot;Just making sure you&#39;re not a bot, while you help me build mine&quot;", "comment_date": "2021-07-11T23:17:32Z", "likes_count": 0}, {"comment_by": "Twitter Analytics by AD", "comment_text": "Sometimes, I have to pause the video so information can sink in.", "comment_date": "2021-07-11T06:18:54Z", "likes_count": 1}, {"comment_by": "Dawud Hassan", "comment_text": "Rong", "comment_date": "2021-07-09T22:50:16Z", "likes_count": 0}, {"comment_by": "FastW", "comment_text": "I have lots to say but I can&#39; t speak. What am I doing with my life?", "comment_date": "2021-07-07T22:21:31Z", "likes_count": 1}, {"comment_by": "Vojt\u011bch Pol", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m20s\">12:20</a> :D that&#39;s exactly why Captcha wants us to label crossings, cars and other nonsense. They just need the labeled data.", "comment_date": "2021-07-07T19:43:48Z", "likes_count": 1}, {"comment_by": "gaurav prasad", "comment_text": "The best explanation for backpropagation \ud83d\udc4d\ud83d\udc4d<br>Ton loads of thank you", "comment_date": "2021-06-30T15:47:01Z", "likes_count": 0}, {"comment_by": "krish J", "comment_text": "Amazing explanations and visuals about the concept of backpropagations.", "comment_date": "2021-06-25T12:18:04Z", "likes_count": 0}, {"comment_by": "Kortan", "comment_text": "I think it&#39;d be extremely beneficial if you were a complete lecture series for deep learning. Your way of teach is billion times better than this somewhat popular Andrew Ng lecture series, and also a lot more intuitive. Please consider this:)<br>Best wishes", "comment_date": "2021-06-23T09:55:16Z", "likes_count": 4}, {"comment_by": "Shidharth Routh", "comment_text": "One thing I actually fail to comprehend is how would u make a single network learn about each and every handwritten digit and make the network actually give u correct labels for everyone of them too\u2026 i am confused to be honest", "comment_date": "2021-06-19T05:22:02Z", "likes_count": 0}, {"comment_by": "Dinu B", "comment_text": "I find it astonishing how well you convey some of the intricacies here, way better than most of ML practitioners who teach the public, whether as youtubers, online instructors or public speakers who end up on video on the internet. I very much resonate with the way you frame things, the metaphors you choose, your visualizations and of course your evident love for understanding and sharing thereof. Your work is a great gift to all of us - students, engineers, researchers, philosophers, random viewers from all walks of life. Thank you.", "comment_date": "2021-06-18T17:03:36Z", "likes_count": 0}, {"comment_by": "Mayank Mishra", "comment_text": "Experienced countless EUREKA moments in this series. Grant Sanderson is a legend.", "comment_date": "2021-06-06T06:26:55Z", "likes_count": 0}, {"comment_by": "Amrish Kelkar", "comment_text": "Is it correct that back propogation also adjusts the activation function (say &#39;filters&#39; in a CNN). Grant seems to say here that only the weights and biases are being adjusted -- and may be he says to keep things simple - But I&#39;m interested in knowing if the filters themselves are modified as the model is tuned.", "comment_date": "2021-05-31T11:43:18Z", "likes_count": 0}, {"comment_by": "Ali Al Shammaa", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m31s\">8:31</a> How do you suddenly switch from changes in activation to changes in weights ? @3blue1brown", "comment_date": "2021-05-28T22:23:26Z", "likes_count": 0}, {"comment_by": "thetruereality", "comment_text": "Wonderful beautiful presentation, really helps in understanding for someone like me who is pursuing AIML. <br>I just have this one question.<br><a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m33s\">7:33</a> Why? Why do we want the other outputs to have lesser activation? Wont that reduce the networks ability to identify numbers other than 2? Are you just giving this one specific example of 2 to explain back propagation?", "comment_date": "2021-05-18T11:14:50Z", "likes_count": 0}, {"comment_by": "Ceder Veltman", "comment_text": "the graphic at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m00s\">8:00</a> is mindblowingly good at explaining what you mean.", "comment_date": "2021-05-16T09:20:31Z", "likes_count": 0}, {"comment_by": "James Mosher", "comment_text": "How do neural networks handle catastrophic failure? Namely, say incorrectly assigning a zero results in death (like say, a car ramming full speed into a barrier, something no non-suicidal human would do intentionally, ie, undistracted)? Is there a meaningful way to encode this? I bring this up bc despite autonomous machines \u201con average\u201d being safer, I don\u2019t see human beings tolerating a system whereby, even if, in the average they are safer, occasionally the system \u201cintentionally\u201d engages in catastrophic failure. Such thinking is a hallmark of safe and reliable engineering; bend before breaking, leak before bursting, etc.", "comment_date": "2021-05-16T05:09:21Z", "likes_count": 0}, {"comment_by": "Leon", "comment_text": "Deep learning of a regimented framework. How excitingly boring next.....", "comment_date": "2021-05-16T03:50:05Z", "likes_count": 0}, {"comment_by": "gaming snake", "comment_text": "this has been better than a year of uni", "comment_date": "2021-05-15T16:15:23Z", "likes_count": 0}, {"comment_by": "william cipto", "comment_text": "Great content!", "comment_date": "2021-05-15T07:27:20Z", "likes_count": 0}, {"comment_by": "Vaddagani Shiva", "comment_text": "Mini Enlightenment starts @ <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m05s\">8:05</a>", "comment_date": "2021-05-13T17:45:52Z", "likes_count": 0}, {"comment_by": "Tymothy Lim", "comment_text": "Thank you very much for this video! It is very educational and intuitive to understand! :)", "comment_date": "2021-05-10T06:28:36Z", "likes_count": 0}, {"comment_by": "Shehnaz Shaikh", "comment_text": "I may have enrolled  to a course which may have violated  your  copyright  and content.<br>Please check it out.<br>They may have looted me by plagiarizing your content.", "comment_date": "2021-05-08T07:43:30Z", "likes_count": 1}, {"comment_by": "Kevin Connolly", "comment_text": "This series is totally brilliant.  I am 73 years old and used to teach mathematics.  I am still learning stuff and with the help of sites like yours it makes it so much easier.  Have you thought of doing any videos on the really complex subject of real analysis/. Keep up the good work. Kevin Connolly", "comment_date": "2021-04-30T13:46:42Z", "likes_count": 0}, {"comment_by": "Giacomo Miola", "comment_text": "Yes he makes it look easy to understand, but that&#39;s because he&#39;s scratching the surface. Real deep learning is not for us average humans", "comment_date": "2021-04-28T20:47:40Z", "likes_count": 1}, {"comment_by": "anotherplatypus", "comment_text": "You sounded so jazzed when you brought up getting a free shirt from that sponser.  = )", "comment_date": "2021-04-28T18:10:52Z", "likes_count": 0}, {"comment_by": "Fahd", "comment_text": "Why do you need two 16-neuron layers? Why not just have one 784-neuron long input layer, then a 10-neuron output layer", "comment_date": "2021-04-27T11:50:43Z", "likes_count": 0}, {"comment_by": "Nicolai Matthew", "comment_text": "interesting stuff!", "comment_date": "2021-04-25T19:52:01Z", "likes_count": 1}, {"comment_by": "Mark da shark", "comment_text": "I wish I had the option to pay my college tuition to you instead of my university. Well done mate!", "comment_date": "2021-04-11T22:05:49Z", "likes_count": 4}, {"comment_by": "Buck Rothschild", "comment_text": "Is it called three blue one brown because of the ratio of sea versus land on the Earth?", "comment_date": "2021-04-06T07:59:24Z", "likes_count": 1}, {"comment_by": "Darijo \u017divkovi\u0107", "comment_text": "How much enough is enough?", "comment_date": "2021-04-03T14:21:17Z", "likes_count": 0}, {"comment_by": "\ub300\ud559\uc6d0\uc0dd \uc0b4\uc544\ub0a8\uae30", "comment_text": "Thanks a lot!!! Actually it help me", "comment_date": "2021-03-29T07:55:39Z", "likes_count": 0}, {"comment_by": "\u0421\u0442\u0430\u0441 \u0421\u0430\u0432\u0438\u043d\u043a\u0438\u043d", "comment_text": "\u041e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u0432\u0438\u0434\u0435\u043e. \u041c\u043d\u0435 \u043f\u043e\u043c\u043e\u0433\u043b\u0438 \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u0442\u044c\u0441\u044f.\r<br>\u042f \u043d\u0430\u043f\u0438\u0441\u0430\u043b \u044d\u0442\u0443 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u043d\u0430 \u043f\u0430\u0441\u043a\u0430\u043b\u0435, \u043c\u043e\u0433\u0443 \u043f\u043e\u0434\u0435\u043b\u0438\u0442\u0441\u044f \u043a\u043e\u0434\u043e\u043c. \u041d\u0435 \u0432\u0441\u0435 \u0432\u043b\u0430\u0434\u0435\u044e\u0442 \u043f\u0438\u0442\u043e\u043d\u043e\u043c \u0438 \u043d\u0435 \u0432\u0441\u0435 \u0433\u043e\u0442\u043e\u0432\u044b \u0435\u0433\u043e \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431 \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u0442\u044c\u0441\u044f \u0432 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044f\u0445. \u041c\u043e\u0436\u0435\u0442 \u043a\u043e\u043c\u0443\u0442\u043e \u044d\u0442\u043e \u043f\u043e\u043c\u043e\u0436\u0435\u0442. \u0414\u0430 \u0438 \u0444\u0430\u0439\u043b mnist.pkl \u044f \u0440\u0430\u0441\u043a\u043e\u0432\u044b\u0440\u044f\u043b, \u043a\u0430\u043a \u0438 \u0433\u0434\u0435 \u0442\u0430\u043c \u0447\u0442\u043e \u043b\u0435\u0436\u0438\u0442, \u0434\u043b\u044f \u0434\u0440\u0443\u0433\u0438\u0445 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u0434\u043d\u043e.", "comment_date": "2021-03-26T09:41:22Z", "likes_count": 0}, {"comment_by": "Henry Chess", "comment_text": "Oh, free shirt? What&#39;s that?<br><b>&quot;The offer has expired&quot;</b><br><b>O O F</b>", "comment_date": "2021-03-22T13:49:51Z", "likes_count": 0}, {"comment_by": "pebre79", "comment_text": "A brilliant piece of work. Bravo. It is a masterpiece of education material and should be shown in classrooms everywhere. Keep it up!", "comment_date": "2021-03-20T02:05:14Z", "likes_count": 0}, {"comment_by": "Edward V", "comment_text": "Hey 3Blue, can you do a video of OPTIC algorithm and DBSCAN??? I&#39;m trying to embark in a project and would really appreciate the intuitive understanding that your videos typically bring", "comment_date": "2021-03-14T00:49:37Z", "likes_count": 0}, {"comment_by": "NoName", "comment_text": "Thank you very much!", "comment_date": "2021-03-11T18:40:04Z", "likes_count": 0}, {"comment_by": "\u041d\u0438\u043a\u0438\u0442\u0430 \u0411\u0443\u043b\u0433\u0430\u0440\u0443", "comment_text": "Please video about convolutional neural networks", "comment_date": "2021-03-09T18:14:19Z", "likes_count": 0}, {"comment_by": "Matthew Read", "comment_text": "How do you know you went to the right local minima?", "comment_date": "2021-03-06T22:14:43Z", "likes_count": 0}, {"comment_by": "Maitha", "comment_text": "great videos thanks to everyone contributed in making it", "comment_date": "2021-03-06T10:11:03Z", "likes_count": 0}, {"comment_by": "Hans-Henrik St\u00e6rfeldt", "comment_text": "I totally like how your commercial sponsor is totally relevant and not just toothpaste !", "comment_date": "2021-03-05T09:46:29Z", "likes_count": 2}, {"comment_by": "Nathan Maves-Moore", "comment_text": "Going to drop squishification in my master&#39;s thesis. Thank you for the outstanding videos", "comment_date": "2021-02-22T22:26:20Z", "likes_count": 0}, {"comment_by": "Jerry An Yu", "comment_text": "Amazing animation! They should give you an Academy Award for Best Animated Short Film for this.", "comment_date": "2021-02-21T06:16:25Z", "likes_count": 0}, {"comment_by": "Chu Rui", "comment_text": "you saved my paper hhhhhhh,thanks again", "comment_date": "2021-02-20T04:32:25Z", "likes_count": 0}, {"comment_by": "Yang Yu", "comment_text": "\u201cTease\u201d. Great joke sir.", "comment_date": "2021-02-20T02:07:31Z", "likes_count": 0}, {"comment_by": "Dafterror", "comment_text": "(<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m39s\">8:39</a>) I think this is a tad misleading. The network would still be shown all images during training, including non-2&#39;s where the loss would be minimal if the second neuron didn&#39;t activate at all. The effect would be a network that has a very high accuracy in classifying whether the image contains a 2 or not, and a very low accuracy for the other digits. <br><br>You&#39;re one of the best youtubers ever btw &lt;3 xoxo", "comment_date": "2021-02-17T22:09:05Z", "likes_count": 0}, {"comment_by": "Aishwarya A R", "comment_text": "I&#39;ve watched these videos 3 times and everytime I watch them, I feel a bit smarter. It starts with understanding little to progressively understanding more and more and finally seeing the big picture. Can I say I&#39;ve understood everything? No. I am getting there and I&#39;ll be coming back for these again. For anyone who&#39;s feeling discouraged, I can assure you that you&#39;ll get there. <br>Thank you so much for creating quality content that brings the driest, most theoretical concepts to life! You&#39;re a hero.", "comment_date": "2021-02-12T01:53:45Z", "likes_count": 22}, {"comment_by": "Stefan Malic", "comment_text": "This really showed me how much I suck at math :(", "comment_date": "2021-02-06T03:07:07Z", "likes_count": 0}, {"comment_by": "Akshay Manthekar", "comment_text": "Is it always a 28x28 grid or it can be something else also? and does it affect the accuracy of neural networks?", "comment_date": "2021-01-22T10:52:18Z", "likes_count": 0}, {"comment_by": "Tristun Alekzander", "comment_text": "Ok but once you have that error what do you do with it? How do you know whether to change the bias or the weight?", "comment_date": "2021-01-12T21:00:50Z", "likes_count": 0}, {"comment_by": "Nedi Sawego Yogya", "comment_text": "reproduction is the cost function of the human brain", "comment_date": "2021-01-11T18:58:37Z", "likes_count": 0}, {"comment_by": "kazenokize2", "comment_text": "this is the clearest explanation I&#39;ve seen. Thank you so much", "comment_date": "2021-01-05T21:15:14Z", "likes_count": 0}, {"comment_by": "Hany Tawfik", "comment_text": "You are a genius!", "comment_date": "2021-01-05T16:24:44Z", "likes_count": 0}, {"comment_by": "Donnie Goodman", "comment_text": "All of the videos shown on this channel have been written so well. Even I can understand, mathematics was a complete mystery for me in school. I have a sixth grade education and I feel really smart after watching one of these programs. Thanks so much. I&#39;m really grateful that you have taken the time to educate the people that have a hard time understanding but still want to learn. What you&#39;re doing is just as important as any other volunteer or charity work. I&#39;m excited I found this .", "comment_date": "2021-01-02T03:00:06Z", "likes_count": 12}, {"comment_by": "Brian Cherry", "comment_text": "I feel really smart until I watch a lesson by someone who is really smart.", "comment_date": "2020-12-23T21:07:55Z", "likes_count": 0}, {"comment_by": "Jiang Yuanbo", "comment_text": "where is Lisha  :(", "comment_date": "2020-12-22T11:42:36Z", "likes_count": 0}, {"comment_by": "Mulan Szechuan", "comment_text": "Thx :D", "comment_date": "2020-12-22T00:50:29Z", "likes_count": 0}, {"comment_by": "Alonso Martinez", "comment_text": "One clarification that might be useful is that weights can be negative, but for this example  all of the initial activations would be positive right?<br><br>Also one unintuitive bit is when you are talking about wanting to increase the activation of the .2.... you want to &#39;make the  negative weight dimmer&#39;, does that mean closer to 0? so that if the weight is negative...it doesn&#39;t count as much?", "comment_date": "2020-12-17T07:05:04Z", "likes_count": 0}, {"comment_by": "Willy Tepes", "comment_text": "With enough feedback (5G, social media, spending habits) you could control the whole world&#39;s economy with this, but you&#39;d have to seize control of all resources first. Anyone want The Great Reset?", "comment_date": "2020-12-07T21:51:14Z", "likes_count": 0}, {"comment_by": "Nolan Zewariligon", "comment_text": "&quot;Neurons that fire together, wire together&quot; is related to old Hebbian learning rule, not backpropagation.", "comment_date": "2020-12-05T00:27:18Z", "likes_count": 1}, {"comment_by": "P .Yannick", "comment_text": "Upvote if you are intersted in the pursuing of the serie with CNN and LSTM !", "comment_date": "2020-12-04T13:27:51Z", "likes_count": 0}, {"comment_by": "Thinh Le Duc", "comment_text": "After watching this video more than 10 times, I was transformed from an ambiguous state to an absolutely stuck state. G\u00fat ch\u00f3p", "comment_date": "2020-12-03T15:54:32Z", "likes_count": 1}, {"comment_by": "Ramil Joaquin", "comment_text": "Thank you very much.  This is amazing content.", "comment_date": "2020-11-29T11:09:54Z", "likes_count": 0}, {"comment_by": "yuan.ping chen", "comment_text": "you need just tell me how to train a chat robot as my girl friend...maybe voice synthesizer also, and combines them.", "comment_date": "2020-11-25T04:13:32Z", "likes_count": 0}, {"comment_by": "Elior Ben-Yosef", "comment_text": "what a great intuitive explanation! thanks 3B1B!", "comment_date": "2020-11-20T20:35:12Z", "likes_count": 0}, {"comment_by": "somi park", "comment_text": "\ub3c4\uccad\uc758\ud63c\uc804\uc21c\uacb0", "comment_date": "2020-11-20T06:55:03Z", "likes_count": 0}, {"comment_by": "somi park", "comment_text": "\ub3c4\uad50\ub3c4\uccad\ubc1c", "comment_date": "2020-11-20T06:54:28Z", "likes_count": 0}, {"comment_by": "The Louis and Kyle Show", "comment_text": "This video is literal ART. Grant is my favorite artist.", "comment_date": "2020-11-16T23:06:08Z", "likes_count": 4}, {"comment_by": "Jaime Argila", "comment_text": "Ah, but what about this formula:<br><br>(for averaging)<br><br>x= old average\r<br>y = new average\r<br>n = new number\r<br>j = old length\r<br>\r<br>y = (x*j + n) / (j + 1)\r<br>j++;", "comment_date": "2020-11-16T13:58:50Z", "likes_count": 1}, {"comment_by": "Bhargav Chaitanya", "comment_text": "What is exactly the cost function?", "comment_date": "2020-11-15T14:16:27Z", "likes_count": 0}, {"comment_by": "ObligedHoss", "comment_text": "I am an engineer at university. Screw Zoom, this is my new lecture \ud83d\ude4c\ud83c\udffb", "comment_date": "2020-11-13T03:08:12Z", "likes_count": 0}, {"comment_by": "Thanh-Tung Nguyen", "comment_text": "This video is amazing now but it sure wasnt at first.<br><br>So if you watch it for the first time and understand very little, its fine. Go read some more and practice with a few simple examples and when you are done, then return to watch it again. Repeat the whole process for a few times. You will be amazed by how much this video reinforces your understanding every time, even though you have grown quite a bit compared to the last time you watched it.", "comment_date": "2020-11-08T14:31:07Z", "likes_count": 0}, {"comment_by": "lianmccc", "comment_text": "This series is better than my in total 6-hour lecture from school.", "comment_date": "2020-11-08T01:17:15Z", "likes_count": 6}, {"comment_by": "doggo", "comment_text": "The biggest challenge is getting the labled data...<br>Google&#39;s ReCAPTCHA would like a word with you... Oh, they happen to be working on self-driving cards? Hmmmm...", "comment_date": "2020-10-30T14:16:39Z", "likes_count": 1}, {"comment_by": "Sandy Bathwater", "comment_text": "This will be helpful :)       I followed a great tutorial that is currently at a cliffhanger:  I was left with a functional forward propagating network, and an activation function I liked, but no training methods, so I decided to go ahead and hack it out and I came up with two fun, very very, and very inefficient training methods.       The first was only with a small training set:    Just randomly generate ALL NEW weights and biases until it works :)      The second was similar but pick a random individual weight or bias and adjust it slightly...  if the network performed better then I keep the change, if it didn&#39;t I swap the last value back in and just do the same with a different one.     It works great but I think I am already running against its usefulness with a training deck of 200 25 element arrays (ints, gt is the sum) ...     WIth 100, it worked very very well, with 99% goodness on testdata...   but I wanted to pump more examples in and the problem exploded ---   which is precisely why I wanted to come up with my own first, so I can understand these issues internally.      /blah", "comment_date": "2020-10-28T19:29:45Z", "likes_count": 0}, {"comment_by": "Firas Kedidi", "comment_text": "awsome visualization !!", "comment_date": "2020-10-25T10:48:51Z", "likes_count": 0}, {"comment_by": "Pritish Mishra", "comment_text": "Please video on SVR and SVC....", "comment_date": "2020-10-23T18:09:42Z", "likes_count": 0}, {"comment_by": "Decode 01", "comment_text": "one of the most best and high quality tutorial on the youtube<br>well i am convinced that this the best tutorial for understanding backpropagation", "comment_date": "2020-10-23T09:00:54Z", "likes_count": 0}, {"comment_by": "Venkat Babu", "comment_text": "Self rectification.", "comment_date": "2020-10-16T16:23:45Z", "likes_count": 0}, {"comment_by": "Sankaran", "comment_text": "Thanks for the time you took to help us . Im just gettign started with my research and this helps! Your presentation quality is extreme! Thanks again :) And i am now subscribed @@@", "comment_date": "2020-10-16T10:55:22Z", "likes_count": 0}, {"comment_by": "Xin Yi", "comment_text": "THIS IS TOTALLY AWESOME!!!!!!!!!!!", "comment_date": "2020-10-14T12:22:47Z", "likes_count": 0}, {"comment_by": "Bruce Wayne", "comment_text": "Today at the current time total subscribers are exactly 3.14 m <br>Now those \u03c0 really work hard", "comment_date": "2020-10-12T05:00:49Z", "likes_count": 0}, {"comment_by": "Gautam Puri", "comment_text": "THIS..... IS GOLD.", "comment_date": "2020-10-12T04:29:25Z", "likes_count": 0}, {"comment_by": "Jeffrey Clem", "comment_text": "A God amongst men", "comment_date": "2020-10-08T01:59:18Z", "likes_count": 0}, {"comment_by": "Artem IA", "comment_text": "I think i do get most of the main idea around the concept, but i have a question about it.<br>Using the same neural network example, input - 2 hidden layers - output, i do get how you can change the weight to output from hidden2, because you know what the result should be. But i don&#39;t get for the correction to hidden2 from 1 how you compute this. How do you know the desire value for hidden 2 ? <br>Thanks for your amazing work. I hope this comment find an answer ^^&#39;", "comment_date": "2020-09-30T11:34:34Z", "likes_count": 1}, {"comment_by": "Smitty Flufferson", "comment_text": "Index chasing is a good name", "comment_date": "2020-09-27T06:13:28Z", "likes_count": 0}, {"comment_by": "Emilio Chavez", "comment_text": "What the hell am I watching. I\u2019m still stuck on how to print Hello World.", "comment_date": "2020-09-26T02:13:00Z", "likes_count": 0}, {"comment_by": "NotGonnaDoMuch", "comment_text": "This lesson is of the highest quality.", "comment_date": "2020-09-24T21:36:26Z", "likes_count": 0}, {"comment_by": "joker28666", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m50s\">4:50</a> is that visually correct? you are adding up the activations of the last layer (visually, the lines go up), but what actually happens is that the weights before it (not on screen) and activations get added? no?", "comment_date": "2020-09-24T18:27:23Z", "likes_count": 0}, {"comment_by": "Jonathan Maccan", "comment_text": "one of the better Machine Learning channels! Delivered very nicely. muchly needed for me and appreciated", "comment_date": "2020-09-24T06:22:44Z", "likes_count": 0}, {"comment_by": "yair katz", "comment_text": "Why are tensor cores useful for ai?", "comment_date": "2020-09-22T18:17:38Z", "likes_count": 0}, {"comment_by": "L", "comment_text": "what do the pie people have to do with any of this?", "comment_date": "2020-09-19T20:01:29Z", "likes_count": 0}, {"comment_by": "074o8tB6St", "comment_text": "Dude, you deserve an award for this.", "comment_date": "2020-09-17T16:23:42Z", "likes_count": 0}, {"comment_by": "Erika Ambrioso", "comment_text": "Im taking a Neural networks course at my university right now and I&#39;m finally starting to understand this stuff after watching this video series! Thank you so much!", "comment_date": "2020-09-15T14:22:30Z", "likes_count": 1}, {"comment_by": "user 28", "comment_text": "great explanation! the best I have ever heard as a student of computer engineering and machine learning! :)", "comment_date": "2020-09-14T11:28:17Z", "likes_count": 0}, {"comment_by": "Soheil Sepahyar", "comment_text": "Thank you so much for the simplicity in your explanations. You try to explain everything not by their definition, but with an understandable, simple way. Thank you!", "comment_date": "2020-09-14T06:27:44Z", "likes_count": 0}, {"comment_by": "anil kumar", "comment_text": "the only thing i like the most is the bgm that comes at the end of the video.", "comment_date": "2020-09-08T17:21:54Z", "likes_count": 1}, {"comment_by": "nathan raynal", "comment_text": "I love the analogy between stochastic gradient descent and a drunk man running down a hill.", "comment_date": "2020-08-25T12:50:24Z", "likes_count": 0}, {"comment_by": "GRE AT WORDS", "comment_text": "++", "comment_date": "2020-08-21T12:01:24Z", "likes_count": 0}, {"comment_by": "Roger Froud", "comment_text": "You talk a lot about local minima. Presumably there are lots of local minima which you could fall towards, depending on where your random weights started. Again, presumably, you can get reasonable results from any local minima, but the best from the lowest minimum? If that&#39;s the case, how do you discover whether there&#39;s a better minimum than the one you&#39;ve stumbled upon? Do you start from several different random starting weights and see if you get better results? There doesn&#39;t seem to be any way of knowing what the absolute minimum is. Can you say something about this?", "comment_date": "2020-08-17T21:09:41Z", "likes_count": 0}, {"comment_by": "GUTTERBOY", "comment_text": "These videos are so well put together, and you&#39;re a great teacher! I apprciate you &lt;3", "comment_date": "2020-08-14T19:03:15Z", "likes_count": 0}, {"comment_by": "Axel Holm", "comment_text": "why would you have an average cost if you&#39;re taking the negative gradient of every individual weight and bias instead of it?", "comment_date": "2020-08-13T16:30:45Z", "likes_count": 0}, {"comment_by": "Agent Smith", "comment_text": "SKYNET. DEFENSE. SYSTEM. IS. NOW. ACTIVATED.", "comment_date": "2020-08-12T17:31:19Z", "likes_count": 0}, {"comment_by": "HLBusiness", "comment_text": "Fantastic work at explaining &quot;difficult mathematical formulas&quot; in very telling simple images! Great job, you are an excellent teacher!!! Thanks for your work.", "comment_date": "2020-08-10T16:35:58Z", "likes_count": 0}, {"comment_by": "Muskan Gupta", "comment_text": "This is officially my avorite youtube channel!", "comment_date": "2020-08-09T23:26:55Z", "likes_count": 0}, {"comment_by": "Devansh Chowdhury", "comment_text": "great video , love your channel.", "comment_date": "2020-08-08T23:16:54Z", "likes_count": 0}, {"comment_by": "FinniKey", "comment_text": "jordan boyd", "comment_date": "2020-08-03T09:53:53Z", "likes_count": 1}, {"comment_by": "\u30ae\u30ac \u30c1\u30e3\u30c9", "comment_text": "@ <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m25s\">12:25</a> so a neural network can eventually determine whether a person is a genius or not just by looking at the face", "comment_date": "2020-08-01T10:04:06Z", "likes_count": 0}, {"comment_by": "iamfishgiver", "comment_text": "\ud83d\udc1f\ud83d\udc1f\ud83d\udc1f\ud83d\udc1f", "comment_date": "2020-07-30T03:58:49Z", "likes_count": 0}, {"comment_by": "Saurav Patil", "comment_text": "Genius ...you are an absolute genius", "comment_date": "2020-07-29T23:02:09Z", "likes_count": 0}, {"comment_by": "Lily Su", "comment_text": "I love how you enunciate the words as if you are truly interested and into the topic. The art direction and the animations are also on point. Kudos to the animator!", "comment_date": "2020-07-25T04:05:01Z", "likes_count": 7}, {"comment_by": "Jason Lian", "comment_text": "We know how to change the weights, but what about the biases then?", "comment_date": "2020-07-21T21:05:41Z", "likes_count": 0}, {"comment_by": "Zirion", "comment_text": "I&#39;m graduated in Statistics an Data Science by my local university. Study the subject for almost a decade.\u00a0<br>For years I heard about backpropagation. No one knew how to explain it (or worse, knew how it worked). It always was &quot;a magic algorithm that adjusts weights&quot;. This video did in 13 minutes what many professors during years couldn&#39;t do to me: explain in a simple way the logic behind it. It&#39;s just perfect: well done, free an with a relevant sponsor in the end. Best video I ever watched. Congratulations!", "comment_date": "2020-07-16T13:46:35Z", "likes_count": 0}, {"comment_by": "Saphs", "comment_text": "My University prof just linked to one of your videos for our exam preperations. You are doing awesome work =)!", "comment_date": "2020-07-16T12:29:00Z", "likes_count": 0}, {"comment_by": "Anjel Patel", "comment_text": "These videos and your voice are so beautiful. I&#39;m surprised at how this gold of a channel is free for the world.", "comment_date": "2020-07-11T14:52:57Z", "likes_count": 0}, {"comment_by": "darren logue", "comment_text": "the music bed has to go", "comment_date": "2020-07-08T21:27:48Z", "likes_count": 0}, {"comment_by": "\u041f\u0430\u0432\u0435\u043b \u041a\u0443\u0437\u043d\u0435\u0446\u043e\u0432", "comment_text": "Thank you for this detailed introduction in such a fabulous topic!", "comment_date": "2020-07-08T19:57:11Z", "likes_count": 0}, {"comment_by": "Balu B", "comment_text": "Beautifully explained!  The video and narration cannot be easier to understand given the complexity of the mathematics of stochastic gradient descent.   The artistry is very evident (and exciting!)  Thank you.", "comment_date": "2020-07-06T16:15:08Z", "likes_count": 0}, {"comment_by": "Werapon Pat", "comment_text": "so how do i adjust the weight", "comment_date": "2020-07-05T06:52:04Z", "likes_count": 0}, {"comment_by": "Jonathan Klein", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=2m50s\">2:50</a>\r<br>Is there a mistake in the second formula?\r<br>Souldn&#39;t it be:\r<br>(wl)T\r<br>insted of:\r<br>(w(l+1))T<br>???", "comment_date": "2020-07-02T18:24:49Z", "likes_count": 0}, {"comment_by": "Imran Mahmood", "comment_text": "I&#39;m sorry if this is a silly question but what is the difference between changing the weights in proportion to the activation and changing the activation in proportion to the weights? As you stated in the video we cannot directly change the value of the activations. However, we can adjust the value of the weights which subsequently would change the value of the activation. So in my mind, I am interpreting those two statements as two different ways of the saying the same thing. I am not sure I am misunderstanding something and would be very grateful if you would be able to help clear this up for me. Thanks!", "comment_date": "2020-07-01T11:51:09Z", "likes_count": 0}, {"comment_by": "Macho Valkarie", "comment_text": "Can you use algebra instead of calculus for backpropagation.", "comment_date": "2020-06-30T00:48:01Z", "likes_count": 0}, {"comment_by": "Vincent Lius", "comment_text": "I have a question ? Why we need to care for which give you the most bang in the buck (in the video he mean increase the weight that have higher value neuron) ?<br>Thank you in advance", "comment_date": "2020-06-29T12:41:41Z", "likes_count": 0}, {"comment_by": "Theo Gottwald", "comment_text": "I wonder the Title of the Video is displayed in german, while the video itself is in english.<br>So if english-only speaking people see the title they may skip the video.<br>German only speakers may click it and then see that it is in english and also skip it.<br>So i suggest to get tile and video language into harmony.", "comment_date": "2020-06-28T15:04:22Z", "likes_count": 0}, {"comment_by": "Alperen Taciroglu", "comment_text": "Could be the most professionally created video in all Youtube", "comment_date": "2020-06-27T13:35:16Z", "likes_count": 0}, {"comment_by": "notanape", "comment_text": "Hi Grant. Thanks for the Psychedelic video, I tripped for the whole 14 minutes.", "comment_date": "2020-06-22T10:57:57Z", "likes_count": 1}, {"comment_by": "Khemiri Monem", "comment_text": "Visual representations of these videos are the essence of their success", "comment_date": "2020-06-16T12:24:34Z", "likes_count": 0}, {"comment_by": "Z Z", "comment_text": "For the \u201cutter trash\u201d vector, all the values should add up to \u201c1\u201d as softmax would likely be used in this task.", "comment_date": "2020-06-14T17:58:04Z", "likes_count": 0}, {"comment_by": "Beelzebub", "comment_text": "how do i get the cost of say the first hidden layer", "comment_date": "2020-06-14T12:50:38Z", "likes_count": 0}, {"comment_by": "Beelzebub", "comment_text": "i knwo im really late but practicly we use the cost and based on this we change ALL weights and not only the weights of the fist or last layer", "comment_date": "2020-06-14T12:48:14Z", "likes_count": 0}, {"comment_by": "\u30bd\u30c8\u30e4\u30de\u30de\u30ea\u30a2\u30c6\u30ec\u30b5", "comment_text": "the average human male has 86 billion neurons. you would need an AI to populate a compartmentalized artificial neural network to make on per with a compartmentalized human brain.  the frontal cortex in human brains is not completely developed until age 24 or 25. The frontal cortex controls logic processing and decisions of conscience (or moral decisions) , yet the Insular Cortex controls disgust. tasty rotten apples! step right up and take a bite! Rotten Apples yum! the insular Cortex controls that as well as moral disgust. like that scene where the guy vomits in the Crying Game movie.  Fun fact: we have 100s of neurotransmitters or memory and thoughts that double as hormones or emotions depending  on which brain compartment the biochemcal is in. And that is why medications often have nasty side effects. How would you code and neuron objectect that doubles as a hormone i like an AI chatbot? I wonder.", "comment_date": "2020-06-11T08:56:26Z", "likes_count": 0}, {"comment_by": "Kenn Tollens", "comment_text": "i&#39;m jumping in with my background in playing video games and I&#39;m totally clueless, but it sounds cool. So once you train one and it graduates kindergarten by counting to 10. Can another program program automatically have access to the knowledge? Does it have to learn every time or can it learn once and just know?", "comment_date": "2020-06-11T02:43:22Z", "likes_count": 0}, {"comment_by": "Bao Dinh", "comment_text": "The background music sounds familiar somehow, i think i may have heard it in BOTW", "comment_date": "2020-06-09T13:08:18Z", "likes_count": 0}, {"comment_by": "vulkanosaure", "comment_text": "im starting a long to come study on NN, and these videos are gold. I&#39;m gonna take paper and play around with minimal networks configuration to really get a deep understanding on the reason why it works. Also gonna read that online book you recommended in chapter 2.", "comment_date": "2020-06-07T11:34:55Z", "likes_count": 0}, {"comment_by": "Omkar Mali", "comment_text": "very good", "comment_date": "2020-06-07T05:38:27Z", "likes_count": 0}, {"comment_by": "Anna Nkldun", "comment_text": "Music in educational videos is the bane of my existence.", "comment_date": "2020-06-06T12:06:26Z", "likes_count": 0}, {"comment_by": "PenguinMaths", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=5m00s\">5:00</a> why do the indices go up to n-1? In the previous videos wasn&#39;t n the number of neurons used to represent the training data (784 in your example)? But here n is representing the number of weights connecting to the hidden layer (16)? If this is a just a new variable also named n, why does it count up to n-1 instead of just n? Great series by the way, this is incredibly valuable.<br>Edit: it seems n is the number of neurons in a particle layer, which in the next video he refers to as n_L for the number of neurons in the Lth layer which helps resolve the confusion of using the same name for different variables. And we index up to n-1 since it&#39;s zero indexed.", "comment_date": "2020-06-06T03:41:03Z", "likes_count": 0}, {"comment_by": "Guitarsquirrel", "comment_text": "Honestly after understanding the basics which exist since the 80s I cant blame ppl back than for thinking its going to solve everything, its freaking beautiful, its kind of like the search for a world formular", "comment_date": "2020-06-04T22:16:29Z", "likes_count": 0}, {"comment_by": "Mahim", "comment_text": "After watching this video:<br>Christopher Nolan wants to work with you for next project<br>Neil DeGrasse Tyson wants to study astrophysics from you", "comment_date": "2020-06-03T19:35:46Z", "likes_count": 0}, {"comment_by": "m0rjc", "comment_text": "&quot;This is where we were in the 80s/90s&quot; - I did my degree in the 90s, so this is the level I learned to. The more up to date stuff would be interesting.", "comment_date": "2020-06-03T18:08:37Z", "likes_count": 3}, {"comment_by": "S D", "comment_text": "Here I am practising Surgeon. Enjoying Every bit of your videos.", "comment_date": "2020-06-02T17:09:39Z", "likes_count": 0}, {"comment_by": "Krishna Mishra", "comment_text": "HELP !!!!! In RNN we have only 3 unique weight parameters, so during back prop. their will be only 3 parameters to update then why are RNN goes till the 1st input &amp; creates long term dependencies thereby creates vanishing gradient problem ????", "comment_date": "2020-05-28T14:52:04Z", "likes_count": 0}, {"comment_by": "Random Cat", "comment_text": "I&#39;ve been sitting for so long learning about machine...learning, my spine hurts. I may need some back propagation.", "comment_date": "2020-05-26T15:45:30Z", "likes_count": 0}, {"comment_by": "vagabond", "comment_text": "I started  programming with python one year ago. When it was just basic python and i thought omg this so easy whats the big fuss?<br>THIS THE BIG FUSS<br><br>ps thanks you make this wayyy easier to understand &lt;3", "comment_date": "2020-05-26T03:32:53Z", "likes_count": 0}, {"comment_by": "Gels", "comment_text": "Heeeyy First of all thank you so much for the lecture :D It really gave me a light on my bulb HAHAAH you get it hahahaah<br>Thank you soo muchhh :D<br><br>but may I ask, you mean by cost function in other words it&#39;s the  loss right? CAUSE I 100% THINK IT IS :D thank you", "comment_date": "2020-05-25T17:10:20Z", "likes_count": 0}, {"comment_by": "Hisham", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=6m55s\">6:55</a> I don&#39;t understand why it wants to dim the neurons with a negative weight. Why doesn&#39;t the output neuron just increase all the weights if it wants to increase its activation level?", "comment_date": "2020-05-24T05:47:18Z", "likes_count": 0}, {"comment_by": "Andrej Antevski", "comment_text": "Imas pozdrav od nikita :D", "comment_date": "2020-05-22T16:39:30Z", "likes_count": 0}, {"comment_by": "Vikash Chauhan", "comment_text": "please explain to me why it&#39;s the square of the difference for the cost function?", "comment_date": "2020-05-19T15:30:46Z", "likes_count": 0}, {"comment_by": "shaharm4", "comment_text": "Which visual editor did you use to show the back propagation?  Tensorflow? Photoshop?", "comment_date": "2020-05-18T06:12:42Z", "likes_count": 0}, {"comment_by": "Joey M", "comment_text": "Have you ever thought about how teaching machines runs parallel to teaching in general. That is, we tend to take for granted all the ways our brains fill in the gaps of our explanation until we have to bring another brain to the same level of understanding.", "comment_date": "2020-05-17T21:37:12Z", "likes_count": 0}, {"comment_by": "bubbles grappling", "comment_text": "as always really good, something that is a little confusing to me is how the weigths are being updated using the losses? which operations are actually happening on them?", "comment_date": "2020-05-16T22:06:34Z", "likes_count": 0}, {"comment_by": "Aditya Shukla", "comment_text": "Note to myself:<br>Aditya, if you&#39;re having trouble understanding, read this. <br><br>Scroll to <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=06m17s\">06:17</a>. Listen to what he&#39;s saying, &quot;In a sense, the neurons that are firing while seeing a 2, get more strongly linked to those firing when thinking about a two.&quot; Now, pause the video and listen.<br><br>All this is is just a fancy way of saying: when we show our model a picture of a handwritten 2 and we tell it, &quot;hey listen up this thing is a 2&quot; (like in our training set we have labels for our input pictures) and then we find out the activation units that have a say in influencing the hypothesis value of the 2 label, ie, the activation units which can heavily increase or decrease the value of the output unit for the label 2, we tell those activation units &quot;hey guys when you see something that resembles this thing, fire up the 2 label, ie increase the hypothesis value of the label for 2. Basically, we assign those activation units greater weights (or parameters) which influence the hypothesis value of the 2 label more (so that they can actually give us the right answer and say &quot;oh look this is probably a two&quot;.<br><br>I hope this helped and didn&#39;t complicate it further. If you don&#39;t get it, go over the video a few more times and review Andrew Ng&#39;s notes on this in the ML course on the Backpropagation lecture in week 5. Cheers, bro. Love you.", "comment_date": "2020-05-15T21:28:39Z", "likes_count": 40}, {"comment_by": "RSLak", "comment_text": "It is stunning how much better and clearer you can explain all this than my professor at university, and you are even a lot faster!<br>Thank you so much for producing all those brilliant videos. Some of some help me understand what my professors are trying to tell me, and some of them are just fascinating. Keep on going!", "comment_date": "2020-05-15T11:37:22Z", "likes_count": 0}, {"comment_by": "J Vincent", "comment_text": "If you could wrap up all of the above into a &#39;black box&#39; and then position it as one box in a network array (connected by similar mathematics) of many, many more similarly crafted &#39;black-boxes&#39;, then wouldn&#39;t that begin to approximate a brain (like a biological brain)? Then if you wrapped that approximate &#39;brain&#39; into a box inside an array of others.....<br><br><br>is that what is happening?<br><br><br>And, ultimately, isn&#39;t a bunch of people (or just their biological brains, to be precise) all connected on-line, in real-time via the internet (as slow as it is), just a form of a neural network? And, if it is, do we not then work-out problems in a similarly messy, &#39;to-and-fro&#39;, recursive way? IS this not the aim of collaboration (at the human level?).<br><br><br>I adore this stuff (and learning in general - any subject, anything ), so just thinking around the whole subject outside of the detailed maths inside the concepts, though, I love that stuff too. J", "comment_date": "2020-05-13T08:45:37Z", "likes_count": 0}, {"comment_by": "JJ", "comment_text": "Normally, I never comment on youtube videos, but I&#39;m making an exception for this one. Even though you might not see this, I want to thank you for your contribution to everyone&#39;s education. And for FREE access to it too. Amazing videos with amazing presentation, structure, and explanations. I&#39;m 3 years late, but please do keep up the great work!", "comment_date": "2020-05-12T19:57:14Z", "likes_count": 2}, {"comment_by": "Tuhin Mukherjee", "comment_text": "You&#39;re a genius and most importantly an amazing teacher", "comment_date": "2020-05-10T13:13:12Z", "likes_count": 0}, {"comment_by": "Ahoy Captain", "comment_text": "Will this help neurosurgeons?", "comment_date": "2020-05-09T07:14:40Z", "likes_count": 0}, {"comment_by": "Adeesh Devanand", "comment_text": "This video is extremely usefull and informative, watched it twice but couldn&#39;t have learned it faster anywhere else on the net. Thanks a lot", "comment_date": "2020-05-08T13:51:13Z", "likes_count": 0}, {"comment_by": "Sriram P", "comment_text": "Markus Persson? Notch is your Patron?", "comment_date": "2020-05-07T10:03:47Z", "likes_count": 0}, {"comment_by": "oldcowbb", "comment_text": "everyone: ramp<br>computer scientist: ReLU", "comment_date": "2020-05-05T07:03:18Z", "likes_count": 0}, {"comment_by": "Ashish Agarwal", "comment_text": "You are solidified love-for-math.", "comment_date": "2020-05-04T19:49:42Z", "likes_count": 0}, {"comment_by": "Vera Tsien", "comment_text": "This is a lifesaver for completing week 5 of Andrew Ng&#39;s Machine Learning course!! Been stuck at backpropagation for hours and finally found some clarity in this video. I love your channel! \u2764\ufe0f", "comment_date": "2020-05-02T02:45:29Z", "likes_count": 1}, {"comment_by": "Andreas Nikolaou", "comment_text": "Great job explaining. Born to teach, you have the touch. BEng Computing Grad. Found your video very useful for my course. May the force be with you ;)", "comment_date": "2020-04-30T14:26:31Z", "likes_count": 0}, {"comment_by": "S Prajapati", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=11m43s\">11:43</a><br>I was not in those 3<br>who else", "comment_date": "2020-04-28T11:07:44Z", "likes_count": 0}, {"comment_by": "S Prajapati", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m22s\">9:22</a><br>Everyone&#39;as got that how is that possible. <br>Really", "comment_date": "2020-04-28T11:04:40Z", "likes_count": 0}, {"comment_by": "Daniel Wahl", "comment_text": "Numby food for one&#39;s imagination.", "comment_date": "2020-04-27T22:32:28Z", "likes_count": 0}, {"comment_by": "Prem Banker", "comment_text": "So elegantly and beautifully explained this a complicated algorithm called Backpropogation. The intution You have given is so majestic.", "comment_date": "2020-04-27T19:56:16Z", "likes_count": 0}, {"comment_by": "user", "comment_text": "Maybe it&#39;s a silly question, but:<br>How do we make the network know which target value, which is 0 to 9 in the MNIST dataset, is which in the output layer?", "comment_date": "2020-04-25T18:44:20Z", "likes_count": 0}, {"comment_by": "user", "comment_text": "Thank you for this amazing video too. I now understand what this famous backpropagation is.<br><br>Btw, irrelevant to the subject:<br><a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=2m39s\">2:39</a> &quot;32 times greater than&quot;. Or &quot;32 times as (31 times greater than)?", "comment_date": "2020-04-25T18:35:05Z", "likes_count": 0}, {"comment_by": "Ahammad Shawki", "comment_text": "At what age you start learning machine learning and deep learning?", "comment_date": "2020-04-23T04:13:45Z", "likes_count": 0}, {"comment_by": "Junaid Faruqui", "comment_text": "bro .. the real magic is how effectively you make such a hard subject seem understandable to a layperson .. bless you", "comment_date": "2020-04-21T16:26:29Z", "likes_count": 0}, {"comment_by": "Molten Guava", "comment_text": "This series is beautifully mindblowing.", "comment_date": "2020-04-20T22:24:39Z", "likes_count": 0}, {"comment_by": "Jo\u00e3o Pedro Moreira de Oliveira", "comment_text": "What&#39;s the book&#39;s name? I was focused on the video and now I can&#39;t find it \ud83e\udd7a", "comment_date": "2020-04-20T10:05:44Z", "likes_count": 0}, {"comment_by": "ddu dduru", "comment_text": "Correct me if I&#39;m wrong.<br><br><br>So as I understand, if I have an ith hidden layer, then I consider how the weights in all the other layers affect this hidden layer. I find how the weights with respect to this ith hidden layer should be nudged.<br><br><br>I repeat this process for all the other layers.<br><br><br>Am I correct?", "comment_date": "2020-04-18T04:43:16Z", "likes_count": 0}, {"comment_by": "ddu dduru", "comment_text": "God Every time the wix ad comes up, I just turn off the sound. So annoying", "comment_date": "2020-04-18T04:23:05Z", "likes_count": 0}, {"comment_by": "Miguel G. S\u00e1nchez Weckx", "comment_text": "Hi, just greeting here on my way to this man&#39;s patreon.", "comment_date": "2020-04-17T10:01:46Z", "likes_count": 1}, {"comment_by": "Aaron Ge", "comment_text": "Wait, so backpropagation really only guarantees identifying a local minimum? How does one then find the absolute minimum?", "comment_date": "2020-04-15T18:44:18Z", "likes_count": 0}, {"comment_by": "Bernardo B.", "comment_text": "Thank you so much for these videos. Really, the way you teach is absolutely amazing. Grateful for having such amazing content here in Youtube. I was wondering, do we want to reach the global minimum for the Cost function or the Local seems always fine?", "comment_date": "2020-04-14T02:45:07Z", "likes_count": 0}, {"comment_by": "Fahd Ciwan", "comment_text": "the more number of times I watch this, the more easy it gets to digest the concept...", "comment_date": "2020-04-12T20:16:13Z", "likes_count": 0}, {"comment_by": "MADHUKUMAR S 17BEE0117", "comment_text": "Why can&#39;t you collaborate with ben eater and work on building breadboard perception or stochastic gradient descent hardware circuit?", "comment_date": "2020-04-12T19:21:42Z", "likes_count": 0}, {"comment_by": "Divyanshu Tiwari", "comment_text": "I&#39;m just starting out with Deep learning this quarantine and man...it&#39;s a bit difficult to get my head around these concepts.", "comment_date": "2020-04-12T13:23:24Z", "likes_count": 7}, {"comment_by": "Binayak Thakur", "comment_text": "i am trying to get my feet wet into machine learning you videos are really making me to learn more and more", "comment_date": "2020-04-10T19:15:32Z", "likes_count": 0}, {"comment_by": "Js Nf", "comment_text": "\u201cSongbird of our generation\u201d lmao \ud83d\ude02<br>Your animations are amazing!", "comment_date": "2020-04-10T08:43:38Z", "likes_count": 0}, {"comment_by": "Justin Yang", "comment_text": "The visualization is insame", "comment_date": "2020-04-09T21:28:06Z", "likes_count": 0}, {"comment_by": "Sahaj Singh", "comment_text": "Wow man gotta salute your work on the animation", "comment_date": "2020-04-08T00:22:07Z", "likes_count": 0}, {"comment_by": "Yorick", "comment_text": "I cannot overstate how much I wish I had these videos during my machine learning course a few years back", "comment_date": "2020-04-07T13:20:06Z", "likes_count": 0}, {"comment_by": "L. Jin", "comment_text": "How could any moron find a reason to downvote this amazing video?", "comment_date": "2020-04-05T17:37:11Z", "likes_count": 0}, {"comment_by": "Kylen Williams", "comment_text": "10/10 for the correct pronunciation of asterisks.<br><a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=6m36s\">6:36</a>", "comment_date": "2020-04-03T02:24:16Z", "likes_count": 0}, {"comment_by": "NonTwinBrothers", "comment_text": "I like the music at the end!", "comment_date": "2020-03-31T20:40:54Z", "likes_count": 0}, {"comment_by": "Abhimanyu Shekhawat", "comment_text": "I can&#39;t tell you how much I appreciate this. This is so interesting, I am watching this as the first thing in the morning. I really resonate with your ideology that math is not a dry subject, it is just being taught in a dry way. If this is what Math looks like, I would consider getting a Ph.D. in Maths. Please keep the videos coming.", "comment_date": "2020-03-30T03:54:18Z", "likes_count": 0}, {"comment_by": "Jan Mikicki", "comment_text": "I have a question. If we look at the sixth neuron at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m00s\">7:00</a> you can see that it&#39;s bright but also has a negative weight attached to it. We want to increase the weight because the neuron is currently bright but we also want to dim the neuron because the weight is currently negative. Wouldn&#39;t it lead to some sort of contradiction and a pointless update of the values?", "comment_date": "2020-03-29T15:40:04Z", "likes_count": 0}, {"comment_by": "Shashi Bhattad", "comment_text": "Dude deserves a Emmy for teaching Deep learning.. If there is an Emmy for teaching :)", "comment_date": "2020-03-29T14:25:30Z", "likes_count": 0}, {"comment_by": "Quase Nerd", "comment_text": "Brilliant! Thanks<br><br><br><a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=10m36s\">10:36</a> Summint it up<br>for those who wants to review the video", "comment_date": "2020-03-29T13:05:59Z", "likes_count": 2}, {"comment_by": "Daryoush", "comment_text": "Where does the b (bias) come in? What is the b (bias) for? in the same simple terms that you explained.", "comment_date": "2020-03-25T00:13:39Z", "likes_count": 0}, {"comment_by": "matthew cintron", "comment_text": "your vids are so good i honestly feel like its the closest thing to having that port in the matrix where they plug you in and boom end of your video ..... I KNOW KUNG FU XD)", "comment_date": "2020-03-22T11:53:09Z", "likes_count": 1}, {"comment_by": "Urbano Medina Martinez", "comment_text": "These videos are simply amazing. Thanks for the work!", "comment_date": "2020-03-17T21:07:35Z", "likes_count": 0}, {"comment_by": "Shashwat", "comment_text": "U are GOD", "comment_date": "2020-03-11T12:16:18Z", "likes_count": 0}, {"comment_by": "Julian Gonzales", "comment_text": "Implementing a neural network from scratch for a class project and this series helped me so much to get started. Thanks!!!", "comment_date": "2020-03-10T04:07:57Z", "likes_count": 0}, {"comment_by": "chari Muvilla", "comment_text": "I&#39;m so tempted to use a genetic algorithm instead of all this", "comment_date": "2020-03-09T23:01:05Z", "likes_count": 1}, {"comment_by": "TheShermanTanker", "comment_text": "It pains me as a Biology student to see people think Artificial Neural Networks work exactly like the real ones... Thank you for clarifying that. I&#39;m sick and tired of having to explain the Real Neurons don&#39;t simply just receive a signal and pass it to new layers .-.", "comment_date": "2020-03-09T06:04:19Z", "likes_count": 0}, {"comment_by": "Taher Poonawala", "comment_text": "How does stochastic gradient descent work? For example, If you have a thousand data points and divide them into batches of ten. You&#39;re still running the gradient descent algorithm on all the thousand data points ?", "comment_date": "2020-03-06T10:16:57Z", "likes_count": 0}, {"comment_by": "David Wilkie", "comment_text": "You could make a tentative association of &quot;neurons that fire together, wire together&quot; in resonance/principle, a Timing-spacing coordination vector in brain circuitry that modulates = associates, interference positioning patterns in some degree of chemical compounding. (Basic QM-TIMESPACE Principle)<br><br>Quality Control over the labelling of data is the very best reason for AI.", "comment_date": "2020-03-03T17:25:47Z", "likes_count": 0}, {"comment_by": "Siddhesh Misale", "comment_text": "Humbly owe my whatever little NN understanding to this genius of a person.", "comment_date": "2020-02-25T19:51:31Z", "likes_count": 0}, {"comment_by": "Back To Back SWE", "comment_text": "Bless your soul to its deepest depth", "comment_date": "2020-02-25T19:11:54Z", "likes_count": 0}, {"comment_by": "Shanur Rahman", "comment_text": "Any 25yr old here ?", "comment_date": "2020-02-23T18:53:00Z", "likes_count": 0}, {"comment_by": "Joel Shodamola", "comment_text": "I am definitely recommending this to my colleagues. Diffused the &quot;almighty&quot; neural network explicitly. now it seems like a puny god.", "comment_date": "2020-02-22T17:29:49Z", "likes_count": 0}, {"comment_by": "Player 1", "comment_text": "I&#39;m not smart, but the more I learn about Neural Networks, the more problems I see with it. In terms of where we want neural networks to end up, it seems like we can do a lot better.", "comment_date": "2020-02-21T22:37:35Z", "likes_count": 0}, {"comment_by": "L.M\u306e\u304a\u6d12\u843d\u306aChannel", "comment_text": "extremely well made high quality material, every animation, every transition perfectly timed with the explanation. priceless", "comment_date": "2020-02-19T18:04:26Z", "likes_count": 0}, {"comment_by": "\u0414\u0430\u043d\u0438\u0438\u043b \u0417\u0443\u0435\u0432", "comment_text": "Actually, artificial neural networks are very not like biological ones. They are devoid of many unnesessary complications and developmental stages. Because of this traditional software for creating neural networks is not fit for modelling biological ones.", "comment_date": "2020-02-19T14:25:08Z", "likes_count": 0}, {"comment_by": "Praison Joshua", "comment_text": "i need a free tshirt <br>someone send me <br>+91 9538790077", "comment_date": "2020-02-18T04:32:09Z", "likes_count": 0}, {"comment_by": "Heskan", "comment_text": "phew", "comment_date": "2020-02-16T16:49:51Z", "likes_count": 0}, {"comment_by": "Avi Mehenwal", "comment_text": "It\u2019s the voice of God ....", "comment_date": "2020-02-14T00:41:45Z", "likes_count": 0}, {"comment_by": "J\u0101nis Strods", "comment_text": "Does the NN regulate the weights and biases after each training input or only after the average is calculated? I&#39;m unfortunately on 3 hours of sleep rn so I might have missed that if it was explained.", "comment_date": "2020-02-12T16:02:49Z", "likes_count": 0}, {"comment_by": "Lina Palacios", "comment_text": "nice...", "comment_date": "2020-02-11T10:54:27Z", "likes_count": 0}, {"comment_by": "Drew Hahn", "comment_text": "This is too well taught to be human.. convinced this video was created by an ML algorithm (back prop really did its job..)", "comment_date": "2020-02-11T08:53:19Z", "likes_count": 0}, {"comment_by": "\ub9d0\ub791\uc774", "comment_text": "oh my fucking god, I understood this back propagation thing that I couldn&#39;t understand for a week with just a freakin one video. WTF u teach so well and ur video is amazing... True genius!!", "comment_date": "2020-02-10T10:28:45Z", "likes_count": 1}, {"comment_by": "Anand M Cherian", "comment_text": "Ignoring the notations and index chasing was a smart move. Made the understanding a lot more easier.", "comment_date": "2020-02-06T13:47:06Z", "likes_count": 0}, {"comment_by": "Jemima", "comment_text": "This should be art.<br>I don&#39;t normally comment on videos but I feel I must express my gratitude and appreciation for your amazing explanations and animations! Visualisation is so important and helpful and you have nailed it beautifully!", "comment_date": "2020-02-02T14:09:03Z", "likes_count": 2}, {"comment_by": "Sri Krishnan Balagopalan", "comment_text": "I don&#39;t exactly understand the stochastic gradient descent part. Let&#39;s say your training data set has a million examples, the perfect way to train it, will be to compute the cost and backpropagate for every single training example. Since that takes a long time, you say that we divide it into mini batches, say of 100, and find the cost for each training example in a mini batch and average out the cost, and backpropagate once per mini batch with the averages as the changes to be done. <br><br>So essentially, instead of back propagating for every training example with its cost, we back propagate once per mini batch with the average of all the costs in the mini batch. So we only save time on reducing number of back props, and the time on computing the cost for each training example remains the same. <br><br>Am I explaining this correctly?", "comment_date": "2020-02-02T04:49:37Z", "likes_count": 0}, {"comment_by": "Leelahs Playhouse", "comment_text": "Verify your electrical circuits on the go!  Encounter: androidcircuitsolver/app.html", "comment_date": "2020-01-26T19:34:31Z", "likes_count": 0}, {"comment_by": "Yaswant Sai Krishna Pullela", "comment_text": "\u2764\ufe0f", "comment_date": "2020-01-25T23:56:17Z", "likes_count": 0}, {"comment_by": "Yatin Patel", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=1m56s\">1:56</a> How 13002? dimensions?", "comment_date": "2020-01-18T04:30:05Z", "likes_count": 0}, {"comment_by": "anas .2k", "comment_text": "Hi, I have a quetion, apparently we dont have an analytical formula of the cost function we just have some realisation of it, so how you can menimise a function that you dont know exactly it&#39;s formula ? thank you &lt;^-^&gt;", "comment_date": "2020-01-14T13:57:28Z", "likes_count": 0}, {"comment_by": "Masahiro Ueno", "comment_text": "excellent video", "comment_date": "2020-01-14T05:44:06Z", "likes_count": 0}, {"comment_by": "Martin Martin", "comment_text": "I really like how you focus on providing an intuitive understanding! However, I think you could have done even better by choosing a small network with say 2 neurons per layer and demonstrate how this really rolls out for certain value example; so really showing the weights and bias values on each edge and the input and output values and deltas on each neuron. Nevertheless, this is the best introduction to backpropagation that I have seen so far!", "comment_date": "2020-01-11T15:09:07Z", "likes_count": 1}, {"comment_by": "a", "comment_text": "Thank you so much for this! Honestly didn&#39;t expect myself to understand the idea as well as I did after watching this video.", "comment_date": "2019-12-27T18:40:33Z", "likes_count": 0}, {"comment_by": "CH D", "comment_text": "I could hardly concentrate on what you were speaking", "comment_date": "2019-12-19T07:37:37Z", "likes_count": 0}, {"comment_by": "Steven Smith", "comment_text": "So once you take the derivative of each function. Do you just subtract every element of the weight matrix with the derivative and start the next sample I&#39;m confused", "comment_date": "2019-12-18T23:38:01Z", "likes_count": 0}, {"comment_by": "Wesley Neill", "comment_text": "Thanks for the great videos. I&#39;m looking forward to trying to implement some of this.", "comment_date": "2019-12-18T20:14:44Z", "likes_count": 0}, {"comment_by": "foo bar 167", "comment_text": "video 1: 5,930,983 views and 152k likes and 1.3 dislikes<br>video 2: 2,266,166 views and 50k likes and 394 dislikes<br>video 3: 1,511,127 views and 27k likes and 246 dislikes<br>video 4:   910,574 views and 18k likes and 144 dislikes<br>Views coefficients: v1/v2 = 2.62; v2/v3 = 1.50; v3/v4 = 1.66<br>Likes coefficients: l1/l2 = 3.04; l2/l3 = 1.85; l3/l4 = 1.5<br>Dislikes coefficients: d1/d2 = 3.30; d2/d3 = 1.60; d3/d4 = 1.7<br>Thus number of haters drops faster than number of likers. However, at the beginning number of likers drops faster than number of viewers, but at the end likes become more stable. So those who like these videos continue to view, while haters goes away. Also who view the 2nd video usually go to the 3rd, but skip the 4th.<br>    Yes, I&#39;m nerd!", "comment_date": "2019-12-18T09:54:13Z", "likes_count": 1}, {"comment_by": "rassim simou", "comment_text": "Good", "comment_date": "2019-12-16T15:09:09Z", "likes_count": 0}, {"comment_by": "Levan LRS", "comment_text": "such a great work on visualization and explaining! Nice job!", "comment_date": "2019-12-11T20:43:55Z", "likes_count": 0}, {"comment_by": "Kevin Oduor", "comment_text": "ill try using excel to do this", "comment_date": "2019-12-07T22:50:37Z", "likes_count": 0}, {"comment_by": "Salamanka", "comment_text": "Thanks", "comment_date": "2019-12-07T04:59:15Z", "likes_count": 0}, {"comment_by": "Kshitij Kale", "comment_text": "And this my friend is the true essence of democracy!", "comment_date": "2019-12-05T16:49:38Z", "likes_count": 0}, {"comment_by": "chethan pai", "comment_text": "Explain the math behind slinking neural network", "comment_date": "2019-12-05T14:58:50Z", "likes_count": 0}, {"comment_by": "SploxLabs - Ambiences", "comment_text": "Superb Quality!", "comment_date": "2019-12-03T03:40:08Z", "likes_count": 0}, {"comment_by": "Pedro Rodrigues", "comment_text": "Thank you!", "comment_date": "2019-12-02T00:17:05Z", "likes_count": 0}, {"comment_by": "Ayush Chaudhary", "comment_text": "Amazing animations and explanation!", "comment_date": "2019-12-01T08:04:51Z", "likes_count": 0}, {"comment_by": "Emperor Cyber", "comment_text": "you are a legend", "comment_date": "2019-11-29T04:56:23Z", "likes_count": 0}, {"comment_by": "Mengfei Zhang", "comment_text": "Machine learn but Human train.", "comment_date": "2019-11-26T14:13:42Z", "likes_count": 0}, {"comment_by": "prathyusha", "comment_text": "This video is a gem!", "comment_date": "2019-11-23T14:44:22Z", "likes_count": 0}, {"comment_by": "Inverted 311", "comment_text": "Thankfully God keeps all of our minds safe...<br>This is current, kinda an invasion of the human psyche when put into play here with this information. <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5660637/\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5660637/</a>", "comment_date": "2019-11-22T13:53:50Z", "likes_count": 1}, {"comment_by": "erwin z", "comment_text": "really good explained. i must admit. this is a deep refresh. will restudy your channel tomorrow. my eyes are glued i falllzzzzzzzzzzzzzzzzzzzz", "comment_date": "2019-11-20T00:14:07Z", "likes_count": 0}, {"comment_by": "Lizi Moody Specter", "comment_text": "This is the shit I&#39;m talking about. I am currently in a class of application of deep learning in finance, alpha modelling, to be precise. I&#39;m sure the professor doens&#39;t quite understand it very well.  You are the man.", "comment_date": "2019-11-12T19:38:08Z", "likes_count": 0}, {"comment_by": "Prasanth Suresh", "comment_text": "Anyone else smiling through all of his videos because you&#39;re understanding so much so well like never before? &lt;3", "comment_date": "2019-11-12T00:07:23Z", "likes_count": 327}, {"comment_by": "Curious Bit", "comment_text": "It would be great if you can do a video on notation!", "comment_date": "2019-11-11T11:07:39Z", "likes_count": 0}, {"comment_by": "calvinthedestroyer", "comment_text": "Love your videos, now to tech my microwave how to cook for me...", "comment_date": "2019-10-28T03:30:51Z", "likes_count": 0}, {"comment_by": "lapidations", "comment_text": "What if you scold your computer differently? Like, instead of making it chase a 00010000 kind of last layer, allow it to be in doubt between two options. For instance, using the result of the error the trained network made, create a second label for each data set, that means &quot;what else does this number looks like, besides what it is&quot;. Then retrain the network allowing it to have a &quot;second most probable&quot; number guess. If you consider the second guess as also valid, could we reach 99,99% accuracy?", "comment_date": "2019-10-24T22:04:00Z", "likes_count": 0}, {"comment_by": "Brock Obama", "comment_text": "I hope someday you realize the significance your videos have on this generation.", "comment_date": "2019-10-24T06:08:12Z", "likes_count": 0}, {"comment_by": "Philip Cho", "comment_text": "this is definitely the best video i have ever found regarding machine learning so far.", "comment_date": "2019-10-19T22:50:33Z", "likes_count": 0}, {"comment_by": "Linsu Han", "comment_text": "thank. you.", "comment_date": "2019-10-15T00:55:13Z", "likes_count": 0}, {"comment_by": "Nathan X.", "comment_text": "Bruh, you literally saved my life", "comment_date": "2019-10-14T03:37:30Z", "likes_count": 0}, {"comment_by": "Super Idra", "comment_text": "THANK YOU.", "comment_date": "2019-10-12T22:24:04Z", "likes_count": 0}, {"comment_by": "Taksh Mandiratta", "comment_text": "COOL", "comment_date": "2019-10-11T11:01:13Z", "likes_count": 0}, {"comment_by": "\u5ed6\u4e1c\u6d77", "comment_text": "could you display your code for each vedio? thanks", "comment_date": "2019-10-09T14:00:50Z", "likes_count": 0}, {"comment_by": "Nadin Ibrahim", "comment_text": "Thank you for your video! What do you use for your animations?", "comment_date": "2019-10-03T05:10:13Z", "likes_count": 0}, {"comment_by": "meh", "comment_text": "I can not thank you enough for this video. It&#39;s easy to understand and very well made. Thanks to you i will now be able to make a decent AI. :)  I am lucky to have found your channel this early in life.", "comment_date": "2019-09-27T18:00:55Z", "likes_count": 1}, {"comment_by": "Mad Scientist", "comment_text": "The free shirt offer expired :(", "comment_date": "2019-09-27T13:12:42Z", "likes_count": 0}, {"comment_by": "mabrams Blythedale", "comment_text": "1.3 million views ... wow.  I&#39;m kind of blown away by how many people are interested and following this series.  That&#39;s far more than I would have guessed!  Great!", "comment_date": "2019-09-25T14:54:17Z", "likes_count": 0}, {"comment_by": "Steven Reubenstone", "comment_text": "This is one of the most extraordinary teaching examples I&#39;ve ever seen, and should be modeled in education systems all over the world.", "comment_date": "2019-09-23T17:40:00Z", "likes_count": 0}, {"comment_by": "Paulo Petersen", "comment_text": "Mate you have a gift to teach. Well done :)", "comment_date": "2019-09-23T11:55:58Z", "likes_count": 0}, {"comment_by": "Kenny Omg", "comment_text": "Now I wanna write a dystopian book where AI takes over the world without us knowing called &quot;The Gradient Descent&quot;.", "comment_date": "2019-09-13T11:50:14Z", "likes_count": 0}, {"comment_by": "Edward Cox", "comment_text": "why is that man labeled Tease?", "comment_date": "2019-09-13T05:18:48Z", "likes_count": 0}, {"comment_by": "Taylor Short", "comment_text": "When you get excited about the free t-shirt then realise this has been out for 2 years...", "comment_date": "2019-09-11T03:13:13Z", "likes_count": 0}, {"comment_by": "Adel Ghaenian", "comment_text": "That was &quot;Brilliant&quot;. You&#39;ve answered tons of my questions man. thanks alot.", "comment_date": "2019-09-10T15:09:31Z", "likes_count": 0}, {"comment_by": "Sugito x", "comment_text": "NOBODY LECTURING IN A HALL TRIES TO TALK OVER A LOAD OF BACKGROUND MUSIC! SO, WHY DO YOU DO IT IN YOUR VIDEOS ??? YOU&#39;RE TALKING TO THE WORLD HERE, AND NOT EVERYONE SPEAKS AMERICAN! YOUR ACCENT, AND THE SPEED AT WHICH YOU SPEAK ALREADY PUTS SOME LISTENERS  AT A DISADVANTAGE. TO A BEGINNER, THIS SUBJECT ITSELF IS COMPLICATED, AND REQUIRES EFFORT TO GRASP. SO, THE <b>LAST</b> THING ONE NEEDS IS TO BE ADDITIONALLY STRUGGLING AGAINST A LOT OF UNNECESSARY AND  DISRUPTIVE RHYTHM, AND BACKGROUND NOISE!", "comment_date": "2019-09-10T07:35:36Z", "likes_count": 0}, {"comment_by": "M faraday", "comment_text": "You are awesome,best animation,good knowledge,strong concept,may be computer scientist and etc...<br>But i am not  understanding much<br>What is wrong with me", "comment_date": "2019-09-06T09:35:03Z", "likes_count": 1}, {"comment_by": "Lyaman Agabekova", "comment_text": "Thank you for the explanation. Amazing channel!", "comment_date": "2019-09-02T16:59:42Z", "likes_count": 0}, {"comment_by": "Bat YOK", "comment_text": "You must be wicked smart! Cause the way you explain things is just awesome", "comment_date": "2019-09-01T21:20:37Z", "likes_count": 0}, {"comment_by": "Zachary Thatcher", "comment_text": "What kind of resources do you need to apply neural networks and AI to researching other fields? I know that learning is computationally expensive, but if I am doing image object recognition on a large dataset, is there any description of time or space complexity as a function of the size of the dataset or the number of levels/nodes in a neural network?", "comment_date": "2019-08-27T15:03:27Z", "likes_count": 0}, {"comment_by": "R. D. Machinery", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=11m51s\">11:51</a> I think there&#39;s a mistake in one of the equations. You have z superscript L superscript L instead of just z superscript L.", "comment_date": "2019-08-25T12:06:56Z", "likes_count": 0}, {"comment_by": "mixbaal0", "comment_text": "Wittgestein said something like &quot;What we can talk about, we can say it clearly, otherwise is better to keep silence&quot;. You talk very, very clear. Congratulations!", "comment_date": "2019-08-22T03:44:39Z", "likes_count": 0}, {"comment_by": "Tianze Wang", "comment_text": "So, how could we make a donation?", "comment_date": "2019-08-15T01:30:57Z", "likes_count": 0}, {"comment_by": "Charlie Angkor", "comment_text": "once I took the description of visual system by Hubel and Wiesel and programmed a neural network to simulate it and it was amazing. Then I used the tricks I learned from the brain to program a language cortex. That is even more amazing because I just flush down a language corpus without even telling it what encoding it is in or where the word boundaries are and it learns all the phrases words suffixes prefixes by itself. How do I figure out what I programmed when I dont understand the neural network theory at all? Like how its called and stuff? I find it boring to study and a lot of mental effort. I am lazy to decipher the ideas from the mathematical formulae.", "comment_date": "2019-08-14T20:50:44Z", "likes_count": 0}, {"comment_by": "Sanwal Yousaf", "comment_text": "The song in the background reminds me of the theme song of CityVille, the Facebook game from 2010.", "comment_date": "2019-08-13T03:32:15Z", "likes_count": 0}, {"comment_by": "generaldefence", "comment_text": "I use Brave rewards on you man you&#39;re that worth it :D", "comment_date": "2019-08-09T22:29:03Z", "likes_count": 0}, {"comment_by": "Phil Myday", "comment_text": "Wonderful videos! You would have been my favorite teacher in education.", "comment_date": "2019-08-09T06:56:12Z", "likes_count": 1}, {"comment_by": "Vitalijs Fescenko", "comment_text": "Not drunk, just stochastic", "comment_date": "2019-08-07T14:53:59Z", "likes_count": 4}, {"comment_by": "DowzerWTP72", "comment_text": "I&#39;m forcing myself to watch all these videos before going on to write my own code to do this! I&#39;ve got some awesome ideas that I can&#39;t wait to implement, plus in my next year of University, I will be doing computer vision and machine learning, so having this under my belt will be really great! I just wanna start programming though!!", "comment_date": "2019-08-06T23:01:15Z", "likes_count": 0}, {"comment_by": "Muhammad Sarim Mehdi", "comment_text": "Quite frustrating I have to end up here on this guy&#39;s channel. I don&#39;t like his video format (what&#39;s up with this weird music as if this is some super mysterious stuff? It actually turns away a lot of people and I don&#39;t think he realizes that he is actually making it look way more complicated than it should be). However, I don&#39;t see any proper video on backpropagation so I have to make use of this. But still unliking because of the poor presentation of rather simple ideas. Why do people not use a simple pen and paper approach as should be the case. Weird animations like these just scare a lot of people away.", "comment_date": "2019-08-01T19:09:48Z", "likes_count": 0}, {"comment_by": "Jyoti Prasad Pal", "comment_text": "You are great....", "comment_date": "2019-07-30T15:54:29Z", "likes_count": 0}, {"comment_by": "graph egg species", "comment_text": "is a cost function the same as a loss function?", "comment_date": "2019-07-23T16:30:47Z", "likes_count": 0}, {"comment_by": "user 53503", "comment_text": "Love this channel", "comment_date": "2019-07-13T13:50:12Z", "likes_count": 0}, {"comment_by": "Ajay Hemanth", "comment_text": "very good explanation ...", "comment_date": "2019-07-11T09:51:09Z", "likes_count": 0}, {"comment_by": "Raver", "comment_text": "I love these moments in maths or other sciences, where you have this one singular moment of pure enlightenment. You got me at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m37s\">7:37</a> ....I got goose bumps when the first column of &quot;+&quot; appeared. This is just brilliant.", "comment_date": "2019-07-11T02:42:17Z", "likes_count": 0}, {"comment_by": "Deepak", "comment_text": "I like that pi.", "comment_date": "2019-07-08T18:00:34Z", "likes_count": 0}, {"comment_by": "Ryan Denziloe", "comment_text": "The stochastic gradient descent analogy of the quickly stumbling drunk man is just perfect.", "comment_date": "2019-07-07T17:08:58Z", "likes_count": 0}, {"comment_by": "timkolm2", "comment_text": "@3Blue1Brown Why was there no explanation on how the biases should be adjusted?<br>Otherwise great series!", "comment_date": "2019-07-03T17:34:22Z", "likes_count": 0}, {"comment_by": "Zangruver", "comment_text": "I remember seeing this a year back and not really understanding what is going on. I have recently started Andrew Ng&#39;s Machine Learning course and now this video feels a lot clearer :D", "comment_date": "2019-07-01T12:44:46Z", "likes_count": 1}, {"comment_by": "Little Narwhal", "comment_text": "I mean, in practice, you probably wont even use bath, stochastic, or mini batch gradient descent but a more efficient optimization algorithm like conjugate descent, BFGS, and other algorithms. I would love it if you could make videos on how these work cause i never bothered to try and understand them, and just use them...", "comment_date": "2019-06-27T11:54:20Z", "likes_count": 0}, {"comment_by": "TheOnlyKeksSuchti", "comment_text": "You&#39;re saving my presentation! I didn&#39;t know how to explain neural networks to my fellow students but now I know how to do it! Thanks mate!", "comment_date": "2019-06-24T20:44:13Z", "likes_count": 1}, {"comment_by": "XaRuCu", "comment_text": "I don&#39;t understand one thing. He told that we needed to modify the outputs of the last hidden layer, but then talks about a gradient with a size of 13,000. This would include the weights that go from the last hidden layer to the output layer. Moreover, if we do this recursively, we would arrive to the input layer, where we now have to modify the weights and bias. So I think that the gradient should only have 784x16 elements", "comment_date": "2019-06-23T23:10:00Z", "likes_count": 0}, {"comment_by": "Peter Yeh", "comment_text": "Great explanation and animation", "comment_date": "2019-06-23T22:16:57Z", "likes_count": 0}, {"comment_by": "Gustavo Oliveira", "comment_text": "This is guy is amazing! Can&#39;t stop watching! What a phenomenal teacher! Thank you again and again! I not even working today, just watching these classes :)", "comment_date": "2019-06-20T19:07:48Z", "likes_count": 1}, {"comment_by": "Zia Steele", "comment_text": "Can quantum delayed choice speed up backpropagation in neural networks?", "comment_date": "2019-06-19T23:26:30Z", "likes_count": 0}, {"comment_by": "Active Geometry", "comment_text": "great video. Looking forward to seeing more of your videos", "comment_date": "2019-06-17T17:15:47Z", "likes_count": 0}, {"comment_by": "MakerInTheMaking", "comment_text": "Thank you for these great tutorials! You make something complicated seem logical and fun, definitely fuels my interest for learning more!", "comment_date": "2019-06-17T04:19:24Z", "likes_count": 0}, {"comment_by": "Mathew 13:45-46", "comment_text": "sO FAR, we ALSO keep our mistakes as a reference...............", "comment_date": "2019-06-13T18:14:29Z", "likes_count": 0}, {"comment_by": "Peter", "comment_text": "I imagined the nodes as flexible water balloons and the connections as fixed size pipes! This illustrates the change of the pipe sizes and their effect a little better in my mind. Making one pipe bigger means greater flow of water to a certain balloon making it fill up with more water, getting bigger.<br><br>I also started imagining this neural network of fun balloons with just 2 layers; the first, &quot;input balloons&quot; (each pixel a different sized water balloon based on how lit it was by the example), and second 10 &quot;output balloons&quot; representing the numbers 0-9. So imagine 784 water balloons connected to 10 other water balloons with same sized pipes and water flowing to the output balloons when you squeeze the input balloons. Now fill up the input balloons with water based on how lit a pixel is from an example and then squeeze all the water out of the first layer into the second, the result would be that the 10 output balloons would have the same amount of water in each balloon. Now imagine changing the pipe sizes to allow more water to go to certain output balloons that better representing the number that matches the example, then squeeze all the water back and do it again. You end up with the balloon that represents the number in the example the biggest balloon! A very crude model (i.e combination of pipe sizes) that at least could get that specific example number with some success. It can&#39;t do much else. <br><br>Think about balloons with water being squeezed back and forth between the layers and how sizes of the pipes would change where the water ends up. Kinda fun.<br><br>Now how could you improve on this to guess multiple example numbers and not just one? We probably need to have more control over where the water from the input balloons flows to the output balloons. An easy way to do this is to add more pipes and strategically link only certain balloons with these pipes. BUT like 784 pipes weren&#39;t enough, imagine say doubling this amount. You will have 784*10 * 2, which comes to 15,680 pipes...thats a lot of pipes. And even if we used math to calculate the best pipe combinations, that would already be more calculations than the 13k or so as explained in the video. <br><br><br>Is there a more efficient solution?<br><br>How can we add more control but not go crazy with the number of pipes we need to control?<br><br>Knowing that we only have 784 input balloons and have to end up with 10 output balloons, we can create more pipes by adding another layer of balloons in between them, with say 16 balloons - my imagination starts to get a little hazy around here - we connect the input balloons to this middle layer and the middle layer to the output balloons. Water flows from the first to the second giving us a whole bunch of pipe sizes we can play with and then another set between the second and third. The middle balloons kind of group the input balloons pipes into just 16. I feel like it&#39;s kinda summarizing or zooming out.<br><br>It&#39;s super hard to imagine so many pipe connections, I find it MUCH EASIER thinking of an example with smaller number of balloons, like a grayscale image of a traffic light that can be expressed in just 6 pixels or so with say 3 balloons in the middle, and 1 output balloon that tells you to STOP if the balloon is FULL or go if its empty.... or something like that.<br><br>I&#39;ve probably screwed the examples up, but wanted to share because it helped me understand it a little better. Hope it helps someone.<br><br>btw these videos are so awesome! thank you so much!!!", "comment_date": "2019-06-13T01:09:45Z", "likes_count": 0}, {"comment_by": "\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u041d\u0435\u0432\u0441\u043a\u0438\u0439", "comment_text": "How about rights of use the results of machine learning? Imagen the possibilities for big market players to predict how to kill (economically) all the concurrency from side of small players just to get their bigger market parts? They need just to predict for how much time they must keep damping prices and that it. And I without mashing learning can predict that they will do this with accuracy 100%. What about using of those technologies for something like this which is no in theory forbidden by the low, but we all know what can do businessmen to get dominance and authority. How about rights for using data of face recondition and moving and all other those statistical data of people what certainly want to be stored.  How we will keep democracy and power of low and equality for all people in this new world? Or I am crazy and nobody thinks about this?<br>Are we as society ready for this changes? What about politics? They tell only about dominance bot not about freedom and equality", "comment_date": "2019-06-12T10:37:45Z", "likes_count": 0}, {"comment_by": "Ansh Mehta", "comment_text": "After <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=6m42s\">6:42</a>, does it mean that the weights and biases in the hidden layer do not change but only the activation value changes by changing the weights and biases from the first to second layer??", "comment_date": "2019-06-09T10:15:48Z", "likes_count": 0}, {"comment_by": "Auxowave", "comment_text": "Note: backpropagation is not biologically plausible", "comment_date": "2019-06-08T17:46:08Z", "likes_count": 0}, {"comment_by": "\u00c1tila Couto", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m00s\">9:00</a> Eureka moment. I cried.<br>* eagerly waiting for a video about convolutional neural networks *<br>At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m25s\">12:25</a>, he labels Fermat as a tease \ud83d\ude02", "comment_date": "2019-06-08T17:07:48Z", "likes_count": 1}, {"comment_by": "Suvir Misra", "comment_text": "This guy is a genius.", "comment_date": "2019-06-08T16:25:17Z", "likes_count": 0}, {"comment_by": "Colin Byerly", "comment_text": "Thank you , well done !", "comment_date": "2019-06-06T19:35:39Z", "likes_count": 0}, {"comment_by": "Sergio Garc\u00eda", "comment_text": "Eres increible meu te quiero", "comment_date": "2019-06-03T10:42:33Z", "likes_count": 0}, {"comment_by": "Shade", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m05s\">8:05</a> - that&#39;s where I don&#39;t get the idea of the back proapagation method. The net shows the wrong results, right -- but how can we be of any assertiveness that it wasn&#39;t just the last layer&#39;s fault? What&#39;s more, why should the whole net should be pushed in one direction? <br>For example, let&#39;s imagine the net &quot;decided&quot; to do what you suggested it could do in the first video: one layer picks out geometry and the other analyzes this geometry&#39;s locations relative to each other. Then, a mistake in determining the right geometry positions for the desired result has in general nothing to do with detecting geometry itself. Vice versa, if the net doesn&#39;t detect geometry very good, positioning it correctly will still yield some wrong results and adjusting these both the same way might be detrimental. And we <b>are</b> changing both layers in the same way instead of making these do better work on their own. How and why does this work?", "comment_date": "2019-06-03T03:59:47Z", "likes_count": 0}, {"comment_by": "Ye Pan", "comment_text": "Amazing video with funny animation.", "comment_date": "2019-05-30T09:50:33Z", "likes_count": 0}, {"comment_by": "Jacob Lepley", "comment_text": "Imagine a neural network learning how to make a neural network.... Woah....", "comment_date": "2019-05-28T23:17:23Z", "likes_count": 0}, {"comment_by": "Kohinoor Basu", "comment_text": "Mind blown.  Now I under why you need training data for ML.  Thank you...", "comment_date": "2019-05-27T22:10:58Z", "likes_count": 0}, {"comment_by": "Chris", "comment_text": "Came for the knowledge, stayed for the Animations.<br><br>Here I&#39;m thinking how it can be done in After Effects, and turned out it&#39;s custom engine!", "comment_date": "2019-05-26T18:04:21Z", "likes_count": 22}, {"comment_by": "Vikas Jha", "comment_text": "This is such a treasure, your channel. I love it, huge respect for your work.", "comment_date": "2019-05-25T10:03:06Z", "likes_count": 1}, {"comment_by": "Jonathan Grynspan", "comment_text": "this is the best description of neural networks by a landslide", "comment_date": "2019-05-19T15:15:09Z", "likes_count": 0}, {"comment_by": "moji487", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=1m30s\">1:30</a> there shouldn&#39;t be w13,001, should there?", "comment_date": "2019-05-18T22:22:33Z", "likes_count": 0}, {"comment_by": "Neel Deshpande", "comment_text": "When AI finally surges into power and becomes an indomitable force, it&#39;ll be grateful to you.", "comment_date": "2019-05-17T16:10:15Z", "likes_count": 0}, {"comment_by": "decrepitCrescendo", "comment_text": "Do the pi-people have names?", "comment_date": "2019-05-16T23:18:40Z", "likes_count": 0}, {"comment_by": "Nisha George", "comment_text": "Please make a video for Convolutional Neural Networks!!!! <br>You are an amazing teacher! Thank you for these videos.", "comment_date": "2019-05-16T21:39:09Z", "likes_count": 0}, {"comment_by": "Vinnie vinnie", "comment_text": "Good video! I&#39;m liked and share 983 times :D", "comment_date": "2019-05-11T17:30:47Z", "likes_count": 0}, {"comment_by": "42", "comment_text": "Can i use different activation functions per layer, even within one layer?", "comment_date": "2019-05-11T14:26:45Z", "likes_count": 1}, {"comment_by": "Probhakar Sarkar", "comment_text": "please make video on kalman filter", "comment_date": "2019-05-11T05:55:03Z", "likes_count": 0}, {"comment_by": "Neggplant Chen", "comment_text": "This is the best videos about Neural Networks I have everseen.", "comment_date": "2019-05-04T01:53:52Z", "likes_count": 0}, {"comment_by": "Rajarshi Sahu", "comment_text": "Awesome explanation.. i have a doubt.. How do u decide the number of neurons in the hidden layer?? is it by trial and error??.. what if i increase the number of hidden layers ?? will it increase the efficiency of the network?? and also how it will affect the system when the number of neurons in the hidden layer is increased or decreased?", "comment_date": "2019-05-03T05:25:32Z", "likes_count": 0}, {"comment_by": "Haojia Li", "comment_text": "Amazing video ! Really really helpful!", "comment_date": "2019-05-02T02:26:15Z", "likes_count": 0}, {"comment_by": "Evyn T", "comment_text": "Back-prop replaces formal education. Transhuman. &quot;I know kungfu&quot;", "comment_date": "2019-05-01T08:00:51Z", "likes_count": 0}, {"comment_by": "Fun Love and Cooking", "comment_text": "Here is another very nice tutorial with step by step Mathematical explanation and full coding.<br><br><br><a href=\"http://www.adeveloperdiary.com/data-science/machine-learning/understand-and-implement-the-backpropagation-algorithm-from-scratch-in-python/\">http://www.adeveloperdiary.com/data-science/machine-learning/understand-and-implement-the-backpropagation-algorithm-from-scratch-in-python/</a>", "comment_date": "2019-04-30T02:34:59Z", "likes_count": 0}, {"comment_by": "Jeff Neuser", "comment_text": "Incredible.  So visual and detailed.  The more visual and more detailed the better!", "comment_date": "2019-04-28T00:12:20Z", "likes_count": 0}, {"comment_by": "Luke the Giant", "comment_text": "Help. I have question. Is it true that the gradient gets computed for for a training example and that the average of the costs function is only for measurement?", "comment_date": "2019-04-24T15:52:09Z", "likes_count": 0}, {"comment_by": "Bartosz K.", "comment_text": "Sorry for a depth and detailed analysis, but shouldn&#39;t the colors of the arrows (related to the amount of desired change) appearing at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m37s\">7:37</a> be in an opposite color (for digit outputs 0, 1, 3, ..., 9 i.e. last layer)?", "comment_date": "2019-04-22T13:19:34Z", "likes_count": 5}, {"comment_by": "Ayush", "comment_text": "how did you create those animations?", "comment_date": "2019-04-20T04:12:25Z", "likes_count": 0}, {"comment_by": "UPSC IAS \u0939\u093f\u0928\u094d\u0926\u0940", "comment_text": "Sir I subscribed you. Please tell me how do you create videos.", "comment_date": "2019-04-17T16:44:04Z", "likes_count": 0}, {"comment_by": "DrMantisToboggan", "comment_text": "Thanks, this is super helpful. I studied back prop in grad school and after rewinding a few times, I think I understand now. I&#39;m ready to tackle the math now. Really great explanation.", "comment_date": "2019-04-17T02:56:55Z", "likes_count": 0}, {"comment_by": "AB", "comment_text": "Hey Grant. Could you do a video on intuition of vanishing and exploding gradients ?", "comment_date": "2019-04-12T05:38:49Z", "likes_count": 0}, {"comment_by": "AB", "comment_text": "I want to see this in IMAX !!", "comment_date": "2019-04-11T04:50:58Z", "likes_count": 0}, {"comment_by": "hupa1a", "comment_text": "This explained it great! Thank you!", "comment_date": "2019-04-10T21:06:35Z", "likes_count": 0}, {"comment_by": "Iorek", "comment_text": "This is awesome, thanks man!", "comment_date": "2019-04-05T23:48:10Z", "likes_count": 0}, {"comment_by": "FJmus1C", "comment_text": "I finally understood it :D Thanks a lot!", "comment_date": "2019-04-04T08:08:15Z", "likes_count": 0}, {"comment_by": "Brice Chivu", "comment_text": "I might misunderstand but maybe not. At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m38s\">7:38</a>, we want to reduce the activation of the neuron responsible for 3 right? So we should decrease the activation of the neurons that have a positive weight to 3, no? In the video, it&#39;s actually the opposite. For example, the first neuron has a positive weight with the neuron responsible for 3, so we should decrease its activation right?<br>Can someone help me on that please?", "comment_date": "2019-03-29T01:48:34Z", "likes_count": 2}, {"comment_by": "\u80e1\u5b89\u5566", "comment_text": "OMG! You are the legend! Saves my homework", "comment_date": "2019-03-27T08:56:07Z", "likes_count": 0}, {"comment_by": "David Swygart", "comment_text": "I am a neuroscientist, and all your comparisons of actual neural networks to artificial neural networks seem pretty spot on to me.  This is a great video series.", "comment_date": "2019-03-26T18:43:02Z", "likes_count": 0}, {"comment_by": "Son In Cheong", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m25s\">12:25</a> Love how Fermat is labelled as &quot;Tease&quot;", "comment_date": "2019-03-26T18:12:15Z", "likes_count": 3}, {"comment_by": "Sahad Zahir", "comment_text": "Just wondering, is there a minor error at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m07s\">4:07</a>? The probabilities don&#39;t add up to 1 for classification in this &#39;bad&#39; network?", "comment_date": "2019-03-19T13:44:01Z", "likes_count": 0}, {"comment_by": "U", "comment_text": "\uc81c\ubc1c \ud55c\uad6d\uc5b4 \ucd94\uac00\ud574\uc918", "comment_date": "2019-03-13T18:03:45Z", "likes_count": 0}, {"comment_by": "karan jathoul", "comment_text": "Thank you sir. You are a brilliant teacher. You made such a complicated topic a piece of cake. %Respect..<br>Thanks again.", "comment_date": "2019-03-12T23:17:50Z", "likes_count": 0}, {"comment_by": "weerobot", "comment_text": "Chubbyemu watched this....this is what happened to his Brain...", "comment_date": "2019-03-12T17:08:48Z", "likes_count": 0}, {"comment_by": "80 KG", "comment_text": "where is the &quot;Join&quot; button?", "comment_date": "2019-03-12T11:26:23Z", "likes_count": 0}, {"comment_by": "Ibrahim alshubaily", "comment_text": "Thank you for the great videos!", "comment_date": "2019-03-11T13:57:43Z", "likes_count": 0}, {"comment_by": "Stefan Stankovi\u0107", "comment_text": "I love the fact that I can watch these videos on mute and still get what I came here for. :D Great job!", "comment_date": "2019-03-11T12:00:34Z", "likes_count": 0}, {"comment_by": "Paul Bloemen", "comment_text": "1. The (activation) value of a neuron should be between 0 and 1, or? ReLu has a leaking minimum around 0, shouldn&#39;t ReLu have also a (leaking) maximum around 1?<br><br>2. Is there one best activation function, delivering the best neural network with the least amount of effort, like the amount of tests needed, and computer power?<br><br>3. Should weights and biases be between 0 and 1 or between -1 and 1? Or any different values?<br><br>4. Against vanishing and exploding gradients: can this be prevented with a (leaking) correction minimum and maximum for the weights and biases? There would be some symmetry then with the activation function suggested in the first paragraph.", "comment_date": "2019-03-03T15:45:15Z", "likes_count": 0}, {"comment_by": "loveld Lu", "comment_text": "Let&#39;s hear it for Lindsey she did the right thing to find that girl she was so disrespectful in such a way that you do not talk to your boss and let them know how they should handle their business you&#39;re there to represent the brand and be the best of your ability and achieved goal with the bread needs we don&#39;t need your personal opinion your sloppy mess has your disrespectful and starting trouble<br>Her father failed her gave us the wrong tools and telling her to behave in such a matter and think that that makes her being a strong woman and respectful therefore his father feel him to pop or introduce him to the right way to handle yourself in the real world so it comes right back down to his daughter who&#39;s going to be a failure in the real world she is incapable of being strong she is weak so raw arrive for Lindsey continue on boss lady continue on a million blessings to you because it was disrespectful how dare you bring somebody&#39;s pass into their face you are weak person you never be straw you want to be real go to an Island by yourself be real you think you&#39;re perfect you think above everybody else on this planet know you know better your sinner just like everyone else so you hide behind your Cockiness are you disrespectful so this is not you so you not really real", "comment_date": "2019-02-26T14:49:14Z", "likes_count": 1}, {"comment_by": "Rixing Wu", "comment_text": "Super helpful", "comment_date": "2019-02-26T04:19:01Z", "likes_count": 0}, {"comment_by": "Richard Zipper", "comment_text": "Really nicely done.  I would love to see a video on the application of the backprop on the weights and a step-by-step walkthrough with a small real example.  This series has been very helpful !!!", "comment_date": "2019-02-25T18:49:55Z", "likes_count": 0}, {"comment_by": "Aio Project", "comment_text": "finally starting to understand this years later", "comment_date": "2019-02-24T09:37:17Z", "likes_count": 0}, {"comment_by": "Kevin Gray", "comment_text": "Thank you so much for the quality background music in your videos. So much better than the ubiquitous upbeat ukulele.", "comment_date": "2019-02-18T23:34:10Z", "likes_count": 0}, {"comment_by": "merril almeida", "comment_text": "probably by far the best intuitive video", "comment_date": "2019-02-17T13:16:30Z", "likes_count": 0}, {"comment_by": "zachos 2000", "comment_text": "but how do you decide whether to change the activations of the neurons or the weights connecting them???", "comment_date": "2019-02-11T16:16:19Z", "likes_count": 1}, {"comment_by": "Neigee Pierrot", "comment_text": "Crazy that neurons are capable of this", "comment_date": "2019-02-10T05:36:08Z", "likes_count": 0}, {"comment_by": "Neigee Pierrot", "comment_text": "So incredible can\u2019t believe we can actually visualize this", "comment_date": "2019-02-10T05:33:14Z", "likes_count": 0}, {"comment_by": "Ihsees91", "comment_text": "Graphics are on point!", "comment_date": "2019-02-09T11:02:33Z", "likes_count": 0}, {"comment_by": "zachos 2000", "comment_text": "I don&#39;t get this, im stumbling around and can&#39;t visualize it, I thought I was the smart one, im usually the one my classmates goes to for help, my world is crumbling around me, im a fraud.", "comment_date": "2019-02-08T23:46:07Z", "likes_count": 0}, {"comment_by": "k", "comment_text": "This is my favorite anime", "comment_date": "2019-02-04T13:34:45Z", "likes_count": 0}, {"comment_by": "Gustavo Exel", "comment_text": "But when you flatten (or squish?) these &quot;rows&quot; into a single line, you are losing information. If a one, for example, is moved sideways, our best hope is that the NNet just remember the most common translations. It is sort of taking the pixels out of context.", "comment_date": "2019-02-02T04:49:35Z", "likes_count": 0}, {"comment_by": "satwik", "comment_text": "what software you used for the graphics", "comment_date": "2019-02-01T17:19:15Z", "likes_count": 0}, {"comment_by": "dehou Qian", "comment_text": "very amazing!  very impressive\uff01", "comment_date": "2019-02-01T15:17:50Z", "likes_count": 0}, {"comment_by": "Vincent Oostelbos", "comment_text": "So backpropagation only works when you have a very clearly defined desired output for a given trial, right? Since that is required to come up with the cost function that is at the basis of it all. But you don&#39;t always have this clear definition of what you want from a neural network, I think, or at least not in a way that is easy to give to the network as feedback at each time step. For example, I coded a very simple feedforward neural network with a single hidden layer to make decisions for simulated creatures in a simulated ecosystem. There&#39;s no clear &#39;right&#39; move for any creature at any point, but they behave, and based on the rules of the simulation they compete for resources, reproduce (with a mutation to the weights and biases) if they succeed and die if they fail.<br><br>This led me to wonder which sorts of problems would have a clear cost function associated to them that would allow for backpropagation, and which ones would require another approach, such as the evolutionary approach that I described above. (Or yet other approaches, perhaps? And then what might those be?) If anyone has any thoughts on this, I would be happy to read them.", "comment_date": "2019-01-31T22:00:16Z", "likes_count": 0}, {"comment_by": "CV M", "comment_text": "This is the best video on machine learning I have seen so far. Thank you for making this complexity as simple as possible.", "comment_date": "2019-01-30T06:46:27Z", "likes_count": 1}, {"comment_by": "Robbie Smith", "comment_text": "&quot;Sorry! That offer has expired.\r&quot; regarding the free t-shirt", "comment_date": "2019-01-29T21:05:27Z", "likes_count": 0}, {"comment_by": "Bip901", "comment_text": "My neural network hurts...<br>I&#39;ll let it sleep and keep training it on your videos tomorrow.<br>Thank you!", "comment_date": "2019-01-29T20:36:16Z", "likes_count": 0}, {"comment_by": "Dev deal", "comment_text": "you are the best!!!!", "comment_date": "2019-01-29T18:08:20Z", "likes_count": 0}, {"comment_by": "Nova Super", "comment_text": "Great visualization!!  Thank you.", "comment_date": "2019-01-29T05:07:19Z", "likes_count": 0}, {"comment_by": "Rami Cohen", "comment_text": "Great great explanations and animations, thanks a lot for all these, you enlight so many people and deserve to feel huge satisfaction !", "comment_date": "2019-01-28T22:35:08Z", "likes_count": 0}, {"comment_by": "Subrat Das", "comment_text": "All comments are 1 year ago. Felt..I am late. Hope ,some peoples are also learning at this time @2019<br>Animations  are killer for understanding.And they are damn good at it.<br><br>One Random Thought : What does mean by 3Blue1Brown!!!", "comment_date": "2019-01-26T13:42:32Z", "likes_count": 0}, {"comment_by": "Jonas Eira", "comment_text": "This video is great! Thanks for sharing!", "comment_date": "2019-01-26T09:05:11Z", "likes_count": 0}, {"comment_by": "\u0421\u0435\u0440\u0433\u0435\u0439", "comment_text": "I think this network lacks output value &quot;not a cipher&quot;.<br>With this value all dimensions of an input will be covered.", "comment_date": "2019-01-26T03:22:38Z", "likes_count": 0}, {"comment_by": "saurabh kumar", "comment_text": "that is a great video presentation you made.<br>thank you.<br>but can you tell me the software you used for presentation making???", "comment_date": "2019-01-19T08:59:55Z", "likes_count": 0}, {"comment_by": "Well Chiu", "comment_text": "Easy to understand but amazing!", "comment_date": "2019-01-19T08:52:41Z", "likes_count": 0}, {"comment_by": "Jan Golda", "comment_text": "Well done!<br>You are referenced in official NVIDIA NN Course :D", "comment_date": "2019-01-19T08:06:01Z", "likes_count": 0}, {"comment_by": "Swap Nayak", "comment_text": "You talked about dividing the image into small edges and loops. I don&#39;t think you did that", "comment_date": "2019-01-17T09:08:40Z", "likes_count": 0}, {"comment_by": "Parsa Rahimi", "comment_text": "I&#39;m confused how you backpropagate a minibatch, you explained the backpropagation for 1 value at a time. I don&#39;t get how you can backpropagate a bunch of variables together.<br>Also, you eventually hit the starting layer or input layer, well we know what the input is so you can&#39;t change the ai value for that layer, what do you have to say about that? -Actually, I found the answer to this one, you&#39;re just creating the ai variables to keep the track of how much the values at those nodes should change.", "comment_date": "2019-01-16T20:05:27Z", "likes_count": 0}, {"comment_by": "Najia Ahmadi", "comment_text": "Hi, Thanks for the nice videos. A small suggestion: whenever you refer to another related video, it would be better if you put a link of it on the screen that one can easily click and find that video. An alternative would be to properly name the title of the mentioned video.", "comment_date": "2019-01-11T16:00:28Z", "likes_count": 1}, {"comment_by": "Luis David Pascual Callejo", "comment_text": "I had to stop seeing your video for a moment because I needed to write down my admiration for your work. It is outstanding the amount of intelligence, communication skills, careful graphic design and passion in very single frame of your videos. Congratulations and thank you for your contribution to maths education. See you in patreon!", "comment_date": "2019-01-10T20:28:48Z", "likes_count": 0}, {"comment_by": "nandan kumar", "comment_text": "@<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m32s\">4:32</a> shouldn&#39;t the sigmoids of all the units in the output layer sum to one, no matter how &quot;trashy&quot;, the initial weights are. Since, sigmoid being essentially a probability function, its sums must add to one, isnt it?", "comment_date": "2019-01-10T10:54:03Z", "likes_count": 0}, {"comment_by": "Edvin Holm", "comment_text": "Still great", "comment_date": "2019-01-08T16:30:15Z", "likes_count": 0}, {"comment_by": "Matthew Kleinmann", "comment_text": "First off, thank you for the videos.  My head is still hurting, but that is not a bad thing.  I can see going over the three of them again a few more times.  Some questions have popped into my head while watching them, if you have a moment I would appreciate any input you may have.<br><br>First, we have a goal of taking a grid that has essentially pixel values, and we want to identify that as a number.  Is this neural network approach faster than taking the input and comparing it to each piece of test data, assigning each one a &quot;hit ratio&quot; if you will, and much like your solution in the end, you have a few choices, but hopefully one stands out.  Brute force but easy math.  <br><br>Second in your solution, it was interesting when you showed the graphic representations of what the network was looking for and they in fact looked all random.  Would it be possible to hand plug in idealised examples of what you are really looking for, and back calculate the rest from there?  I think that would make human training of the model possible.  As it is in this case, I don&#39;t see how a human can train the model short of backpropagation.  That may be hard in more subtle areas.  Numbers are easy, but say separating things where one distinct charestitic sets them apart.  <br><br>When I get a chance I am going to download the ebook and give the demo a shot.  Noise2noise got me interested in this but the computational requirements and the raw size of the training datasets is more than I can deal with as a home experimenter.  The network for the numbers I can probably deal with on much more modest hardware.", "comment_date": "2019-01-06T04:44:15Z", "likes_count": 0}, {"comment_by": "Atharva S", "comment_text": "is more intermediate layers and neurons per layer always good? ie does it always lead to improvement in confidence, learning ,,even tho computation gets expensive?", "comment_date": "2019-01-05T10:37:09Z", "likes_count": 0}, {"comment_by": "Kanva", "comment_text": "Guys QUICK QUESTION-<br>How does the layer second from the last know whether to increase or decrease its sigmoid value to change the weights that is affecting it?<br><br>I am a class 10 student and i have not studied calculus yet... i had this one doubt and if i get an answer to it, i will pretty much understand the whole thing. Thank you<br>Edit: If have read this comment, sorry to have your time wasted, cz i understood that part by carefully watching the episode again. YAY!! now i get it", "comment_date": "2018-12-27T05:03:32Z", "likes_count": 1}, {"comment_by": "Shengchuang Feng", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m37s\">7:37</a>, shouldn&#39;t the activations associated with the digit 0 neuron (whose activation should be decreased) change the opposite direction?  For example, if the neuron at the very top of the second-to-last layer increases activation, with a positive weight, the digit 0 neuron&#39;s activation would increase. Is that correct?", "comment_date": "2018-12-25T04:46:13Z", "likes_count": 1}, {"comment_by": "nxidplease", "comment_text": "Hey I love your videos, but there&#39;s one thing I can&#39;t completely understand. When you train the network by calculating the output error and then adjusting the weights and biases, how do you determine which proportion of the nudge should be done by each of these components(weights, biases and previous layer activation)?", "comment_date": "2018-12-24T06:10:29Z", "likes_count": 0}, {"comment_by": "FinalRockstar", "comment_text": "Why do we need to shuffle data?", "comment_date": "2018-12-22T18:06:46Z", "likes_count": 0}, {"comment_by": "Sky Shoesmith", "comment_text": "Ok so I don&#39;t get the calculus (part 4 as it were), and I&#39;m only an A level maths student (English year 12), but this is enough that I&#39;m actually using back prop now, even if it&#39;s amateurish. Once I have a firmer grasp on back prop, matrix maths and calculus, I&#39;ll go further and make my code more efficient, and for now this has been a great starting point!", "comment_date": "2018-12-21T18:54:34Z", "likes_count": 0}, {"comment_by": "Riccardo Damiani", "comment_text": "Really great videos, very educational, needless to say. Just a little suggestion: Wouldn&#39;t it be less confusing to call the function subject to minimization &quot;error function&quot; instead of &quot;cost function&quot;?", "comment_date": "2018-12-17T08:57:06Z", "likes_count": 1}, {"comment_by": "Helvidion", "comment_text": "Thank you so much for you visual teaching, it really helps me to understand math related problems", "comment_date": "2018-12-13T21:36:35Z", "likes_count": 0}, {"comment_by": "Moshfiqur Rahman", "comment_text": "Your voice is so soothing. Your videos and animations in it are amazing.", "comment_date": "2018-12-07T11:15:57Z", "likes_count": 0}, {"comment_by": "High Voltage", "comment_text": "Has anyone mentioned Andrew Ng&#39;s free course on Coursera concerning the subject?", "comment_date": "2018-12-07T09:08:56Z", "likes_count": 0}, {"comment_by": "Wild Urbex", "comment_text": "I thank you very much for posting these tutorials. It helps a lot! Kind regards!", "comment_date": "2018-12-06T20:48:48Z", "likes_count": 0}, {"comment_by": "SRIJAN SRIVASTAV", "comment_text": "what a great video!", "comment_date": "2018-12-02T12:41:11Z", "likes_count": 0}, {"comment_by": "John C Gibson", "comment_text": "It&#39;s when your mother spank you for running with scissors or next to the street almost getting yourself killed. Changing your behavior is what it is.", "comment_date": "2018-12-02T01:35:14Z", "likes_count": 0}, {"comment_by": "Forbidden Crystal Internet", "comment_text": "NYER <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=6m07s\">6:07</a>", "comment_date": "2018-12-01T20:55:28Z", "likes_count": 0}, {"comment_by": "Eli Mead", "comment_text": "Why can&#39;t I hit the like button more than once?", "comment_date": "2018-12-01T07:47:38Z", "likes_count": 0}, {"comment_by": "CH D", "comment_text": "ok... this goes in a little weird since I am enjoying looking at the animation more than the topic!!", "comment_date": "2018-11-23T13:56:53Z", "likes_count": 0}, {"comment_by": "Mirla Monta\u00f1o", "comment_text": "as always, good job! I was legit impressed by some stuff I saw on this video. It&#39;s so well explained and graphic, thanks so much for this content!", "comment_date": "2018-11-20T04:15:19Z", "likes_count": 0}, {"comment_by": "THE TNT MINECART", "comment_text": "when making oir ow neural network how would be know the which is the brightest neuron?", "comment_date": "2018-11-17T05:38:25Z", "likes_count": 0}, {"comment_by": "Vighnesh Nayak S", "comment_text": "Amazing and simple.", "comment_date": "2018-11-15T03:37:03Z", "likes_count": 0}, {"comment_by": "Temi Segun", "comment_text": "this video is extremely helpful, thank you so much for this content", "comment_date": "2018-11-13T17:22:18Z", "likes_count": 0}, {"comment_by": "\uc774\uc740\uacbd", "comment_text": "I really love youtube!!\u2665", "comment_date": "2018-11-13T13:17:12Z", "likes_count": 0}, {"comment_by": "Freak", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=11m39s\">11:39</a>, it seems the code uses several transposes, especially for the weights, but I don&#39;t really understand where this comes from. Because in your video you said that dz/da = W, and not W.transpose.<br>I&#39;ve seen the same thing in Welch Labs channel, but it doesn&#39;t explain it either. Can anybody explain it please?<br><br>Thanks.", "comment_date": "2018-11-10T18:55:28Z", "likes_count": 0}, {"comment_by": "Jonny H.", "comment_text": "Respect. Crystal clear. Insta like and subscribe.", "comment_date": "2018-11-08T04:53:50Z", "likes_count": 0}, {"comment_by": "Angela Nikolova", "comment_text": "I am writing my internal assesment for school about neural networking and thank god your channel exists cuz everything about this subject is now clear to me. You explanations are perfect!", "comment_date": "2018-11-02T10:53:47Z", "likes_count": 0}, {"comment_by": "Leland Reardon", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m00s\">9:00</a>, when talking about averaging the preferences of all the samples, am I correct in observing that to prevent incentivizing one value over others we need a well-distributed set of sample data (i.e. not 1/2 of them are 2)?", "comment_date": "2018-11-01T03:36:21Z", "likes_count": 0}, {"comment_by": "Christoffer Jonsson", "comment_text": "Okay, I get how this works, what I don&#39;t understand is exactly what the network does when you give it a number and it predicts it. Lets say we give it a hand-drawn 4 after it&#39;s done training, does the network recognize how a the weights and biases were when it was given a 4 and then changes it into that? or the network stay as it was and it can recognize the 4 anyway?, if so how?", "comment_date": "2018-10-31T20:11:47Z", "likes_count": 0}, {"comment_by": "Maxime G", "comment_text": "Woah, thanks !", "comment_date": "2018-10-27T19:17:15Z", "likes_count": 0}, {"comment_by": "GetSchwifty.", "comment_text": "Geez, what platform did you use to get these animations?! It is simply wonderful!", "comment_date": "2018-10-24T19:00:49Z", "likes_count": 0}, {"comment_by": "Bahram Dolati", "comment_text": "Amazing animation, excellent explanations, and very intuitive content. Thank you very much.", "comment_date": "2018-10-23T19:11:20Z", "likes_count": 0}, {"comment_by": "Mechan Chen", "comment_text": "\u611f\u8c22\u89c6\u9891\u5236\u4f5c\u8005\uff0c\u611f\u8c22\u7ffb\u8bd1\u8005\uff0c\u6211\u624d\u80fd\u5b66\u4e60\u8fd9\u4e9b\u8fc8\u5411\u672a\u6765\u7684\u77e5\u8bc6", "comment_date": "2018-10-22T15:35:48Z", "likes_count": 0}, {"comment_by": "Abhishek Kashyap", "comment_text": "What happens if for the same problem, i increase or decrease the number of hidden layers? <br>(since the nn isn&#39;t exactly following the process we initially set it. i.e Segments- shapes- digits)", "comment_date": "2018-10-21T08:57:27Z", "likes_count": 0}, {"comment_by": "\ub9e5\uadf8\uc2a8", "comment_text": "I&#39;ve been wondering for a while.<br><br>Existing General Learning Methods<br>Input value: 00 -&gt; 0 Learning the label Input value: 01 -&gt; 1 Learning the label<br>00 Learning 01 Degrees 0 Labeled<br>01 Learned at learning 00 degrees with 1 label<br>Results = Thinking about yourself and learning about other situations with your own label.<br>If you do this, you will eventually learn 00, 01,<br>Problem = For example, if you have a total of 1000 numbers of 1 to 9 data and 100 of each, then you have to have a total of 1000 data.<br>In this case, 30 pieces of numerical data 1 to 9 data are added. If you learn only the added data, then all the weights of the 1000 data you have already learned<br>It becomes like a mess with a window and the data flew away.<br>So if you add 30, you need to re-learn a total of 1,300 numbers from 1 to 9 data.<br>1000 is learned, 300 is added and 1300 is added. The amount of learning is 1000 + 1300 = 2300 due to re-learning.<br>And once you learn, you have to save all the data and labels you have learned, not the end.<br><br>The conclusion is that it does not matter if you already set the data and learn it, but I think it is hard to learn additional data.<br>Is there a solution to this problem?", "comment_date": "2018-10-19T12:39:39Z", "likes_count": 0}, {"comment_by": "Asheesh Bhuria", "comment_text": "That cursor is awesome, and the video too!", "comment_date": "2018-10-17T16:20:34Z", "likes_count": 0}, {"comment_by": "Dani Walmsley", "comment_text": "I feel like as an 16 year old art student I shouldnt understand this yet it just clicks bravo", "comment_date": "2018-10-16T23:07:59Z", "likes_count": 0}, {"comment_by": "Gsoo Linda", "comment_text": "\u5982\u5750\u6625\u98ce", "comment_date": "2018-10-12T13:33:24Z", "likes_count": 0}, {"comment_by": "Vladimir Georgiev", "comment_text": "I wonder will all that work properly if you rotate the number let&#39;s say 45 degree", "comment_date": "2018-10-05T13:26:00Z", "likes_count": 0}, {"comment_by": "Jonathan Barlow", "comment_text": "How do you figure out how much to nudge the bias vs. weight?  Most of your examples show nudging the weight and not the bias.", "comment_date": "2018-10-04T06:51:19Z", "likes_count": 0}, {"comment_by": "Craig Wood", "comment_text": "Unbelievably beautiful explanation and graphics.", "comment_date": "2018-09-26T00:30:12Z", "likes_count": 0}, {"comment_by": "yashen taher", "comment_text": "you are doing gods work", "comment_date": "2018-09-25T21:57:06Z", "likes_count": 0}, {"comment_by": "Paul Young", "comment_text": "Just getting my head around AI/ML  .. the thought I had last night is why do the academics make this so hard .. in the end we will just embrace the concepts and libraries and build something ..  your series makes sense of it ... thanks", "comment_date": "2018-09-21T08:10:08Z", "likes_count": 0}, {"comment_by": "Christophe Peytier", "comment_text": "Very inspiring.. please continue these videos", "comment_date": "2018-09-20T17:39:09Z", "likes_count": 0}, {"comment_by": "Akash Dixit", "comment_text": "I think you have mistaken Stochastic Gradient Descent process. The mini batches process you explained happen in Batch Gradient Descent.<br>Batch Gradient Descent &gt;&gt; Derivative is calculated and weights are updated after each batch.<br>Stochastic Gradient Descent &gt;&gt; Derivation is calculated for each training instance and update happen instantly. That is why Stochastic Gradient Descent is much faster.", "comment_date": "2018-09-19T13:07:27Z", "likes_count": 1}, {"comment_by": "i hate google plus", "comment_text": "Are you meant the adjust the biases, weights, and previous values all at once? So say if a neuron has to be brighter,  you lower the bias, increase the weight, and increase its previous value all at the same time?", "comment_date": "2018-09-17T21:03:40Z", "likes_count": 0}, {"comment_by": "Tun\u00e7 K\u0131ral", "comment_text": "epic epic epic ...", "comment_date": "2018-09-16T18:58:26Z", "likes_count": 0}, {"comment_by": "MegaShadoworld", "comment_text": "Can you just be a replacement for all of my teachers? you teach better than all of them.", "comment_date": "2018-09-14T19:52:21Z", "likes_count": 0}, {"comment_by": "Mattshu", "comment_text": "I&#39;m just gonna stare at this video and hope that learning by osmosis is a real thing", "comment_date": "2018-09-14T12:57:20Z", "likes_count": 0}, {"comment_by": "Jill Cai", "comment_text": "This video is amazing!  Thank you!", "comment_date": "2018-09-08T15:26:01Z", "likes_count": 0}, {"comment_by": "TheRandomMan210", "comment_text": "You are the best", "comment_date": "2018-09-08T07:22:44Z", "likes_count": 0}, {"comment_by": "joshisushant", "comment_text": "your videos are insanely amazing, and perhaps THE best source to learn any mathematical concept. We are extremely grateful that you take so much time and effort to create such a fantastic content.", "comment_date": "2018-09-07T18:14:35Z", "likes_count": 0}, {"comment_by": "Filipe Rebollo", "comment_text": "Amazing!!", "comment_date": "2018-09-04T21:59:54Z", "likes_count": 0}, {"comment_by": "Filipe Rebollo", "comment_text": "It will take a few year to digest everything auhauhua", "comment_date": "2018-09-04T21:58:53Z", "likes_count": 0}, {"comment_by": "Manoj J", "comment_text": "watching this series fired up a gazzilion neurons in my brain that were never activated on listening to my university lectures on the same topic!! thanks a lot sir :)", "comment_date": "2018-09-04T15:11:59Z", "likes_count": 0}, {"comment_by": "Saurabh", "comment_text": "Grant, if you ever stop making such beautiful illustrations, my interest in Math and ML will die a very painful death.", "comment_date": "2018-09-02T13:16:08Z", "likes_count": 0}, {"comment_by": "Deep Learning", "comment_text": "Very interessting....respect!", "comment_date": "2018-09-01T17:39:59Z", "likes_count": 0}, {"comment_by": "Bevel", "comment_text": "Ahhhhhh! I see!", "comment_date": "2018-08-30T02:01:19Z", "likes_count": 1}, {"comment_by": "Avvv Qvvv", "comment_text": "nudge-core", "comment_date": "2018-08-26T21:26:02Z", "likes_count": 0}, {"comment_by": "Shivam Pandey", "comment_text": "THE GREATEST VIDEO ON BACKPROPAGATION OF ALL TIME !!! ENTIRE YOUTUBE DOESN&#39;T HAS THIS GOOD VIDEO ON BACKPROPAGATION !!", "comment_date": "2018-08-25T07:21:41Z", "likes_count": 0}, {"comment_by": "Brian Fossati", "comment_text": "I literally laughed out loud at @<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=3m53s\">3:53</a> with reflection from the previous video", "comment_date": "2018-08-25T01:28:49Z", "likes_count": 0}, {"comment_by": "Ram Aditya", "comment_text": "I &lt;3 3blue1brown", "comment_date": "2018-08-21T18:29:22Z", "likes_count": 0}, {"comment_by": "Pavel Pivaev", "comment_text": "Hey, Grant!<br>I&#39;m under the great impression of the things you are doing and want to express an immeasurable gratitude for that. Your works really eye-opening in math topics. I could say that I&#39;ve digested first two episodes about Neural Networks but now I&#39;m stuck, at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m38s\">7:38</a>, and cannot afford myself to go further as that moment remains unclear to me. You stated there (on the left side of the screen) three options to increase the resulting sum which corresponds to number 2 activation because we want it to be higher than the others as we deal with image of the number 2.  At the same time, it is our desire to make all the activations corresponding to other numbers lower. <br>And my question is: If we apply those methods (on the left side of the screen and especially the last method) to increase the desired activation, why will we apply the same rules to decrease activations of the rest? We want terms of, e.g, the 0 neuron&#39;s activation to be the lowest, don&#39;t we? If it is so, the third method won&#39;t provide such a behavior of the activation sum, that is, won&#39;t decrease it, will it? I thought that we should do the opposite. <br>Thank you in advance.<br>I still have some hope for your answer in this fathomless pit of comments :)", "comment_date": "2018-08-16T22:30:02Z", "likes_count": 0}, {"comment_by": "Kevin King", "comment_text": "Spectacular... Thank you so much!", "comment_date": "2018-08-15T02:46:54Z", "likes_count": 0}, {"comment_by": "Neil Mathew", "comment_text": "Question: So is there one gradient descent for one training example? Also, I&#39;m not sure I understand the &quot;mini-batches&quot; concept so I would greatly appreciate any explanation.", "comment_date": "2018-08-11T21:48:36Z", "likes_count": 0}, {"comment_by": "Lucas Germano", "comment_text": "Best videos ever and best explanation too!! Greetings from Brazil", "comment_date": "2018-08-11T04:31:28Z", "likes_count": 0}, {"comment_by": "pmac", "comment_text": "I just want to clear up some terminology. At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m58s\">9:58</a> you mention the &#39;actual&#39; gradient of the cost function. I am confused on how the term &#39;actual&#39; or &#39;true&#39; gradient in these situations. When I think of the &#39;actual&#39; gradient, I am thinking of the gradient of a function with respect to all of its independent variables, instead of taking the gradient in a certain &#39;plane&#39;. Mini-batching still takes into account every weight at each step, so to me this is still a &#39;true&#39; gradient.", "comment_date": "2018-08-10T19:12:51Z", "likes_count": 0}, {"comment_by": "A Man Kumar", "comment_text": "I do not know why people like his videos. So worst", "comment_date": "2018-08-09T15:55:58Z", "likes_count": 0}, {"comment_by": "Peter Nachtwey", "comment_text": "Why gradient descent?  There are a few algorithms for optimizing or find the minimum.  I use Levenberg-Marquardt a lot for finding coefficients for non-linear differential equations.<br><br>Are all the coefficients optimized at one time or just layer by layer?<br>Are all the training samples in a mini batch used at one time?", "comment_date": "2018-08-07T09:08:46Z", "likes_count": 0}, {"comment_by": "Wang Yi", "comment_text": "Have to say this is the most amazing UI presentations I&#39;ve ever seen", "comment_date": "2018-08-05T00:13:50Z", "likes_count": 0}, {"comment_by": "Sriram Sundar", "comment_text": "Great explanation, your team is awesome. &quot;A drunk man stumbling aimlessly downhill,but taking quick steps&quot; is the best analogy ever for Stochastic gradient descent. :-)", "comment_date": "2018-08-03T16:52:27Z", "likes_count": 123}, {"comment_by": "CrimsonKnight 02", "comment_text": "Cool stuff but I haven\u2019t taken calculus so can go to the next video \u2639\ufe0f", "comment_date": "2018-08-03T16:44:12Z", "likes_count": 0}, {"comment_by": "Yashash Gaurav", "comment_text": "If you&#39;d open a university, I&#39;d enroll right now!", "comment_date": "2018-08-03T06:36:04Z", "likes_count": 1}, {"comment_by": "fourten0410", "comment_text": "Brilliant...thank you!", "comment_date": "2018-07-30T19:07:49Z", "likes_count": 0}, {"comment_by": "Mabeesha Wijekoon", "comment_text": "Best explanation. Thank You!!!", "comment_date": "2018-07-30T18:22:23Z", "likes_count": 0}, {"comment_by": "Amar Kachari", "comment_text": "Best explanation", "comment_date": "2018-07-30T12:00:00Z", "likes_count": 0}, {"comment_by": "Sabarish K", "comment_text": "Nice one", "comment_date": "2018-07-28T12:36:26Z", "likes_count": 0}, {"comment_by": "Nigel Foster-Jones", "comment_text": "Fermat= tease 10/10", "comment_date": "2018-07-25T02:38:55Z", "likes_count": 0}, {"comment_by": "Pranav Jain", "comment_text": "Your entire series about ANNs is very well made.<br>I just had a question. So I understand the idea of Hebbian learning in which the weights attached to neurons with the highest activation have the biggest changes/steps/nudges. So thinking about it from the perspective of a single weight + cost graph, is this equivalent to saying that the weight&#39;s current location had a steeper slope and this is why the nudge/step it had to take was bigger?<br>If you need me to rephrase the question, I will gladly do my best.<br>Thank you so much for your time.", "comment_date": "2018-07-24T00:23:04Z", "likes_count": 0}, {"comment_by": "IrradiatedOne", "comment_text": "awesome videos on this topic, but are there going to be any more on this subject?<br><br>There is so much more to this topic. :)", "comment_date": "2018-07-19T19:13:48Z", "likes_count": 0}, {"comment_by": "Devin Cory", "comment_text": "Then what&#39;s all of this about Neural Networks learning to play games? It doesn&#39;t seem like there is any &quot;Training Data&quot; like you&#39;re describing, and even if there were, there would be no way to label what the &quot;Correct Answer&quot; is. So what&#39;s going on with that?", "comment_date": "2018-07-19T04:36:08Z", "likes_count": 0}, {"comment_by": "Matan Levy", "comment_text": "Thank you, that was beautiful", "comment_date": "2018-07-18T16:08:05Z", "likes_count": 0}, {"comment_by": "Rahul Singh", "comment_text": "I am literally greeting you from my heart!", "comment_date": "2018-07-16T16:31:09Z", "likes_count": 0}, {"comment_by": "Pedro S", "comment_text": "Always impressed by your videos! You have no idea how much they helped with my EE degree over the years. Eternal thanks &lt;3", "comment_date": "2018-07-13T00:51:51Z", "likes_count": 0}, {"comment_by": "mutaemma", "comment_text": "I don&#39;t know why I don&#39;t understand these concepts.I am trying so hard to understand though.", "comment_date": "2018-07-11T22:29:10Z", "likes_count": 0}, {"comment_by": "Antek Borkowski", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=5m50s\">5:50</a> &quot;We care about witch ones give you the most pain for your butt&quot;", "comment_date": "2018-07-11T16:43:00Z", "likes_count": 1}, {"comment_by": "hjoab hjoab", "comment_text": "Excellent!!", "comment_date": "2018-07-09T16:24:11Z", "likes_count": 0}, {"comment_by": "vg", "comment_text": "Hey pls pls continue this series by covering RNNs (LSTMs &amp; GRUs)", "comment_date": "2018-07-05T18:07:43Z", "likes_count": 0}, {"comment_by": "Sepiormon", "comment_text": "Here to learn? <br>nah here for calming voice and smooth animation.", "comment_date": "2018-07-02T06:10:15Z", "likes_count": 0}, {"comment_by": "Terminator .Tanish.", "comment_text": "Please do a video on how you work with Python to create these videos!!!! They&#39;re so beautiful!!!!! just how!!!!", "comment_date": "2018-07-01T18:28:35Z", "likes_count": 0}, {"comment_by": "Ivan Goncharov", "comment_text": "Thank you", "comment_date": "2018-06-28T05:58:55Z", "likes_count": 0}, {"comment_by": "Thomas Bingel", "comment_text": "Very recommendable! Well done!", "comment_date": "2018-06-27T04:32:15Z", "likes_count": 0}, {"comment_by": "nesa1126", "comment_text": "Hey, I am watching this to decide should I try to use NN for my masters thesis in psychology. We have a lot of statistics here and there are some similar methods used for discrimination, but they ponder itself by their discrimination power. For example: If &quot;neurons&quot; often correctly react, when they are recognizing what they are supposed to (2 and their value would be close to 1.0) AND other neurons that really rarely react when there is that type of stimulus (2 and their value would be close to 0.0) (others would be random (from 0.0 to 1.0 but not close to these values)) those information&#39;s are also valuable (In statistical terms they have more variance, negative correlation is also important). SO why is there no similar case? It seems to me (and there is a big chance that I don&#39;t understand this at all and therefore I ask stupid questions) that that would be more efficient (also, some real neurons do that). As I understood, &quot;neurons&quot; that react when stimuli is present are only weighted but not the ones that never react when that same stimulus is here. Does this even have sense ? haha I need to inform myself better. Anyway, thanks for great videos.  Sorry for bad english.", "comment_date": "2018-06-26T15:20:07Z", "likes_count": 0}, {"comment_by": "Shubham Banerjee", "comment_text": "You are an amazing teacher, loved your work a lot.<br>It would be great if you make some series over design analysis of algorithm and the explanation of the coding part particularly, as there isn&#39;t really any nice tutorial available and it&#39;s in great demand by the students. We need teachers like you.", "comment_date": "2018-06-26T07:46:59Z", "likes_count": 0}, {"comment_by": "shanaka jayatilake", "comment_text": "A great video series", "comment_date": "2018-06-25T18:29:58Z", "likes_count": 0}, {"comment_by": "Long Nguyen-Vu", "comment_text": "I am curious about the values in each node in the last layers (for example, at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m23s\">4:23</a> we have  0.5  0.8 0.2 1.0 0.4 etc.)<br>Aren&#39;t they supposed to have their summation equal to 1?<br><br>Please correct me if I&#39;m wrong", "comment_date": "2018-06-25T07:42:53Z", "likes_count": 0}, {"comment_by": "Daniele PS", "comment_text": "How do you graph the cost function...", "comment_date": "2018-06-23T11:43:12Z", "likes_count": 0}, {"comment_by": "XelPixels", "comment_text": "You are great at teaching!<br>So many gets  lost in the mathematics of it, not caring if their audience understand it or not. You make it incredibly simple!<br>Great job!!", "comment_date": "2018-06-22T14:55:26Z", "likes_count": 0}, {"comment_by": "John Christopher Wrightson", "comment_text": "Mind = blown", "comment_date": "2018-06-18T10:40:50Z", "likes_count": 0}, {"comment_by": "K Alec Petersen", "comment_text": "So for the last hidden layer you wouldn&#39;t need to do any averaging for the weight between one of those neurons and one of the output neurons right? I can understand why the rest need to be averaged.", "comment_date": "2018-06-15T19:46:50Z", "likes_count": 0}, {"comment_by": "PhiDX Games", "comment_text": "Most helpful guide ever, thanks so much", "comment_date": "2018-06-14T18:33:40Z", "likes_count": 0}, {"comment_by": "BLoodyEx", "comment_text": "What an awesome series. Thanks so much, that&#39;ll help A LOT for my presentation on artificial intelligence and it made me understand neural networks so much better =)", "comment_date": "2018-06-14T16:25:58Z", "likes_count": 0}, {"comment_by": "blafuckyou", "comment_text": "wow your videos are actually really really good. I am literally asking myself if I was the last generation where university actually was necessary since today I am able to find much better sources of education online and can pick my own pace. Awesome but somewhat scary since it also makes me feel like the competition is much bigger than it originally was when only the few who happened to end up in elite universities would get access to this knowledge. I feel like the world is evolving faster than ever before without humanity even being able to graps what is actually happening. All these algorithms are actually so efficiently controlling our every day lifes and each day thousands of kids tweek it a bit more. It&#39;s both awesome and scary to me. I should go to bed, I am being weird again. ha", "comment_date": "2018-06-13T21:37:26Z", "likes_count": 0}, {"comment_by": "Aviv Levi", "comment_text": "Thank you so much ! I do not deserve you", "comment_date": "2018-06-12T16:35:59Z", "likes_count": 0}, {"comment_by": "Mollyarty", "comment_text": "Ugh, this was so much more difficult to understand because you kept saying &quot;moving in 13,000 dimensions is beyond the scope of our imaginination&quot; or something like that in multiple videos. No. No it&#39;s not. It is SO much easier to imagine it as a movement than as relative magnitude of vectors in a matrix, that was so much more confusing. Seriously, watched this like 8 times because this one thing kept tripping me up. Don&#39;t assume everyone who is watching your vids thinks the same way you do.", "comment_date": "2018-06-11T23:46:47Z", "likes_count": 0}, {"comment_by": "\u3007", "comment_text": "thank you my friend. this is intuitive as heaven", "comment_date": "2018-06-08T16:30:28Z", "likes_count": 0}, {"comment_by": "Indecision6326", "comment_text": "@<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=13m22s\">13:22</a><br>Fanboy LLC.... This person needs a real job.", "comment_date": "2018-06-07T17:19:02Z", "likes_count": 0}, {"comment_by": "Emilio Maoddi", "comment_text": "you explained it superbly, thank you!", "comment_date": "2018-06-07T14:55:10Z", "likes_count": 0}, {"comment_by": "Nikhil Lahoti", "comment_text": "Can you please explain CNN and RNN?<br>There is just no good tutorial which clears concepts like you do.<br>Hats off Sir!!!", "comment_date": "2018-06-07T07:12:11Z", "likes_count": 1}, {"comment_by": "Sally", "comment_text": "This is amazing thank you so much <br>I stopped at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m08s\">9:08</a> to leave a comment and someone else already commented about the Eureka moment<br>Thank you so much you&#39;re a great teacher!", "comment_date": "2018-06-07T06:27:10Z", "likes_count": 0}, {"comment_by": "Julian", "comment_text": "there are rarely videos with literally NO bad comments below them, this is one of them (and it has half a million views)", "comment_date": "2018-06-03T01:29:18Z", "likes_count": 0}, {"comment_by": "Israel Miller", "comment_text": "What does it take to get a job in machine learning? How does one go about learning how to build algorithms? I&#39;m a college student and this is... Well... The future, and I recognize that. Thanks for the videos.", "comment_date": "2018-06-02T02:15:42Z", "likes_count": 0}, {"comment_by": "Jacques Kirstein", "comment_text": "Possibly the most education dense video I have seen in my life! After having done a lot of self studying on my way to getting a mechanical engineering degree, I believe this speaks volumes in itself", "comment_date": "2018-06-01T06:41:39Z", "likes_count": 0}, {"comment_by": "Jacques Kirstein", "comment_text": "Thank you so much for all the time and effort that you put into making a visual understanding of Neural Networks. I&#39;ve been trying (admittedly on and off) to get a good intuition on Neural Networks for at least a year now. This is by far the best fundamental video I have come across to get an understanding of what the theory behind the scenes is for. I know that this will increase my (and many others&#39;) learning curve by a large amount. Thanks again!", "comment_date": "2018-06-01T06:38:53Z", "likes_count": 1}, {"comment_by": "Neutron Chicken", "comment_text": "Love this series, I&#39;m trying to build a NN of my own and I have a feeling I&#39;m seriously way in over my head. However, my plan is to be able to create an NN than can &quot;guess&quot; letters of the alphabet too so that it can scan handwriting and convert it to a word document.", "comment_date": "2018-05-30T05:38:29Z", "likes_count": 0}, {"comment_by": "Sagan McVander", "comment_text": "I understood what you said back propagation is, but, wouldn&#39;t it just be better if someone with data analytic and computer science skills built the neural network manually? Honestly it seems more effective than wasting all that computing power. I use macros a lot to cheat in some games. When I do so, the macro not only allows me to detect a specific image, but, also, images that deviate from that image to as much of a percent as I desire. I never have to wait for the macro to calculate every possibility. It seems like a waste of time. I&#39;m not saying self generating neural networks are bad, but, that they certainly need more work, as the effectiveness they have is behind already existing algorithms in other fields of study which do similar things.", "comment_date": "2018-05-28T21:18:52Z", "likes_count": 0}, {"comment_by": "Mr.sunflower", "comment_text": "i love oppai", "comment_date": "2018-05-28T17:46:45Z", "likes_count": 0}, {"comment_by": "Srinjoy Choudhury", "comment_text": "Please upload more contents.The explanation are of the simplest yet detailed.Its the best resource to learn the basics of neural networks", "comment_date": "2018-05-28T06:40:32Z", "likes_count": 0}, {"comment_by": "L\u00e1szl\u00f3 Katona", "comment_text": "@3Blue1Brown Couldn&#39;t you try to make it &quot;draw&quot; each number by setting the last nodes according to the number you want it to draw and sort of backtrace it, that is reverse the weights and biases and then you would get fractions for the original input pixels in case of a given output. I am really interested in what sort of result you would get, probably something resembling the numbers only slightly. Please, if you find time for it share it with us :)", "comment_date": "2018-05-26T19:35:12Z", "likes_count": 0}, {"comment_by": "Victor Engel", "comment_text": "I&#39;ve been watching these videos to give myself an idea on how I should proceed with a project I&#39;ve recently started on, and I&#39;m beginning to think that the model I&#39;ve chosen is problematic. At least, it doesn&#39;t seem to fit well with the way things are flowing here up to this point. More specifically, the model here has 10 possible outputs. Those are either right or wrong. In my model, any number of points could be be right and any number of points could be wrong. So I&#39;m unsure how to even calculate a cost.<br><br>Confused? Let me explain. Imagine a hungry automaton needs to navigate through a minefield to find some morsels. It can see ahead a finite direction. Let&#39;s say it has half a dozen visual receptors that can detect the food and the mines. The job of the neural network is to generate a vector, giving an acceleration in order to successfully navigate the minefield consuming as much food as possible without blowing up. I figure the output of the network is an acceleration vector, say, an ordered pair, (x,y). There is not a single right answer, because the minefield could be navigated any number of ways, both successfully and unsuccessfully. Cost is determined by whether applying that acceleration results in food acquisition or stepping on a mine or finding nothing. So how do I calculate the cost for such a model? If I need to change the model, what change could make this work?", "comment_date": "2018-05-25T21:25:49Z", "likes_count": 0}, {"comment_by": "Albert Galilei", "comment_text": "Give it one more output wich is &quot;noise&quot; and feed it &quot;noise&quot; that should make it more safe against output a 0-9 for &quot;noise&quot;<br>as far as I understand it. Would that work?", "comment_date": "2018-05-24T07:57:04Z", "likes_count": 0}, {"comment_by": "BanditFoxx", "comment_text": "How about training a program to DRAW the numbers, then using that program to train this one to understand them? Just a thought...", "comment_date": "2018-05-18T04:44:20Z", "likes_count": 1}, {"comment_by": "Indecision6326", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=1m55s\">1:55</a><br>Ah! Ghost!", "comment_date": "2018-05-16T18:10:05Z", "likes_count": 0}, {"comment_by": "Charlie Kilo", "comment_text": "your videos are simply amazing.. awesome visualisation of what actually happens...i imagine a huge effort went into making those.... kudos - i almost never comment in that manner... greatest thanks and respect for that!", "comment_date": "2018-05-16T09:23:39Z", "likes_count": 0}, {"comment_by": "m\u00e4\u00e4\u00e4\u00e4\u00e4\u00e4\u00e4\u00e4\u00e4x", "comment_text": "how long is this trial?", "comment_date": "2018-05-15T16:24:10Z", "likes_count": 0}, {"comment_by": "jenejn", "comment_text": "Amazing visual and intuitive explanation. Keep it on man", "comment_date": "2018-05-15T08:11:34Z", "likes_count": 0}, {"comment_by": "ring ramesh", "comment_text": "Amazing!!!Great job.", "comment_date": "2018-05-14T21:21:07Z", "likes_count": 0}, {"comment_by": "Another GoogleUser", "comment_text": "I don&#39;t understand, how the all 13k weights and biases are getting changed don&#39;t we only need one layer of the neurones to change to show us 2, so only (16*16)+16 weights and biases needs changing?", "comment_date": "2018-05-08T21:58:39Z", "likes_count": 0}, {"comment_by": "Angel", "comment_text": "The animations are impeccable!", "comment_date": "2018-05-08T18:46:55Z", "likes_count": 0}, {"comment_by": "bkmalay", "comment_text": "Amazing animation....one view is equivalent to reading texts for 10 times.", "comment_date": "2018-05-06T12:32:25Z", "likes_count": 0}, {"comment_by": "Yunfei Chen", "comment_text": "great more calculus!!!! rip headaches????", "comment_date": "2018-04-29T22:03:17Z", "likes_count": 0}, {"comment_by": "Manav Kothari", "comment_text": "In backpropagation, you have updated weights and you have estimated activations for the second to last layer. But is that (the new activation for the second to last layer) based on the updated weights or the original weights?", "comment_date": "2018-04-28T22:40:12Z", "likes_count": 0}, {"comment_by": "CH D", "comment_text": "I enjoyed the animation....but understood nothing", "comment_date": "2018-04-28T12:51:55Z", "likes_count": 0}, {"comment_by": "Cristy\u6263\u8089\u7761\u9192\u4e86\u54a9", "comment_text": "Incredible work! Thanks for your amazing videos!", "comment_date": "2018-04-28T04:45:14Z", "likes_count": 0}, {"comment_by": "Gregory Williams", "comment_text": "Your videos are so amazing. Incredibly well explained. Thanks!", "comment_date": "2018-04-25T02:41:53Z", "likes_count": 0}, {"comment_by": "theworld joy", "comment_text": "I hope this video exists when i was doing my master!!!  even better than the lecturer //", "comment_date": "2018-04-22T14:12:54Z", "likes_count": 0}, {"comment_by": "Stelios Toulis", "comment_text": "You would give us a source to dowload the software...where is that?", "comment_date": "2018-04-19T12:31:27Z", "likes_count": 0}, {"comment_by": "Nicolo Pareja", "comment_text": "My Optimal Control class makes a lot more sense thanks to this video! thank you!!", "comment_date": "2018-04-17T05:43:40Z", "likes_count": 0}, {"comment_by": "TheGrimravager", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m00s\">9:00</a>, I wonder if averaging the desired changes is really the most efficient way decrease the cost functions next output", "comment_date": "2018-04-16T19:57:21Z", "likes_count": 0}, {"comment_by": "Darshil D", "comment_text": "Excellent video and animations!", "comment_date": "2018-04-14T20:13:40Z", "likes_count": 0}, {"comment_by": "warby579", "comment_text": "Fantastic video, actually understood, thank you!", "comment_date": "2018-04-12T22:23:15Z", "likes_count": 0}, {"comment_by": "Noface", "comment_text": "vast amounts of labelled alphanumeric datasets exist in the form of fonts.", "comment_date": "2018-04-11T01:37:40Z", "likes_count": 0}, {"comment_by": "Eugene Burdeinyi", "comment_text": "why would each training example affect to all 13000 weights if we are good to go just adjusting only 16x10 weights between two last layers (second hidden and output ones)? set up these 160 weights and thats all because second hidden layer activations remain unchanged as well as all weights on the left side of them", "comment_date": "2018-04-07T20:15:17Z", "likes_count": 0}, {"comment_by": "Amol Ashtikar", "comment_text": "Amazing! Great audio - visuals and great knowledge. Neuroscience made easy.", "comment_date": "2018-04-05T19:21:01Z", "likes_count": 0}, {"comment_by": "Jows Shibe", "comment_text": "Thanks for explaining so many (complex) things in such a clear and easily understandable manner, the visualizations are stunning and help a lot! Could you perhaps make a video about CNN&#39;s instead of the more traditional ANN&#39;s? I would love to see that :3", "comment_date": "2018-04-03T14:33:06Z", "likes_count": 0}, {"comment_by": "Muhammad Roshan", "comment_text": "<b>Correct me if I am wrong please.</b><br>So first we randomly put activations and its weights. Then for each training example keep on calculating cost. At end avg the cost to know how badly our algorithm performed.<br>As gradient Descent tells us about how to change the weights (As we cannot change the activations by ourselves) so to find the gradient we apply back propogation.<br>In back propogation (taking 1 example), we check right answer of training data and find out how to change the weights for that particular circle and then also check the change of weights for other circles as well.<br>We check the above method for many many examples but not all (as stochastic gradient Descent) and apply those weights.<br>Then we keep on changing the weights for all layers until we get to the root layer.<br>Then after some examples check the cost function to know how it performed", "comment_date": "2018-04-03T08:13:52Z", "likes_count": 0}, {"comment_by": "Dunkleosteus", "comment_text": "I tried implementing my own neural network (to solve a problem other than this one). It was not a very complicated network (although the inputs weren&#39;t graphics, so I don&#39;t know if that messes it up). The output would be something similar to this where all output neurons should be 0 unless the neuron was the desired answer, in which case 1. My network decides to turn everything to 0 after the first pass. There were only three outcomes possible: 100, 010, 001. It just outputs 000 each time. Is my network just not complex enough to figure out a better solution?", "comment_date": "2018-04-02T16:36:47Z", "likes_count": 0}, {"comment_by": "Achilles Armstrong", "comment_text": "great one", "comment_date": "2018-04-02T09:27:36Z", "likes_count": 0}, {"comment_by": "Sree Gowtham Josyula", "comment_text": "Very Helpful!!", "comment_date": "2018-03-29T05:54:12Z", "likes_count": 1}, {"comment_by": "Soham Harnale", "comment_text": "This man is the eternal fountain of knowledge straight from the heavens &lt;3", "comment_date": "2018-03-27T08:35:37Z", "likes_count": 6}, {"comment_by": "Paul Lee", "comment_text": "Great, great videos.", "comment_date": "2018-03-25T15:26:09Z", "likes_count": 1}, {"comment_by": "Tuxedo Productions", "comment_text": "So, in order to save computing resources when determining the gradient, it is efficient to use &quot;Drunk-Man&#39;s Multivariable Calculus&quot;", "comment_date": "2018-03-25T04:53:24Z", "likes_count": 0}, {"comment_by": "MemeNowDealWIthIt", "comment_text": "I feel like I missed something. This backpropagation seems only to affect the penultimate layer of neurons. How are the other layers affected?", "comment_date": "2018-03-24T18:21:37Z", "likes_count": 0}, {"comment_by": "MyOther Soul", "comment_text": "Great video....   but how does that add up to the singularity?  It must add up to a singularity because Kurzweil said it would and he is rich.  (Sarcasm)", "comment_date": "2018-03-24T04:42:35Z", "likes_count": 0}, {"comment_by": "Ram Ramabhadran", "comment_text": "Superb teacher and three great videos which gave the very basics of how neural networks function. I loved all three and learned a lot!", "comment_date": "2018-03-23T16:00:25Z", "likes_count": 1}, {"comment_by": "\u738b\u752f", "comment_text": "it&#39;s still not clear how the cost of certain output(e.g. the cost of 2 in the video) is &quot;distributed&quot; among &quot;all layers&quot;. Apparently, the back-propagation is from right to left, but in the video you only know that all the responsibilities of adjustment of weights for a single output node are just push left again and again.<br>That is, at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m36s\">8:36</a> and <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=10m34s\">10:34</a>, the network already knows how to distribute the cost to all the weights, but we don&#39;t... . A portion of the responsibilities should &quot;stop&quot; at each layer, not just totally pushed left.", "comment_date": "2018-03-21T04:38:13Z", "likes_count": 0}, {"comment_by": "Alex Yuan", "comment_text": "I am going to fail my exam tomorrow.", "comment_date": "2018-03-19T06:08:25Z", "likes_count": 0}, {"comment_by": "Arman", "comment_text": "What do you use to make these animations? especially the neuron network animations?", "comment_date": "2018-03-18T21:27:10Z", "likes_count": 1}, {"comment_by": "shreyas shubham", "comment_text": "Amazing mannnnnnnn", "comment_date": "2018-03-17T19:56:05Z", "likes_count": 0}, {"comment_by": "Batkhuyag Batsaikhan", "comment_text": "awesome!", "comment_date": "2018-03-16T05:02:45Z", "likes_count": 1}, {"comment_by": "Johannes Bernstein", "comment_text": "Which idiot added to those perfect videos german subtitles?<br>Carry on, your videos and your explanations are wondeful.<br>Congratulations!", "comment_date": "2018-03-15T10:44:03Z", "likes_count": 2}, {"comment_by": "Sara Grimaldi", "comment_text": "Great videos; did save me hours of head banging on the wall, trying to get the boring book concepts; while now I can fly through it because I have to learn only the math part :)<br><br>BTW are you planning to do a follow up where you go through the code to actually make a simple NN from scratch, maybe in C++ ? I know that there are libraries, and the book you suggested has that; but your videos really take the whole thing and break it up so nicely that is probably the most efficient thing I have ever seen. Thanks again, from a student perspective these videos are invaluable.", "comment_date": "2018-03-14T06:22:09Z", "likes_count": 0}, {"comment_by": "G\u00e1bor Nagy", "comment_text": "What happens if you change the direction of the trained network? So, if you set the neuron 2 to one and the other numbers to zero, will it draw a number 2 on the pixel neurons?", "comment_date": "2018-03-13T20:20:31Z", "likes_count": 0}, {"comment_by": "viktor marin", "comment_text": "what i dont understand is why would splitting up the training examples benefitt us. We still have to go trough all of them or is there some exponential thing im missing. sorry for bad english", "comment_date": "2018-03-09T12:15:50Z", "likes_count": 0}, {"comment_by": "Raul Guarini", "comment_text": "Absolutely great video and playlist as a whole. A great head start!", "comment_date": "2018-03-08T19:16:09Z", "likes_count": 2}, {"comment_by": "Adam Young", "comment_text": "For increasing weights to get better performance, what does in proportion to a mean? Could anyone tell me? Thnx", "comment_date": "2018-03-07T07:37:06Z", "likes_count": 0}, {"comment_by": "Manjunath Sastry", "comment_text": "Oh dear! Absolutely brilliant. This is one heck of video. <br>How would you consider creating an exhaustive ML and Neural Network video series? I&#39;m sure there are a lot of curious folk out there, waiting for it!", "comment_date": "2018-03-05T10:29:57Z", "likes_count": 5}, {"comment_by": "ziya dalk\u0131l\u0131\u00e7", "comment_text": "This is very similar to, if not the same as, model updating in structural health monitoring. I think all such approaches shall use I for identification (as in SHM) instead of I for intelligence. The fascinating thing is the identification problem is solvable by some arbitrary number of linear combination optimizations, not that it is solvable. By the way, after this video, that &quot;girls&#39;&quot; (forgot her name) claims about using a proper training data to start with, makes tons of sense. Personally, I would train your network with Sans Serif or something first, before moving to more fuzzy drawings. Is that what you did? or did you just force feed it?", "comment_date": "2018-03-04T15:26:10Z", "likes_count": 0}, {"comment_by": "BigToinE976", "comment_text": "And what is the gradient&#39;s composante for the biaises ? Simply proportional to the neuron&#39;s intended change ?", "comment_date": "2018-03-04T14:30:30Z", "likes_count": 0}, {"comment_by": "Johns Junk", "comment_text": "great job,  I watched AI training videos from Udacity, intel, and many others.  Yours are the most clear and well stated", "comment_date": "2018-03-03T13:49:45Z", "likes_count": 0}, {"comment_by": "Ilya Pukhov", "comment_text": "Is there a way to support you with a one time donation?", "comment_date": "2018-03-03T07:23:30Z", "likes_count": 0}, {"comment_by": "drKk", "comment_text": "clear, essential, elegant. Your videos are gems", "comment_date": "2018-02-25T14:49:23Z", "likes_count": 1}, {"comment_by": "Rahul Shelke", "comment_text": "And also on origin of trigonometry sin cos tan", "comment_date": "2018-02-25T09:59:21Z", "likes_count": 0}, {"comment_by": "Rahul Shelke", "comment_text": "sir can you make a video on long short-term memory deep learning plz...", "comment_date": "2018-02-25T09:56:11Z", "likes_count": 0}, {"comment_by": "Yuri Aps", "comment_text": "love that videos!", "comment_date": "2018-02-23T18:14:46Z", "likes_count": 0}, {"comment_by": "Hasni Med Bilel", "comment_text": "Mind blowing animation .. Excellent work !!!", "comment_date": "2018-02-23T00:34:45Z", "likes_count": 1}, {"comment_by": "jenellewful", "comment_text": "This is so helpful - exactly the explanation I was looking for!!", "comment_date": "2018-02-21T15:29:00Z", "likes_count": 1}, {"comment_by": "thelifter", "comment_text": "Thank God I found you. You&#39;re a gem.", "comment_date": "2018-02-17T06:26:11Z", "likes_count": 2}, {"comment_by": "Jim Yao", "comment_text": "Super cool demo, and everything nicely explained", "comment_date": "2018-02-16T05:26:19Z", "likes_count": 1}, {"comment_by": "John Gleeson", "comment_text": "Before I ask my question I just want to say that these videos are awesome. It&#39;s really interesting stuff and it&#39;s very nice to have someone developing the resources to learn about it and presenting it in such an approachable way, so thank you. My question is this, how does adding more neurons to the network affect how it adapts? You mention in your previous videos that it&#39;s somewhat arbitrary, or at least that your choice in this specific example of a neural network is somewhat arbitrary though I&#39;m interested in what effect it might have. From what I understand it would change the amount of dimensions present in the gradient but does that have any meaningful effect on gradient descent or local minimums? What difference would there be in adding another sixteen neurons to the both hidden layers as compared to adding another two layers of sixteen neurons? The answer is probably more technical than I think but at any rate I appreciate the opportunity to ask. So again, thank you.", "comment_date": "2018-02-15T23:25:23Z", "likes_count": 0}, {"comment_by": "nitesh kumar Sharma", "comment_text": "Hey 3blue1Brown i have a doubt!<br>Since the example that you have shown is a classifcation problem should&#39;nt we use logarthmic cost function as opposed to least squares cost ( which used to linear regression).Or am i missing something here?", "comment_date": "2018-02-15T12:18:26Z", "likes_count": 0}, {"comment_by": "Bj\u00f6rn Mor\u00e9n", "comment_text": "Didn&#39;t know that Dave Rubin is also great at math.", "comment_date": "2018-02-14T07:01:46Z", "likes_count": 0}, {"comment_by": "Draktharr", "comment_text": "Is my understanding of the stochastic gradient descent correct? So far, what I&#39;ve understand is the following :<br>let&#39;s say our training data is 10 000. If we don&#39;t do the SGD, we will have to do, for each training data:<br>- calculate the cost function,<br>- do the backpropagation<br>- change the weight and bias of the neural network according to the backpropagation calculation<br>-move on to the next training data<br>This would result in 10000 calculation of the cost function, 10000 backpropagation calculation and changing 10000 times the weight and bias in our neural network.<br><br>If instead we choose to do 100 batch of 100 training data, we will instead do instead :<br>- calculate the cost function of all the training data for a batch and calculate the mean of this cost function<br>- do the backpropagation for this batch<br>- change the weight and bias of the neural network according to the backpropagation calculation<br>- move on to the next batch<br>This would result in 10000 calculation of the cost function, 100 backpropagation calculation and changing 100 times the weight and bias.<br><br>Am I right? Or is my understanding not exact?", "comment_date": "2018-02-11T00:40:43Z", "likes_count": 0}, {"comment_by": "M. Sierra", "comment_text": "For whoever wants a more in-depth and up-to-date read to that topic, I highly recommend the book &quot;Deep Learning&quot; from MIT, written by Ian Goodfellow.", "comment_date": "2018-02-10T13:23:57Z", "likes_count": 0}, {"comment_by": "Jon Snow", "comment_text": "Why do we need the <b>bias b</b> ?", "comment_date": "2018-02-10T10:40:27Z", "likes_count": 0}, {"comment_by": "Yella Dart", "comment_text": "So the question I&#39;m left with is will we ever be able to reach a point where a neural network can identify something that has a correct answer it&#39;s not prepared to give? For example, if the outputs are a finite set of things it can identify, but it then hits a case where the provided solution is not one of those outputs, how complex would it be for the network to dynamically add that possible solution?", "comment_date": "2018-02-09T21:43:51Z", "likes_count": 0}, {"comment_by": "Ling W", "comment_text": "thank you for making these vids", "comment_date": "2018-02-08T05:57:30Z", "likes_count": 1}, {"comment_by": "Gaurav Mishra", "comment_text": "I&#39;ve been reading the book for quite some time but your explanations using animations have pushed my understanding to new levels. So many thanks to you. <br><br>PS: Keep the background music. Holds our concentration for a long time.", "comment_date": "2018-02-05T08:00:32Z", "likes_count": 2}, {"comment_by": "Yuan-Lu Chen", "comment_text": "This is the most explicit and accessible explanation of backpropagation! In less than 14 minutes. Amazing!", "comment_date": "2018-02-04T19:04:39Z", "likes_count": 0}, {"comment_by": "Akarshan Sarkar", "comment_text": "In this example, you are using the gradient descent approach (and the Stochastic as well) to converge the network to local minima that mean the network won&#39;t perform well in inputs around global minima maybe? So does the methods for finding global minima (maybe like Simulated Annealing) actually used in Neural networks for giving better results?", "comment_date": "2018-02-04T08:30:19Z", "likes_count": 0}, {"comment_by": "Matthew Carrano", "comment_text": "Whoa notch is a patron", "comment_date": "2018-02-03T08:48:45Z", "likes_count": 1}, {"comment_by": "Matthew Crawford", "comment_text": "So a monte carlo on batches of subsets of data... already have to break data into subsets for memory and threading limitations anyway, so after watching the video this seems intuitively obvious.  :)<br>My AI predicts the hierarchy path in a star pattern object tree / database between two specified nodes.<br>Ex: Given a specific object, find all related objects. So, import an XML file and list all dependents of employees in a single company.<br>Good: Company -&gt; Employee -&gt; Dependents<br>Bad: Company -&gt; Import XML File reference -&gt; all dependents in the XML file for all companies<br><br>Since all records are monitored for where they came from, all records have a 1 step path to all other records but this relationship is useless.<br>Thanks for the clear descriptions of concepts I have not yet been introduced. I now know where to start :)", "comment_date": "2018-02-02T07:45:40Z", "likes_count": 0}, {"comment_by": "Goddess Kratos", "comment_text": "That&#39;s smart, batching for quick averages for algorithmic pattern stat, I like it", "comment_date": "2018-01-31T08:13:17Z", "likes_count": 0}, {"comment_by": "aventh 1", "comment_text": "What happens if I try to use this on something i dont know the answer for? How could I use a neural network for that? In my mind the question is : &quot;I did this in the past, and got result X, what should i do now for a given problem/question while not knowing if the output answer has a positiv or negative result?&quot;", "comment_date": "2018-01-31T00:21:52Z", "likes_count": 0}, {"comment_by": "Thomas Islinger", "comment_text": "Best  I&#39;v ever heard about that stuff!", "comment_date": "2018-01-30T14:09:05Z", "likes_count": 0}, {"comment_by": "TechWithDesire", "comment_text": "OMG for real I&#39;m lost", "comment_date": "2018-01-30T10:47:53Z", "likes_count": 0}, {"comment_by": "Ruslan Nikolaev", "comment_text": "This is the best explanation of backprop i have ever seen. AMAZING WORK", "comment_date": "2018-01-30T00:57:11Z", "likes_count": 0}, {"comment_by": "KotSR", "comment_text": "How does the network know though that the 2 is the desired/correct output in order to make the back propagation feasable?", "comment_date": "2018-01-29T13:01:09Z", "likes_count": 0}, {"comment_by": "Optimistas777", "comment_text": "The T-Shirts are for US residents only aren&#39;t they ?", "comment_date": "2018-01-28T14:05:03Z", "likes_count": 0}, {"comment_by": "M. Zahit \u00d6zcan", "comment_text": "Wonderful explanations with intuitive animations. Thanks.", "comment_date": "2018-01-27T14:51:29Z", "likes_count": 0}, {"comment_by": "Karthik Aravind Krishnamurthy", "comment_text": "Which software did you use to make these animations ? Brilliant videos !", "comment_date": "2018-01-27T08:14:26Z", "likes_count": 0}, {"comment_by": "srinivas krishnaswamy", "comment_text": "Amazing way of teaching concepts.  I wish this was around when I was learning stuff.", "comment_date": "2018-01-25T02:17:55Z", "likes_count": 0}, {"comment_by": "Government Official", "comment_text": "I&#39;m pickle rick", "comment_date": "2018-01-23T05:54:11Z", "likes_count": 0}, {"comment_by": "Petr Hartmann", "comment_text": "Great series! Superb animation and clear explanation of a complex subject. Will the series continue? I&#39;d love to learn something about Self Organising Maps.", "comment_date": "2018-01-17T14:31:04Z", "likes_count": 0}, {"comment_by": "Orlando Rodriguez", "comment_text": "Great serie of videos. Thank you very much. However, I have a doubt: In minute <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=2m41s\">2:41</a> you say &quot;...that \u00a0change (this for weight &quot;n&quot;) is 32 times greater the same wiggle to that second weight (...). Here my point: Let&#39;s assume \u00a0a(n) = a(k) = a, \u00a0this is, the same value of &quot;a&quot; \u00a0for both neurons, (for simplification) , \u00a0and the &quot;wiggle&quot; (because this is the &quot;same wiggle&quot; for \u00a0any a , according your conditions) is \u00a0a constant &quot;k&quot;. \u00a0So, for any w, the change is \u00a0 \u00a0a(w+ k) - aw = ak. \u00a0That&#39;s it, no matter the value of w, the final effect on \u00a0C \u00a0is the same, this is independent of the value of w. \u00a0 Im I wrong in my analysis? \u00a0 Where is my mistake?", "comment_date": "2018-01-17T00:42:22Z", "likes_count": 0}, {"comment_by": "\u5f20\u4efb\u70b3", "comment_text": "Even if both my english and basic for the specific topic is poor, i can even understand! Amazing!", "comment_date": "2018-01-16T20:32:28Z", "likes_count": 0}, {"comment_by": "Anna McCann", "comment_text": "Thank you!", "comment_date": "2018-01-16T18:05:26Z", "likes_count": 0}, {"comment_by": "Yue Wang", "comment_text": "Am constantly in awe of how intuitive this is.", "comment_date": "2018-01-16T11:13:56Z", "likes_count": 0}, {"comment_by": "Travis Heck", "comment_text": "Dude....  The animations are absolutely mind blowing.  What program do you use for the animations?", "comment_date": "2018-01-14T17:23:35Z", "likes_count": 1}, {"comment_by": "Omar Cusma Fait", "comment_text": "This channel is absolutely my favourite!", "comment_date": "2018-01-12T22:45:58Z", "likes_count": 1}, {"comment_by": "Maziar Ghorbani", "comment_text": "a question!<br>is neural network a good approach for live data. No storing permanently or accessing the data in stages??? for instance detecting a straight line while samples are coming in. Making decision with what we have, present and previous samples in an ongoing process.", "comment_date": "2018-01-12T12:52:57Z", "likes_count": 0}, {"comment_by": "Robin Marty", "comment_text": "Is it possible that a neuronal network that gets trained with some data converges to two (or more) solutions for the task it is trained for ? That there are more than one and totally different weight / bias combinations to effectively perform the demanded task ?", "comment_date": "2018-01-11T13:28:03Z", "likes_count": 0}, {"comment_by": "Vidhu Chauhan", "comment_text": "beautifully explained..nice animation", "comment_date": "2018-01-11T07:52:20Z", "likes_count": 1}, {"comment_by": "Levi Pack", "comment_text": "Excellent", "comment_date": "2018-01-10T18:12:07Z", "likes_count": 1}, {"comment_by": "Yannic F", "comment_text": "Hey 3Blue1Brown,<br>first of all: I absolutly like your videos.<br><br>But i think, that i did find a mistake:<br>In the image showing at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m38s\">7:38</a> you show, which changes to the activations of the second block of neurons should be done to shrink the digit 0 neuron of the output. I think, that you mixed up the sign. To shrink the output the activation of a neuron, which has a positive connection to the output, should shrink and the activation of a neuron, which has a negative connection, should rise. In my opinion all the following arrows are the wrong way round too. The arrows of the digit 2 neuron are correct.<br><br>It would be great, if you could add a tag in the video, which informs the viewer about this change. I think, that it&#39;s critical for the understanding.<br><br>Kind regards<br>Yannic", "comment_date": "2018-01-09T12:38:28Z", "likes_count": 0}, {"comment_by": "Bruce Wang", "comment_text": "Great illustration and annotation!", "comment_date": "2018-01-07T14:41:23Z", "likes_count": 1}, {"comment_by": "Eniotna Yssaneb", "comment_text": "It&#39;s a bit confused for me:<br>is the cost function something used in deep learning or just something theorical that we try to approach with backprop process ?<br>And why is it faster to do these mini batches ?<br><br>I&#39;d love an answer, since I understood almost everything but this...", "comment_date": "2018-01-07T00:06:49Z", "likes_count": 0}, {"comment_by": "Squares", "comment_text": "Nice", "comment_date": "2018-01-06T22:29:43Z", "likes_count": 0}, {"comment_by": "Yash Gaikwad", "comment_text": "What is the point of taking such a strange cost function? I thought that squaring in the cost function was to amplify the error. But it should be effective only when all the data was larger than 1. n^2 grows exponentially only when n &gt; 1. When n &lt; 1, it decreases. So squaring (1.00-0.23)^2 = (0.77)^2 = 0.5929.", "comment_date": "2018-01-04T03:18:10Z", "likes_count": 0}, {"comment_by": "quebono100", "comment_text": "Man, i addicted to your videos, it is so deep and so easy to understand. Thx a lot", "comment_date": "2018-01-01T22:28:23Z", "likes_count": 1}, {"comment_by": "Danya Kelvich", "comment_text": "Great explanation, still don&#39;t get it.", "comment_date": "2018-01-01T20:29:53Z", "likes_count": 0}, {"comment_by": "Roy B", "comment_text": "i can&#39;t believe how smart some people are. aliens among us confirmed", "comment_date": "2017-12-30T15:31:03Z", "likes_count": 1}, {"comment_by": "Saman Rahbar", "comment_text": "Hey! Just want to shout you are my HERO! I&#39;m an AI and ML researcher and I LITERALLY cry watching those awesome explanations! YOU ARE MY HERO! &lt;3 From Canada!", "comment_date": "2017-12-30T03:51:02Z", "likes_count": 4}, {"comment_by": "Jason Katz", "comment_text": "amazing explanation", "comment_date": "2017-12-26T21:51:14Z", "likes_count": 1}, {"comment_by": "Lewis Lu", "comment_text": "best neural network explanation video on youtube hands down", "comment_date": "2017-12-26T00:56:22Z", "likes_count": 1}, {"comment_by": "Tim Kellermann", "comment_text": "would it make sense to start with a real small mini-batch and increas the size with the error of the network gowing down ?", "comment_date": "2017-12-24T11:12:00Z", "likes_count": 0}, {"comment_by": "Ali Alwerfally", "comment_text": "thank you so much", "comment_date": "2017-12-24T09:16:21Z", "likes_count": 1}, {"comment_by": "Alisson Damasceno", "comment_text": "When you think it&#39;s not awesome enough, this guy renders animations in 60 fps. Perfect :-D", "comment_date": "2017-12-24T02:33:32Z", "likes_count": 1}, {"comment_by": "legend519", "comment_text": "Great video series! Thank you so much for making these!<br><br>I&#39;m working on software with a few friends that requires handwriting recognition for some of the functionality. Can anybody direct me to (preferably free) data sets for written characters? I see that MNIST is for number characters only. <br><br>Any help is greatly appreciated. Thanks", "comment_date": "2017-12-24T01:43:39Z", "likes_count": 0}, {"comment_by": "Avice", "comment_text": "Your videos are amazing ! Thanks !!", "comment_date": "2017-12-24T01:09:37Z", "likes_count": 1}, {"comment_by": "Khushal Badhan", "comment_text": "Please create video series on reinforcement learning", "comment_date": "2017-12-23T13:42:21Z", "likes_count": 0}, {"comment_by": "Miguel Contreras", "comment_text": "It&#39;s pretty neat how this process guides you to a local minimum of the &quot;cost &quot; function, but there is a problem to this as far as I can see, which is that if you randomize the initial values of the biases and weights you will involuntarily preset your function to approach a local minimum that might not be even close to the lowest possible value of the function i.e. an absolute minimum. And I imagine that this can keep your final neural network from being as accurate as it could be. So, wouldn&#39;t it help to have some randomization between each step of backpropagation? this might help your function jump to another place where it might find a lower minimum than the previous one you were approaching. This has a problem though, because there might be an equal amount of possibilities of the function moving to a local minimum that is higher than the previous minimum you were approaching, which would be bad as it would reduce the accuracy of the output results of your network.<br><br>So my question is: is there any way of helping the function move to the absolute minimum or does it just depend on how lucky you are on getting a good set of random biases and weights at the beginning of the learning process?", "comment_date": "2017-12-22T23:36:50Z", "likes_count": 0}, {"comment_by": "Hrithik Bandaru", "comment_text": "i am understanding almost every concept he speaks about but i want to know how the actual program works how do we get the next number in each step i still dont get it and it bugs me someone help me", "comment_date": "2017-12-21T16:47:43Z", "likes_count": 0}, {"comment_by": "Antairez", "comment_text": "Sorry, very noob learner here. But on <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m50s\">7:50</a> each last layer neuron either outputs a positive or negative value as the preferred delta for the second to last layer, wouldn&#39;t that sort of cancel each other&#39;s efforts out?", "comment_date": "2017-12-20T07:42:12Z", "likes_count": 0}, {"comment_by": "John Coleman", "comment_text": "This was intresting, though I should be studying for finals, also I suck at math and have no idea what is going on mathematically\ud83d\ude02, though I get the neuroscience part", "comment_date": "2017-12-19T22:33:37Z", "likes_count": 0}, {"comment_by": "Rick Bergolla", "comment_text": "Remember to take a break you fellow viewers so you don&#39;t get over loaded with information", "comment_date": "2017-12-19T14:57:13Z", "likes_count": 0}, {"comment_by": "Arturas Karbocius", "comment_text": "X-ray use big data base of Detecting Guns Using Parametric Edge Matching is hundreds and thousand time easier to learn people recognize patterns, U2 plane photos prove that personal can  recognize rockets from agriculture equipment. Is impossible security personal to be sharp all year, x-ray security monitoring is more boring job than watch how paint dry.", "comment_date": "2017-12-18T18:20:37Z", "likes_count": 0}, {"comment_by": "Micky Tambunan", "comment_text": "LVQ, SOM, PCA (supervised &amp; unsupervised learning) ?<br>Please?", "comment_date": "2017-12-18T07:04:57Z", "likes_count": 1}, {"comment_by": "videoguy640", "comment_text": "Love this series", "comment_date": "2017-12-16T08:15:46Z", "likes_count": 1}, {"comment_by": "Alessandro Schneider", "comment_text": "Really perfect!!!!", "comment_date": "2017-12-15T22:56:16Z", "likes_count": 1}, {"comment_by": "Kolbein", "comment_text": "In this video: How to get much bang for the bucks!", "comment_date": "2017-12-15T21:02:47Z", "likes_count": 0}, {"comment_by": "welrod", "comment_text": "Best channel ever!", "comment_date": "2017-12-13T18:18:58Z", "likes_count": 1}, {"comment_by": "Phoe", "comment_text": "i love the end pi animation", "comment_date": "2017-12-12T11:22:19Z", "likes_count": 1}, {"comment_by": "Darin Hitchings", "comment_text": "For an intro video, this was very, very well done.  Congrats.  You&#39;re an excellent teacher.  And I think of myself as an excellent teacher (and I&#39;ve had 100&#39;s...)", "comment_date": "2017-12-12T05:53:44Z", "likes_count": 1}, {"comment_by": "Spektrob", "comment_text": "This is by far the best educational content i&#39;ve ever seen on youtube. Thank you and go on with it.", "comment_date": "2017-12-11T20:46:35Z", "likes_count": 1}, {"comment_by": "Owen de Heer", "comment_text": "i think you made a mistake at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m38s\">7:38</a> because first it says that the neuron for the two wants to go up. to do that the first neuron in the left layer needs to go up as well which is correct, but then it also says that the neuron for the 0 wants to go down and it says that the top left neuron needs to go up for that. is this wrong? i&#39;d think it needs to go down, right?", "comment_date": "2017-12-11T13:51:37Z", "likes_count": 0}, {"comment_by": "Nikoloz", "comment_text": "I have to ask: when you know the last neuron should give &quot;2&quot; the highest activation, you then look to adjust weights before the previous layer based on knowing which of that layer&#39;s neurons should most contribute to activating a &quot;2&quot;. But can you always know which combination of neurons in mid layers should give some specific outcome?<br><br>Because with backpropagation it looks like you either take neuron values as given and adjust weights, or weights given and adjust neuron activations from previous set of weights. But by checking ebery possible direction during gradiant descent, you check every combination of weoght adjustments in each iteration", "comment_date": "2017-12-09T17:14:56Z", "likes_count": 0}, {"comment_by": "Alexandte RFST", "comment_text": "Excellent", "comment_date": "2017-12-09T11:15:27Z", "likes_count": 1}, {"comment_by": "Joey Ortiz", "comment_text": "These visualizations are spot-on. Only a few people in the entire world need to make a great explanations backed by powerful visualizations about a topic - the rest of the world just needs to discover these. So much time wasted by learners trying to locate easily-digestible information, among all the inferior presentation methods out there. Glad to have found one of the best for this topic.", "comment_date": "2017-12-07T20:49:53Z", "likes_count": 98}, {"comment_by": "Samuel Ford", "comment_text": "I have just found your videos. I have enjoyed watching science educators on youtube for many years, but these videos are the best examples of complex ideas being explained clearly whilst still being entertaining. I can&#39;t believe I had never heard of this channel before.", "comment_date": "2017-12-07T17:52:41Z", "likes_count": 1}, {"comment_by": "ludvercz", "comment_text": "I hear drunk people might even stumble further down from a local minimum if there is a better one nearby.  So next time you see a guy who reeks of liqueur snoring in the ditch by the roadside, know that he isn&#39;t drunk, he is stochastic.", "comment_date": "2017-12-07T08:06:27Z", "likes_count": 0}, {"comment_by": "Romeo Kienzler", "comment_text": "Thanks so much for the video. It&#39;s on my personal top 3 list on the topic, kudos! What tool have you used for the animations? They are awesome!", "comment_date": "2017-12-06T10:09:16Z", "likes_count": 1}, {"comment_by": "Thijs de Wit", "comment_text": "This is exactly what I needed.  Thank you!", "comment_date": "2017-12-06T07:42:39Z", "likes_count": 1}, {"comment_by": "Calcifer777", "comment_text": "I just realized that the three blue students and the one brown teacher are not a coincidence, now I feel dumber than before... P.s. love your videos, please keep it up if you can, you are doing a wonderful thing here :)", "comment_date": "2017-12-05T22:10:12Z", "likes_count": 1}, {"comment_by": "Alias", "comment_text": "<a href=\"https://i.imgur.com/NmZsxYM.png\">https://i.imgur.com/NmZsxYM.png</a><br>Both of those are red, but we want one of the output neurons to raise in value and the other to decrease, in that case, why do we have to lower the weight &quot;twice&quot;? I&#39;d guess you have to decrease the activation for the  2 and increase it for the 0 (as you want it to be a null neuron).", "comment_date": "2017-12-04T15:52:05Z", "likes_count": 0}, {"comment_by": "Kotobii", "comment_text": "But how do things get their names to begin with?", "comment_date": "2017-12-01T01:24:41Z", "likes_count": 1}, {"comment_by": "Neil McFarlane", "comment_text": "Amazing series; the best I&#39;ve been able to find on explaining such a complex topic.  Thanks!", "comment_date": "2017-11-30T00:41:44Z", "likes_count": 1}, {"comment_by": "H dee", "comment_text": "Your animations are so good!! And when you do recaps and reference to what you have showed us..this is to awesome!", "comment_date": "2017-11-27T12:26:07Z", "likes_count": 1}, {"comment_by": "Ray Ng", "comment_text": "Amazing use of visuals to explain concepts. Well done man ...", "comment_date": "2017-11-26T02:01:31Z", "likes_count": 1}, {"comment_by": "Richard Fox", "comment_text": "this is pretty good explanation of what backprop. is doing, thanks for making this video!.. HOWEVER, I believe your explanation of SGD is incorrect. What you are explaining is Mini-batch gradient descent, SGD is looking at only one example at a time.", "comment_date": "2017-11-25T23:41:53Z", "likes_count": 0}, {"comment_by": "Rainfawkes", "comment_text": "so the video describes that each last node expresses how much the weights connecting to the second last layer should be adjusted. however it seems to gloss over how the very last layers weights are adjusted. my assumption is that each layer changes the weights attached to it, and then expresses the wanted changes in the next layer. that way a complete recursion could be done without simply forgetting the last layers weights.", "comment_date": "2017-11-25T20:55:21Z", "likes_count": 0}, {"comment_by": "SamSamSam", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=10m16s\">10:16</a> metaphor +1", "comment_date": "2017-11-25T05:02:29Z", "likes_count": 0}, {"comment_by": "Wajdan Ali", "comment_text": "Fun fact : Stroke patients are rehabilitated by detecting the &quot;intention&quot; of the patient&#39;s motion e.g arm and inturn electrically stimulated to create motion in the respective arm prooving the hebian theory as stated above.", "comment_date": "2017-11-24T18:06:28Z", "likes_count": 0}, {"comment_by": "juustgowithit", "comment_text": "You&#39;re awesome!", "comment_date": "2017-11-22T14:29:57Z", "likes_count": 1}, {"comment_by": "WhiteSpatula", "comment_text": "Throughout this series my mind goes back to a book I read many moons ago. On Intelligence by Jeff Hawkins. I think there most definitely are more than coincidental similarities between organic brains and learning machines. I know we\u2019re not \u201cthere\u201d yet, but I think we can rely on much more than intuition now to know we\u2019re on the right track to discovering the fundamentals of our mentals! Sorry. Last minute pun. Couldn\u2019t help it. -Phill, Las Vegas", "comment_date": "2017-11-22T06:38:15Z", "likes_count": 0}, {"comment_by": "\u2191\u00fdr", "comment_text": "The only thing I didn&#39;t understood in this video is how does the system does actualy choose between the first or second layer (or maybe they are them both changed but in a different proportion?).", "comment_date": "2017-11-21T21:24:08Z", "likes_count": 0}, {"comment_by": "Saim Mehmood", "comment_text": "Will be delighted to see English subtitles as well.", "comment_date": "2017-11-21T15:16:42Z", "likes_count": 1}, {"comment_by": "80sROCKKKKK", "comment_text": "IF YOU CAN PLEASE ANSWER THIS!!<br>Sorry for the caps, it&#39;s just that I really thought this out, and I wouldn&#39;t like it to go unanswered or at least looked at.<br><br>I have a question. Referring to the image at around <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m38s\">7:38</a>. So... the second neuron in the second last layer is bright, but its connection to the 0.2 neuron in the last layer is red. That means the value of the initial random weight is negative, which we don&#39;t want, since we&#39;d like the 0.2 neuron to be activated more, so we would want bright neurons like this second one in the second last layer, to positively stimulate it. So we&#39;d like the weight to go from the current red (negative) value, to a positive value (although not as positive as the very bright first neuron in the second last layer). Next you also said, change ai in proportion to wi. So, you&#39;d want the activation of that neuron to go down because the weight is negative. And the image shows that too. So now, what happens in the next iteration, when the weight goes up because we&#39;d like bright neurons like this one to be fired up more, and then purpose is kind of defeated because the neuron&#39;s brightness is decreased because at that moment the weight was negative (as you can see the arrow is red pointing down). Wouldn&#39;t that mean that the ai*wi product would always remain constant, never improving, in such cases? I mean, the decreased activation in the next round would furthermore reduce the incentive for the weight to be increased as much... consequently an initially bright neuron, positively stimulating the neuron we want stimulated, would end up a dull neuron with, likely, a still negative weight, although not as negative as before. Isn&#39;t that a waste?", "comment_date": "2017-11-19T20:09:36Z", "likes_count": 0}, {"comment_by": "Punit Pandey", "comment_text": "Best video on this topic so far !!!", "comment_date": "2017-11-19T17:33:12Z", "likes_count": 1}, {"comment_by": "yina jiang", "comment_text": "\u5389\u5bb3\uff01", "comment_date": "2017-11-19T10:32:17Z", "likes_count": 0}, {"comment_by": "R\u00e9mi Boutin", "comment_text": "Amazing work  !! Such a pleasure to understand those underlying notions ! Thank you so much", "comment_date": "2017-11-17T20:36:00Z", "likes_count": 2}, {"comment_by": "Taylor Allred", "comment_text": "Great Video!", "comment_date": "2017-11-16T16:33:51Z", "likes_count": 0}, {"comment_by": "Sharma Kunapalli", "comment_text": "It would be great if you can do a video of the poincare conjecture. Thanks!", "comment_date": "2017-11-15T15:24:10Z", "likes_count": 0}, {"comment_by": "ixuz07", "comment_text": "These videos are very very very very good", "comment_date": "2017-11-14T23:53:46Z", "likes_count": 1}, {"comment_by": "Ritwik Mishra", "comment_text": "I am not able to get what method you are using? <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=5m08s\">5:08</a> you mentioned there 3 ways. Out of which you explained second method and then moved over to third method <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=6m41s\">6:41</a><br>Then you suddenly jumped and started using second method at <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=8m31s\">8:31</a>. My question is, how can you used 2nd and 3rd method at  the same time? In other words, how can you change weights proportional to activation AND change activation proportional to weights? (This looks like a race around condition)", "comment_date": "2017-11-14T06:19:37Z", "likes_count": 0}, {"comment_by": "Roman Vey", "comment_text": "So cool explanation and animations, amazing!", "comment_date": "2017-11-13T22:27:29Z", "likes_count": 1}, {"comment_by": "Alex Van de Kleut", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m25s\">12:25</a> femat - tease... LOL.", "comment_date": "2017-11-12T20:17:54Z", "likes_count": 0}, {"comment_by": "Thomas O'Dea", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=11m36s\">11:36</a> Is that outdated Python...? xrange() is gone", "comment_date": "2017-11-12T02:07:28Z", "likes_count": 0}, {"comment_by": "David Jurelius", "comment_text": "This series is the best I&#39;ve seen on neural networks. Thank you for making it. \u2764\u2764\u2764", "comment_date": "2017-11-08T11:21:33Z", "likes_count": 2}, {"comment_by": "Kaixo", "comment_text": "You are amazing!! Best explanation I&#39;ve seen on the internet yet!!", "comment_date": "2017-11-08T07:50:47Z", "likes_count": 45}, {"comment_by": "Larry Bird", "comment_text": "wooooooooooow! this guy is amazing! F*(@^  school, this made more sense then my computer science degree! lol", "comment_date": "2017-11-08T06:09:39Z", "likes_count": 2}, {"comment_by": "frozenbacon", "comment_text": "Wow, this is weird. Your videos got me through some of my math classes in college (you uploaded linear algebra series when I was taking linear algebra, they were a GREAT help) and now you are talking about something I work with right now. It is neat to see someone talk about it in a plain fashion, but the actual mathematics are a lot more clear to me than your video. Like all you have to tell me is that the gradient is determined by the change of the cost function in the relation of the previous layer (or, more bluntly, determine the partial derivatives) and I would understand. But your videos aren&#39;t for people who have a 100% solid understanding of the core mathematical concept, and this video accomplishes that beautifully. Just a lot of extra words for people who know what is going on, though :)", "comment_date": "2017-11-08T04:48:53Z", "likes_count": 0}, {"comment_by": "Rodrigo Appendino", "comment_text": "How are these animations made?", "comment_date": "2017-11-08T00:18:16Z", "likes_count": 0}, {"comment_by": "Grosser Salat", "comment_text": "This series is really well done and so important - thank you so much!", "comment_date": "2017-11-07T13:07:57Z", "likes_count": 1}, {"comment_by": "Goutham Reddy Kotapalle", "comment_text": "very well!!", "comment_date": "2017-11-07T11:45:05Z", "likes_count": 1}, {"comment_by": "Max Smith", "comment_text": "Hello, <br><br>I&#39;d like to start by saying your videos are amazing and beyond intersting. <br>One thing I&#39;m still confused about is how to update the biases through backpropagation?<br><br>Thanks! and keep it up!", "comment_date": "2017-11-07T00:27:39Z", "likes_count": 4}, {"comment_by": "Rented Mule", "comment_text": "I can&#39;t claim to have understood everything from the first watch-through of this series, and I will watch these videos again with pen and paper in hand, but even this first viewing has made neural networks go from pure witchcraft and wizardry to something that actually makes sense in my head.<br><br>I can&#39;t possibly thank you enough for posting these videos.", "comment_date": "2017-11-06T23:49:07Z", "likes_count": 1091}, {"comment_by": "sergio urquijo", "comment_text": "Mannnn this videos keep getting better and better. This is gold for me!!", "comment_date": "2017-11-06T21:16:47Z", "likes_count": 0}, {"comment_by": "Dank Mastor", "comment_text": "Love your videos! Could you PLLLEAAASSEEE do one on Fourier Transforms? I&#39;m in Signal Processing and this is a large portion of the curriculum. Me and many others are struggling in building an intuitive understanding of the underlying concept/theory.", "comment_date": "2017-11-06T19:49:37Z", "likes_count": 0}, {"comment_by": "Brendan Sullivan", "comment_text": "What is backpropagation and what is it actually doing? | Deep learning, chapter 3", "comment_date": "2017-11-06T19:26:17Z", "likes_count": 0}, {"comment_by": "pokedude westorn", "comment_text": "Your next video should be about the 3-D sphere to 2-D surface mapping problem and how it shuts up flat earthers. That would be so cool!", "comment_date": "2017-11-06T19:01:54Z", "likes_count": 0}, {"comment_by": "Sigfrid Stj\u00e4rnholm", "comment_text": "Is it possible for the algorithm to be garanteed to find a global minimum of the cost function? It would be computationally demanding is my guess, but better for the neural network, right?", "comment_date": "2017-11-06T18:15:34Z", "likes_count": 0}, {"comment_by": "Nigel Murillo", "comment_text": "Ok but can we talk about how bad some of those people&#39;s handwriting is when trying to write a number lol", "comment_date": "2017-11-06T18:06:12Z", "likes_count": 0}, {"comment_by": "Kenzi Jeanis", "comment_text": "Is it just me, or are there black bars across the top and bottom?", "comment_date": "2017-11-06T17:14:45Z", "likes_count": 0}, {"comment_by": "Patrick Apom", "comment_text": "Hi! Great video as always!<br>Anyway, I think you made a mistake around 7&#39;40 when you display all the tiny arrows for each output neuron as neuron 2 needs to increase (so positive weights need to increase and negative weights need to decrease) but the other neurons (0 tou 9) need to decrease and so the positive weights need to decrease (so with a red arrow) and negative weights need to increase (so with a blue arrow), except you kept using blue and red arrows respectively instead...", "comment_date": "2017-11-06T08:39:41Z", "likes_count": 1}, {"comment_by": "Nikita Luparev", "comment_text": "I really like an approach of this tutorials. They give me a high-level description of how this works and also really a deep mathematical foundation of particular algorithms. I&#39;ve been looking for such material for really long time. There are plenty of material out there which vague explains to you how things work on high level, but lack of mathematical foundation don&#39;t allow you to fully understand the idea behind it.<br><br>really look forward to learning more from you.", "comment_date": "2017-11-05T18:31:26Z", "likes_count": 0}, {"comment_by": "RoGeorgeRoGeorge", "comment_text": "Please, please, loose the musical background!<br><br>At first, I noticed something was terribly annoying. Then, once I realized it was the background music, it was game over. Couldn&#39;t focus on the content any more. Now, looking for some audio filters so I could watch the rest.<br>:o/<br><br>Otherwise, outstanding explanations and remarkable animations for each and every video.<br>Thank you very much for such a high quality channel.", "comment_date": "2017-11-05T17:55:50Z", "likes_count": 0}, {"comment_by": "Reckless Roges", "comment_text": "Would uploading videos with &gt; 24 hour gap help clarify which order to watch them?", "comment_date": "2017-11-05T16:28:33Z", "likes_count": 0}, {"comment_by": "SpaceOwl", "comment_text": "Great Video!\u00a0<br><br>But I don&#39;t understand how it works, when you use the &#39;mini-batches&#39;...<br><br>Are you like recording how each sample in the batch &#39;wants&#39; to change the last layer and then afterwards you back propagate the average of the batch or something like that?<br><br>But I thought that how weights are adjusted is proportional to the activation of the neurons that they are attached to. And these activations are completely different for each input?? Do you average the activations, too?<br><br>Like, you have to adjust the network so it reacts a certain way to a certain input. And then You just give it the average of like 10 inputs, and then reward it when it gives you the average of the desired outputs/classifications? What if you gave it a batch with all numbers from 0 to 9 in it - would you reward it for calssifying the input as <br>0 -- 0.1<br>1 -- 0.1<br>2 -- 0.1<br>...<br><br>?<br><br><br>I just can&#39;t wrap my head around how this would work.. :(", "comment_date": "2017-11-05T15:48:48Z", "likes_count": 0}, {"comment_by": "david howard", "comment_text": "Google chrome flags the t-shirt link as harmful", "comment_date": "2017-11-05T15:20:54Z", "likes_count": 0}, {"comment_by": "Petch85", "comment_text": "love these videos:<br>How do one &quot;calculate&quot; hvor many layers and nodes one need for at given problem?<br>fx if i need to approximate y=x^2 for x between 0 and 100, I suspect that would be way easier than recognize a face in a 10 MP color photo. But have due I estimate the optimal size of my network?<br>Keep up the good work.", "comment_date": "2017-11-05T13:35:16Z", "likes_count": 0}, {"comment_by": "Rafael Marques", "comment_text": "I&#39;m not versed in CS, can anyone explain to me the part around <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m20s\">4:20</a> where he says we should keep track of the value we wish it would represent? My question is, does it mean that all neural networks or this one specifically can only work properly based on a initial help from outside telling it what it should read when it did wrong at the beginning? This might seem like a dumb question, but I&#39;ve always visualized it being more independent. Putting it more clearly, it seemed (and it might as well be just my own ignorance of the subject ofc) as it went like this: YOU saw a two and realized the computer didn&#39;t interpreted it as &quot;A TWO&quot; and then made some adjustments to make it do so. How wrong am I?", "comment_date": "2017-11-05T11:42:58Z", "likes_count": 0}, {"comment_by": "Jan Rensen", "comment_text": "This is amazingly amazing!", "comment_date": "2017-11-05T11:36:03Z", "likes_count": 0}, {"comment_by": "Cedric Chee", "comment_text": "So far, I think this is the best intuitive intro to backprop I&#39;ve seen. This channel means a lot to me. Because of its teaching style, I managed to get back to learning and grok maths while I was studying machine learning last year, 13 years since doing my undergrad in CS. It&#39;s almost the end of 2017 and I still keep hearing from the people I talked to that they fear/hate maths because they think it&#39;s a tough subject to tame. May be this example tell us why our education system (internationally) is still broken? As an aside, David Perkins, in his book &quot;Making Learning Whole&quot; also touch about this widespread diseases of the educational system, namely &quot;elementitis&quot;. I think we can do better. Grant is doing great work to lower the barrier and making math more accessible to everyone. This is not an easy feat. I think we need some sort of concerted effort for encouraging more people to teaching maths or any subject through intuition. Visualization is one way to improve the teaching methodology. We can also distill intuition from stories, feelings, situations, etc. More examples, see:<br><a href=\"https://distill.pub/\">https://distill.pub/</a><br><a href=\"http://colah.github.io/\">http://colah.github.io/</a>", "comment_date": "2017-11-05T09:14:53Z", "likes_count": 4}, {"comment_by": "Point of view", "comment_text": "Please do an episode in Deep reinforcement learning and policy gradients.", "comment_date": "2017-11-05T05:51:25Z", "likes_count": 0}, {"comment_by": "lavtien", "comment_text": "Thanks you!", "comment_date": "2017-11-05T04:25:33Z", "likes_count": 0}, {"comment_by": "FBU", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=3m24s\">3:24</a> that animation HNNNNNGGG :0", "comment_date": "2017-11-05T03:11:41Z", "likes_count": 1}, {"comment_by": "\u041e\u0445\u0442\u0435\u0440\u043e\u0432 \u0415\u0433\u043e\u0440", "comment_text": "Do we need to adjust last weights if we are adjusting weights before it?", "comment_date": "2017-11-05T00:12:24Z", "likes_count": 0}, {"comment_by": "Dave J", "comment_text": "3Blue1Brown Is there a function that describes the tradeoff in computational speed compared to accuracy when training the cost function of the neural network? e.g. would decreasing the size of mini-batches further be detrimental to the total speed of the process?", "comment_date": "2017-11-04T23:53:06Z", "likes_count": 0}, {"comment_by": "ailijic", "comment_text": "Can you do a video on the vanishing gradient problem?", "comment_date": "2017-11-04T23:26:49Z", "likes_count": 1}, {"comment_by": "ct2034", "comment_text": "Thanks for this again really great video. But one thing that did not really came across is: At around <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m40s\">7:40</a> you show how the second to last layers activations are to be changed. But how does this affect the connection weights? And how to define the weights of the connections from their to the output layer? Or do you never change them?", "comment_date": "2017-11-04T20:04:01Z", "likes_count": 0}, {"comment_by": "TiagoTiago", "comment_text": "What if you apply the backpropagation at each step (not in batches, each test), without averaging, just applying it more weakly, like with 10% of the strength or something like that?", "comment_date": "2017-11-04T19:57:12Z", "likes_count": 0}, {"comment_by": "Stick Figure", "comment_text": "Just a heads up. The &quot;free&quot; t-shirt is $10. Lol. You have to launch a job, which is a minimum of $10. And it doesn&#39;t tell you this until you&#39;ve spent a bunch of time setting up a job.<br>I ended up doing it anyway because I imagine 3b1b gets a commission, and I liked the shirt. $10 isn&#39;t too bad for it.", "comment_date": "2017-11-04T18:11:12Z", "likes_count": 0}, {"comment_by": "jomamas", "comment_text": "Absolutely amazing videos that should have been done years ago.", "comment_date": "2017-11-04T18:01:31Z", "likes_count": 1}, {"comment_by": "keyur paralkar", "comment_text": "I got to understand backpropogation more easily from this video than From Andrew Ng&#39;s backpropogation machine learning video. Great video.", "comment_date": "2017-11-04T17:21:03Z", "likes_count": 2}, {"comment_by": "Sapient Pearwood", "comment_text": "It seems obvious that not all training data will have the same impact. Maybe certain examples clarify some crucial ambiguity, whereas others just solidify what the network has already figured out. I wonder if there is a way we can &quot;ask&quot; a semi-trained network what training data would be most useful to it? Sort of like increasing supervision to decrease training iterations.", "comment_date": "2017-11-04T17:15:29Z", "likes_count": 0}, {"comment_by": "Ryan", "comment_text": "(784/10)^(1/3)=x. Penultimate layer=ceil(10x), second layer=floor(784/x)", "comment_date": "2017-11-04T17:04:44Z", "likes_count": 0}, {"comment_by": "Conguy Monty", "comment_text": "Would your neural network work better if you used a Pseudo hilbert structure to make your 2d data into a 1d matrix?", "comment_date": "2017-11-04T16:37:02Z", "likes_count": 0}, {"comment_by": "Little Owl", "comment_text": "@3Blue1Brown Can we use composite function to explain this.", "comment_date": "2017-11-04T15:11:48Z", "likes_count": 0}, {"comment_by": "Little Owl", "comment_text": "Can this be explained using &quot;composite function&quot;", "comment_date": "2017-11-04T15:10:28Z", "likes_count": 0}, {"comment_by": "Krish Nathan", "comment_text": "Damn this video is fire", "comment_date": "2017-11-04T14:48:58Z", "likes_count": 0}, {"comment_by": "Aaron Cottle", "comment_text": "Really well done. I&#39;ll probably have to go through the entire series once again, but that&#39;s to be expected. There is so much content in so little time. I&#39;ll probably actually get into designing a neural network from the ground up.", "comment_date": "2017-11-04T14:08:39Z", "likes_count": 0}, {"comment_by": "feihcsim", "comment_text": "Back propagation has never been cleared - thank you so much !!", "comment_date": "2017-11-04T13:57:55Z", "likes_count": 0}, {"comment_by": "Dodi", "comment_text": "Nope. I love your videos and your style, but seriously...I&#39;m swimming in a pool of darkness. Hope that bite by bite I&#39;ll sooner or later get to grasp what you are talking about.", "comment_date": "2017-11-04T13:44:05Z", "likes_count": 0}, {"comment_by": "sihingbenni", "comment_text": "Q", "comment_date": "2017-11-04T13:42:28Z", "likes_count": 0}, {"comment_by": "wongwanchap", "comment_text": "If we initial the neuron network as uniform network, all weight between each neuron are the same. As a result, the network will not learn anything no matter how long you train the network, because each hidden neuron are equvalence so no differentiation will happen.<br><br>Irony, human learn things because human have biase.", "comment_date": "2017-11-04T13:41:00Z", "likes_count": 0}, {"comment_by": "MrMegaPussyPlayer", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m55s\">12:55</a> and if I take AlphaGo Zero n consideration all of this is yesterdays news / outdated (that humans train AI system / crowdflower ... but most likely the rest of the video, too)<br>AlphaGo Zero was not trained at all. It just figured out how to play Go better than anything that came before (and those where better than any human can ever be) on its own. And it even uses less processing power than anything before.", "comment_date": "2017-11-04T13:16:57Z", "likes_count": 0}, {"comment_by": "Tejas", "comment_text": "Dude how are you making these animations", "comment_date": "2017-11-04T12:56:06Z", "likes_count": 17}, {"comment_by": "0 1", "comment_text": "I&#39;d much prefer this channel to create small but fun math videos rather than jumping on the deep NN hype train.", "comment_date": "2017-11-04T12:00:14Z", "likes_count": 0}, {"comment_by": "Iliyan Zankinski", "comment_text": "Would you please slightly increase the sound level of your videos so we can listen to them on the go. Great content!!!", "comment_date": "2017-11-04T11:27:22Z", "likes_count": 1}, {"comment_by": "Kaliin", "comment_text": "Omg that editing, how long did it take too make this video? <br>The content is also really good, you just won a new subscriber!", "comment_date": "2017-11-04T11:11:11Z", "likes_count": 0}, {"comment_by": "Bram Beer", "comment_text": "I have been waiting for this vid!", "comment_date": "2017-11-04T11:05:41Z", "likes_count": 0}, {"comment_by": "Hyunsung Go", "comment_text": "Best explanation of blackpropagation I&#39;ve seen so far!", "comment_date": "2017-11-04T10:28:03Z", "likes_count": 1}, {"comment_by": "Rahul MK", "comment_text": "You are awesome!!<br>Everything you teach I understand Crystal clear.<br>Keep making more videos on Machine learning.", "comment_date": "2017-11-04T06:38:51Z", "likes_count": 0}, {"comment_by": "EponaMind EponaShoe - Metron", "comment_text": "The graphic animations are fantastic.  Could you tell us how you make these videos?  Is there an available software tool that you use to create them.  Great job.", "comment_date": "2017-11-04T06:21:55Z", "likes_count": 0}, {"comment_by": "Shakes McTremens", "comment_text": "&quot;For those of you who <i>do</i> want to dive into the math ...&quot;<br>Oh please.", "comment_date": "2017-11-04T06:17:32Z", "likes_count": 2}, {"comment_by": "Ming-Feng Ho", "comment_text": "It&#39;s COOOL!! better than any lecture I&#39;ve taken in the university", "comment_date": "2017-11-04T06:00:27Z", "likes_count": 0}, {"comment_by": "Harlequin314159", "comment_text": "You have to pay CrowdFlower like $10 to be able to run a job on a trial account. You should mention that it is not exactly a free t-shirt.", "comment_date": "2017-11-04T04:43:16Z", "likes_count": 0}, {"comment_by": "anonimo aninimus", "comment_text": "am I the only one who doesn&#39;t understand anything from any of these videos?", "comment_date": "2017-11-04T04:39:11Z", "likes_count": 0}, {"comment_by": "Artem McKay", "comment_text": "After dozens of videos and text explanations, this is the first one, when I could really wrap my head around it! <br>Thank you so much! You&#39;re the best!", "comment_date": "2017-11-04T04:38:37Z", "likes_count": 0}, {"comment_by": "K van der Veen", "comment_text": "Is (quasi-)Newton method also ever used for &quot;training&quot; a neural network? So instead of only using the gradient, also use the Hessian. Or does the use of stochastic gradient descent, so mini-batches, make the Hessian potentially fluctuate so much between batches that the normally proposed search direction in general does not improve the convergence speed?", "comment_date": "2017-11-04T04:32:44Z", "likes_count": 0}, {"comment_by": "DoemanY", "comment_text": "love you", "comment_date": "2017-11-04T04:06:21Z", "likes_count": 1}, {"comment_by": "PFC1234", "comment_text": "What happens if you reach a local minimum, which is nowhere near the absolute one? Is there a solution?", "comment_date": "2017-11-04T04:04:57Z", "likes_count": 0}, {"comment_by": "Tony Souter", "comment_text": "Is that you pictured toward the end?", "comment_date": "2017-11-04T03:55:42Z", "likes_count": 0}, {"comment_by": "Kickmonlee", "comment_text": "NOTE: It is not a legitimately free T-shirt, you have to pay a non-zero amount in the end", "comment_date": "2017-11-04T03:52:00Z", "likes_count": 0}, {"comment_by": "Stefano Cvitanich", "comment_text": "OMG! This is awesome and so are you! Yesterday I had my first class of Neural Networks and I showed my teacher your vids. And today you upload the backpropagation one. Thanks a lot dude! \ud83d\ude04\ud83d\udc4d\ud83c\udffb", "comment_date": "2017-11-04T03:28:48Z", "likes_count": 0}, {"comment_by": "RemusKingOfRome5", "comment_text": "You made jesus cry ....", "comment_date": "2017-11-04T03:10:13Z", "likes_count": 0}, {"comment_by": "saitaro", "comment_text": "Grant, you&#39;re a diamond.", "comment_date": "2017-11-04T02:44:08Z", "likes_count": 343}, {"comment_by": "Ben Usman", "comment_text": "I&#39;d personally appreciate if you could do more videos on more complex or abstract topics as those you discussed before. Neural nets are pretty visual by themselves, but topology, algebra and other topics you discussed before are hard to grasp visually, and your videos are absolutely amazing in this sense!", "comment_date": "2017-11-04T02:37:00Z", "likes_count": 0}, {"comment_by": "Ben Morgenstern", "comment_text": "Is it possible to see your code on this. Thanks", "comment_date": "2017-11-04T02:17:21Z", "likes_count": 0}, {"comment_by": "Khoa Tran", "comment_text": "Great video. On the side, I think we should not be saying to each other that this has anything to do with how our brains work. it&#39;s really irrelevant and I don&#39;t remember having to look at thousands of images of the same things to learn about such concepts as number or dog vs cat. There is no gradient engine in my brain. Evolution isn&#39;t a design optimization program.", "comment_date": "2017-11-04T01:45:26Z", "likes_count": 0}, {"comment_by": "turbo gt", "comment_text": "is the god teaching us?", "comment_date": "2017-11-04T01:11:58Z", "likes_count": 36}, {"comment_by": "Jay Schauer", "comment_text": "Beautiful video!", "comment_date": "2017-11-04T01:08:16Z", "likes_count": 0}, {"comment_by": "Jacob Claassen", "comment_text": "How does carykh &#39;s work where he uses survival of the fittest?", "comment_date": "2017-11-04T00:58:12Z", "likes_count": 0}, {"comment_by": "Mark Fedorov", "comment_text": "Man, the clarity and the animations make your videos masterpieces", "comment_date": "2017-11-04T00:28:09Z", "likes_count": 4}, {"comment_by": "Patrick Hodson", "comment_text": "\u201cSongbird of our generation\u201d lmao \ud83d\ude02", "comment_date": "2017-11-04T00:20:07Z", "likes_count": 4}, {"comment_by": "Robert MacKinnon", "comment_text": "You might think that your videos are fodder for university students boning up on a subject, or mathematicians/engineers in the early stages of their careers - basically that you cater to a younger audience. I&#39;ll have to prove you wrong.  I&#39;m in my early 60&#39;s and have been involved with information technology in some form or fashion my entire career.  I enjoy learning; always have.  I&#39;ve viewed many of your videos only because they interest me and have you subscribed on my Youtube account so as to get notifications of updates.  I find the topics about which you speak  fascinating and am a bit jealous of those university grads today who now have access to this material at their fingertips.  I wish with all my heart that I was able access these videos back when I was in university.  It would have made life SOOO much easier for me back then.  Your pedagogic skills are astounding, demonstrated by your ability to communicate difficult subjects precisely, concisely and simply.  The animation format is integral with the presentation, adding to the delivery of the material. I salute you!!  Please keep these videos coming.", "comment_date": "2017-11-04T00:13:33Z", "likes_count": 6490}, {"comment_by": "withebois", "comment_text": "I think I understand... But let&#39;s say if we want to change the activation&#39;s in layer 3 by whatever, which is determined by what we want the last layer to show, then if we back propagate and see what we want layer 2 to show, wouldn&#39;t changing the weights along any of the connections between the input layer and layer 2 then cause a complete change in the activation&#39;s in layer 2, therefor changing how much the weights connecting layer 2 and 3 need to change? Is that how this would work? Back propagate all the way back to the input layer, and change all the weights by a certain amount, then redo back propagation to the second layer and change all those weights by a certain amount, then to the third and so on? (I left out biases just for simplicity purposes)", "comment_date": "2017-11-03T23:50:34Z", "likes_count": 0}, {"comment_by": "Ape-ocalypse", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=7m39s\">7:39</a> onwards:<br>These are the effects that you would like to have on the activations themselves, not the weights. How would you determine how much you want the weights to change? Do you just go back until you reach the beginning layer, and then adjust the weights between the first and second layer to achieve the desired changes on the activations on the second layer, then adjust the weights between the second and third layers, etc until you reach the output layer?", "comment_date": "2017-11-03T23:48:25Z", "likes_count": 4}, {"comment_by": "Chrysippus", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m50s\">9:50</a> the notion of &quot;mini-batches&quot; is mentioned, and I can&#39;t avoid thinking, why not smaller batches? Or larger? How is the optimal size for those determined?<br><br>For that matter, I also still don&#39;t know how to decide number of hidden layers/neurons. There&#39;s a proper way, right?", "comment_date": "2017-11-03T23:33:27Z", "likes_count": 0}, {"comment_by": "PennyAfNorberg", "comment_text": "Hessian algorithms? ( the gradient of the gradient, matrix) Is those out in the cold now? I used one in a summer course &#39;95.", "comment_date": "2017-11-03T23:27:01Z", "likes_count": 0}, {"comment_by": "sean mortaz", "comment_text": "another fantastic video!", "comment_date": "2017-11-03T23:15:19Z", "likes_count": 0}, {"comment_by": "Guy Roland", "comment_text": "Thanks a lot for all your videos. That is always a pleasure to learn with you.", "comment_date": "2017-11-03T22:13:19Z", "likes_count": 1}, {"comment_by": "Paul Morenkov", "comment_text": "Wonderful video! However, I\u2019m not understanding something: how is it that stochastic gradient descent is faster? The way I understand it, you still go through all of the data just in small groups at a time, so wouldn\u2019t that mean that the same amount of computation is done?", "comment_date": "2017-11-03T22:09:01Z", "likes_count": 0}, {"comment_by": "Jamie Thelin - Music", "comment_text": "Why do the mini batches need to be randomised? aren&#39;t the inputs already randomised?", "comment_date": "2017-11-03T22:06:17Z", "likes_count": 0}, {"comment_by": "10xorTensor", "comment_text": "Thank you for explaining how the &quot;magic black box we don&#39;t know how it works&quot; thing works", "comment_date": "2017-11-03T21:48:14Z", "likes_count": 0}, {"comment_by": "MegaICS", "comment_text": "SONGBIRD OF OUR GENERATION", "comment_date": "2017-11-03T21:37:41Z", "likes_count": 0}, {"comment_by": "GoriceXI", "comment_text": "This is how artificial entities will gain power us.", "comment_date": "2017-11-03T21:13:26Z", "likes_count": 0}, {"comment_by": "Brayden Haines", "comment_text": "Fantastic video man, I understand most of it now thanks to you! So much effort went into this video and it shows", "comment_date": "2017-11-03T21:09:02Z", "likes_count": 1}, {"comment_by": "realcygnus", "comment_text": "the best", "comment_date": "2017-11-03T20:44:59Z", "likes_count": 1}, {"comment_by": "PlankMaster", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m09s\">9:09</a> Where did w0 go ?", "comment_date": "2017-11-03T20:25:59Z", "likes_count": 0}, {"comment_by": "TJ Reynolds", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=4m09s\">4:09</a><br>That two is definitely a three. And a six too. It&#39;s also probably a one", "comment_date": "2017-11-03T20:24:27Z", "likes_count": 0}, {"comment_by": "James E", "comment_text": "So after training, it seems like you should be able to reverse the direction of this neural network to &quot;draw&quot; a number. That is, take a number and then create a pixel map that best represents that number from the neural network&#39;s perspective. This would be similar to the pixel maps that were shown for the 2nd layer neurons in the second video, but then take a weighted average of those maps to generate a pixel map for the 3rd layer neurons and repeat this step once more for the 4th layer neurons. I am really curious what the map would look like for each number, since the position of the numbers it is trained on are not necessarily consistent. Maybe it would be a blurry &quot;probabilistic&quot; combination of all of the training numbers, or maybe the pixel combination that best represents a &quot;2&quot; to the neural network would be a seemingly random noise pixel map like the one recognized as a 5 in the second video.", "comment_date": "2017-11-03T20:12:24Z", "likes_count": 0}, {"comment_by": "ryan powell", "comment_text": "You talked a lot about how the weights and activations are adjusted/wish to be adjusted. But you didn\u2019t talk in specifics about how you decide to adjust the bias, or maybe I misunderstood. Clarification would be appreciated!", "comment_date": "2017-11-03T20:08:50Z", "likes_count": 0}, {"comment_by": "TheWingDings1", "comment_text": "Why don&#39;t you have ads on these. I have no money for patreon and I want to help you out", "comment_date": "2017-11-03T20:05:20Z", "likes_count": 68}, {"comment_by": "42 scientist", "comment_text": "At least we don\u2019t have to think about G64 dimensions", "comment_date": "2017-11-03T19:11:32Z", "likes_count": 0}, {"comment_by": "Ali Y\u00fcklet", "comment_text": "Thank you soooo much! This channel is pure *GOLD*!", "comment_date": "2017-11-03T18:55:58Z", "likes_count": 3}, {"comment_by": "Shreyansh Darshan", "comment_text": "You have made life so much easier.", "comment_date": "2017-11-03T18:32:43Z", "likes_count": 3}, {"comment_by": "Daniel Astillero", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=9m00s\">9:00</a> Eureka moment. I cried.", "comment_date": "2017-11-03T18:24:03Z", "likes_count": 420}, {"comment_by": "Erosion", "comment_text": "I&#39;ve been labeling bboxes for days and my net is still below the 50/50 precision/recall diagonal<br>Heck me, i&#39;m crowdflowering this shit :o)", "comment_date": "2017-11-03T18:21:02Z", "likes_count": 0}, {"comment_by": "Alp \u00c7evikel", "comment_text": "Thanks for new video, its amazing", "comment_date": "2017-11-03T18:11:49Z", "likes_count": 1}, {"comment_by": "Gustav Rydstedt", "comment_text": "Really appreciate these videos. Thank you.", "comment_date": "2017-11-03T18:08:12Z", "likes_count": 1}, {"comment_by": "Killer Racoon", "comment_text": "Sir<br>I have a question.<br>Is 0 a polynomial?", "comment_date": "2017-11-03T17:59:40Z", "likes_count": 0}, {"comment_by": "abcd12221", "comment_text": "Great vid grant! Anyone else think additional trans-layer weights and operations would improve accuracy of the system? Also, I think a next-gen neural-net might look more like E8 than our current 2d layer model. In general, a multidimensional fractal with weights both within a level of hierarchy and across levels. A little word-soupy, but maybe it will inspire someone :)", "comment_date": "2017-11-03T17:58:03Z", "likes_count": 0}, {"comment_by": "Anant Mishra", "comment_text": "finally!<br>this video was for which i was waiting....", "comment_date": "2017-11-03T17:47:59Z", "likes_count": 1}, {"comment_by": "Harold Bradford", "comment_text": "Awesome!", "comment_date": "2017-11-03T17:39:21Z", "likes_count": 1}, {"comment_by": "John Chessant", "comment_text": "Thank you 3B1B for this series! I enjoyed it immensely!!", "comment_date": "2017-11-03T17:30:21Z", "likes_count": 2}, {"comment_by": "TheEVEInspiration", "comment_text": "I love science videos without calculus, they are the only ones that make sense to me. Knowing a formula to do something gives me no insight, but start with the insight and the formula is just a secondary issue (if at all).<br><br>To me maths (and its notation) always seems to over-complicating problems while disregarding the practical effects of reality, which makes it very non-intuitive and a bad learning tool (for me).<br><br>This video series of yours is very nice so far, I got the gist of it before I started viewing them and you just sharpened the image with each video a bit.", "comment_date": "2017-11-03T17:22:25Z", "likes_count": 1}, {"comment_by": "Guy Ross", "comment_text": "Amazing. Thank you!", "comment_date": "2017-11-03T17:19:53Z", "likes_count": 1}, {"comment_by": "Ruthvik Gundeti", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m27s\">12:27</a>", "comment_date": "2017-11-03T17:14:52Z", "likes_count": 0}, {"comment_by": "Luciano De Benedictis", "comment_text": "For me the most difficult part about maths is ALWAYS notation, as they expect me to know what exactly that greek letter stands for in every situation. Or maybe this is what&#39;s it&#39;s all about", "comment_date": "2017-11-03T17:02:40Z", "likes_count": 3}, {"comment_by": "Alex Hawco", "comment_text": "First of all great video, super interesting with excellent visuals as always.<br><br>One question that I&#39;d like to ask is regarding the end goal for the output of the entire system. If for example the system intakes the image of a &quot;2&quot; you used as the main example in this video, you seemed to be implying (although never explicitly stated) that the ideal output of the network would be a 1.0 in the &quot;2&quot; node and a 0.0 in the other 9 nodes of the final layer. I&#39;m not convinced that this is the most effective result to aim for.<br><br>Again going back to the &quot;2&quot; used as an example in the video, obviously it most closely resembles a 2 so the system should recognise that as the closest match, however, having a 0.0 in every other node implies that the network sees that input as being equally as far from every other number, and I&#39;m not sure that is an ideal result. A human looking at the image clearly sees a 2, but the looping lines also have more of a resemblance to a 3 than for example a 1.<br><br>Should the output of the network ideally take into account these kind of &#39;trace similarities&#39; or is it still preferable for the system to be, as you put it, completely certain in all of its decisions?", "comment_date": "2017-11-03T16:55:55Z", "likes_count": 1}, {"comment_by": "Calvin Smith", "comment_text": "Too verbose for me.  The fancy graphics were overplayed.", "comment_date": "2017-11-03T16:54:25Z", "likes_count": 0}, {"comment_by": "TooLegit ToQuit", "comment_text": "You guys just gotta stop blowing my mind ....\ud83d\udca5", "comment_date": "2017-11-03T16:53:23Z", "likes_count": 84}, {"comment_by": "Sudhendu Pandey", "comment_text": "My weekend will be in black hole. !!!!!!!!!!!!!!", "comment_date": "2017-11-03T16:43:15Z", "likes_count": 0}, {"comment_by": "Kun Shun", "comment_text": "OMG just noticed that Markus Persson (Notch - Creator of Minecraft) sponsored this video.", "comment_date": "2017-11-03T16:35:23Z", "likes_count": 64}, {"comment_by": "See Jian Shin", "comment_text": "Do you do the animations yourself? It looks very very very very good.", "comment_date": "2017-11-03T16:33:23Z", "likes_count": 1}, {"comment_by": "Conguy Monty", "comment_text": "Cool Im here right when it comes out", "comment_date": "2017-11-03T16:13:01Z", "likes_count": 0}, {"comment_by": "Just Revel", "comment_text": "I just want to say that I love that you&#39;re moving into the mathematics of ML. The visualizations convey the concepts so well!", "comment_date": "2017-11-03T16:12:47Z", "likes_count": 12}, {"comment_by": "alasanof", "comment_text": "How do we prevent people from abusing these systems?<br>I read an article that showed how a turtle looked like a rifle to an AI. Is it possible to stop people from applying dangerous labels to mundane things?", "comment_date": "2017-11-03T16:10:27Z", "likes_count": 0}, {"comment_by": "Sachin Shukla", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=1m06s\">1:06</a><br>utter<br>tuh-RASH", "comment_date": "2017-11-03T16:08:26Z", "likes_count": 0}, {"comment_by": "Casey", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m25s\">12:25</a>, he labels Fermat as a tease \ud83d\ude02", "comment_date": "2017-11-03T16:07:19Z", "likes_count": 34}, {"comment_by": "Duke Skookum", "comment_text": "Are you going to talk about momentum and different transfer functions?", "comment_date": "2017-11-03T16:00:39Z", "likes_count": 0}, {"comment_by": "User Interface Masterskill", "comment_text": "Can anybody please explain how can the cost function remain the same even though we are modifying the learning data that it&#39;s being constructed upon (the individual labeled pair or the batches of labeled pairs) ? It seems to me that when the data changes, the function changes altogether. In the 3-d analogy with hills and valleys, since your end goal would be different when your data changed, it would be like trying to get to the bottom of an entire different valley every time.", "comment_date": "2017-11-03T15:52:46Z", "likes_count": 0}, {"comment_by": "Hic-Sunt-Leones", "comment_text": "Excellent video man!", "comment_date": "2017-11-03T15:40:38Z", "likes_count": 0}, {"comment_by": "empCarnivore", "comment_text": "Bruh. Have you been organizing these AI videos using markup language format.", "comment_date": "2017-11-03T15:39:52Z", "likes_count": 0}, {"comment_by": "Marcos Henriques", "comment_text": "I came faster than hunger in communism", "comment_date": "2017-11-03T15:38:29Z", "likes_count": 84}, {"comment_by": "David Edward", "comment_text": "Being familiar with actual neurons myself: no, these &quot;neural networks&quot; are not much like actual neural networks, mainly because the way information is stored in the latter requires that &quot;neurons that fire together wire together&quot;, and you don&#39;t have information in the system without that. Maybe someone can correct me on that, but that&#39;s been my understanding for a while.<br><br>That also brings to mind an additional problem: humans do not assess individual digits in isolation, but in context. If a person has an example of a 4 written by another person and an example of a nine written by the same person, the first person has a massive amount of related information with which to make a judgement. In actual neural networks, there&#39;s no stimulus which cannot be linked to every other possible stimulus in the brain (which can obviously lead to serious problems, like paranoid personalities linking things which have no real connection).", "comment_date": "2017-11-03T15:36:19Z", "likes_count": 0}, {"comment_by": "Powered by Sergey", "comment_text": "Cool", "comment_date": "2017-11-03T15:33:45Z", "likes_count": 0}, {"comment_by": "Apoorva Shastri", "comment_text": "Brilliance in content as always guys, keep it up!", "comment_date": "2017-11-03T15:32:04Z", "likes_count": 0}, {"comment_by": "Cameron Adams", "comment_text": "You are a wizard when it comes to animations and understanding", "comment_date": "2017-11-03T15:23:15Z", "likes_count": 55}, {"comment_by": "Brian Evans", "comment_text": "Your animations are amazing!", "comment_date": "2017-11-03T15:19:19Z", "likes_count": 1468}, {"comment_by": "Paul Gibby", "comment_text": "Oh Fermat, such a tease =p", "comment_date": "2017-11-03T15:17:03Z", "likes_count": 0}, {"comment_by": "ddlow", "comment_text": "You a eerily similar to Dave Ruben.", "comment_date": "2017-11-03T15:12:40Z", "likes_count": 0}, {"comment_by": "Zhuzhefu Zhang", "comment_text": "Have been waiting for this", "comment_date": "2017-11-03T15:12:31Z", "likes_count": 0}, {"comment_by": "Wolf Rage", "comment_text": "Oh boy it is daaaaaaam awesome...<br>Thaaaaaaaaaaaaaaank you sooooooo much....", "comment_date": "2017-11-03T15:09:04Z", "likes_count": 1}, {"comment_by": "Cyan Sea", "comment_text": "Just curious, which programming language do you prefer?", "comment_date": "2017-11-03T15:03:12Z", "likes_count": 0}, {"comment_by": "MasterHigure", "comment_text": "<a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t=12m27s\">12:27</a> The biggest tease in the history of math.", "comment_date": "2017-11-03T14:56:51Z", "likes_count": 0}, {"comment_by": "NoName", "comment_text": "I know that the author won&#39;t answer me, so I have a question for other viewers<br><br>In the essence of calculus, 3blue1brown explained that for implicit defferentiation you derivate all variables (as if all variables are functions)<br>But in my classes we just derived by 1 variable (all but 1 are functions)<br><br>Which way is better?", "comment_date": "2017-11-03T14:47:08Z", "likes_count": 0}, {"comment_by": "caleb cousins", "comment_text": "Thanks so much for the video! For an introduction in learning how to code machine learning systems do you recommend trying to code an example like this yourself or to look at existing examples and playing around and changing them?", "comment_date": "2017-11-03T14:41:58Z", "likes_count": 0}, {"comment_by": "zlac", "comment_text": "Could it be possible to use like a million samples of completely random neutral networks and then statistically extrapolate optimal values by looking at what networks were the closest for what results. Probably use some clever algorithms to merge those randoms into one network that actually works?<br>Maybe then use those values as a new starting point and do another million randomized networks that are only +/- 0.1 off of calculated values.<br>Then compare new optimal to previous optimal to see witch neurons had to move the most and tweak those again, rinse and repeat.", "comment_date": "2017-11-03T14:41:04Z", "likes_count": 0}, {"comment_by": "Sebastiaan Craens", "comment_text": "Thank you SO MUCH", "comment_date": "2017-11-03T14:39:16Z", "likes_count": 0}, {"comment_by": "Michael Tehc", "comment_text": "* eagerly waiting for a video about convolutional neural networks *", "comment_date": "2017-11-03T14:34:34Z", "likes_count": 444}, {"comment_by": "Conor O'Neill", "comment_text": "I feel like I&#39;m missing something with the functionality of backpropogation.<br><br>So, I get that you adjust the weights between the last two rows to adjust how the second last row feeds into the last row in the optimum way, and then you change the second last way, and then you backpropogate the second row of weights away from the end, etc. But, what I don&#39;t understand is that if you change the second last weights, that&#39;s going to change the second last row of inputs, which means that the tweaked weights between the last hidden layer and the output will be wrong again. I&#39;m trying to work out what I&#39;m missing. Is it that all the layers of weights are tweaked simultaneously in order to be accurate? If so, I&#39;m still not sure I understand how that works (I&#39;ll probably try and watch the video again in the morning when my brains properly awake.)", "comment_date": "2017-11-03T14:32:50Z", "likes_count": 12}, {"comment_by": "Shreyas Hervatte", "comment_text": "You are the best teacher ever Mr. Brown!! I love this series .. you explain with so much simplicity that it blows my mind!", "comment_date": "2017-11-03T14:31:54Z", "likes_count": 0}, {"comment_by": "Nick V", "comment_text": "I would expect the training data to also include a bunch of random images that the network could classify as &quot;not a number&quot; (not hotdog \ud83d\ude09) That way, instead of just taking its wildest guess at which number it&#39;s detecting, it could also intelligently say &quot;this doesn&#39;t look like a number to me&quot;", "comment_date": "2017-11-03T14:29:15Z", "likes_count": 173}, {"comment_by": "thevoodooninja", "comment_text": "I&#39;ve never clicked on a video this fast", "comment_date": "2017-11-03T14:24:29Z", "likes_count": 37}, {"comment_by": "13thxenos", "comment_text": "please make more videos!", "comment_date": "2017-11-03T14:24:15Z", "likes_count": 0}, {"comment_by": "Tyler Matthew Harris", "comment_text": "Seriously been waiting for this all week", "comment_date": "2017-11-03T14:20:38Z", "likes_count": 2}, {"comment_by": "Simon van der Poel", "comment_text": "I disable adblock for this", "comment_date": "2017-11-03T14:17:46Z", "likes_count": 2157}, {"comment_by": "3Blue1Brown", "comment_text": "There&#39;s a clarification I want to make on the <a href=\"http://3b1b.co/crowdflower\">http://3b1b.co/crowdflower</a> promotion.  Anyone interested does actually run a job on their system in order to get that free shirt, which means having humans label/generate data for you, and running those jobs requires paying something to those humans (e.g. $10 to have 1,000 pieces of data labeled).  In your initial trial, you&#39;ll have to pay nothing additional to Crowdflower, and the T-shirt is meant just as a fun thing to send to people who have tried their system.<br><br>I actually reached out to Crowdflower for this one, because I thought they might be a really good match with the audience here, given that some subset of you actually work in ML (also they&#39;re just really good people).  That goes for any sponsorships I do, by the way, I genuinely do care about only showing things that I think will be valuable to you and align with the channel, and there&#39;s a lot I say no to.  Apologies to anyone hoping for a free T-shirt without trying the system, I realize the way I said it in the video was unclear.", "comment_date": "2017-11-03T14:16:09Z", "likes_count": 132}, {"comment_by": "Utsav Munendra", "comment_text": "Kurzgesagt and 3Blue1Brown in the single hour! I am in heaven.", "comment_date": "2017-11-03T14:13:57Z", "likes_count": 531}, {"comment_by": "Blrsh Music", "comment_text": "Check3d for this video every day. It&#39;s finally here!", "comment_date": "2017-11-03T14:13:49Z", "likes_count": 1}, {"comment_by": "noneuclidean", "comment_text": "(-e^i\u03c0)th", "comment_date": "2017-11-03T14:12:05Z", "likes_count": 0}, {"comment_by": "Zion J", "comment_text": "Came here from a Vsauce video, left right in the middle of it. Also for some reason it took me about a minute to get the notification after the video was uploaded. :/", "comment_date": "2017-11-03T14:11:00Z", "likes_count": 1}, {"comment_by": "Delta Beta", "comment_text": "<a href=\"https://m.youtube.com/watch?v=lebz80SnL3I\">https://m.youtube.com/watch?v=lebz80SnL3I</a>", "comment_date": "2017-11-03T14:10:40Z", "likes_count": 0}, {"comment_by": "Taha Magdy", "comment_text": "I&#39;ve been waiting &lt;3", "comment_date": "2017-11-03T14:10:11Z", "likes_count": 0}, {"comment_by": "Roberto Buenafe", "comment_text": "I&#39;m 188th \ud83d\ude02", "comment_date": "2017-11-03T14:10:01Z", "likes_count": 0}, {"comment_by": "The Bukkit Area", "comment_text": "AAAAAAAAA YEEEESSSS<br>I&#39;VE BEEN WAITING FOR THIS VID", "comment_date": "2017-11-03T14:09:48Z", "likes_count": 6}, {"comment_by": "Jean", "comment_text": "Finally!!!", "comment_date": "2017-11-03T14:09:45Z", "likes_count": 0}, {"comment_by": "Don Wald", "comment_text": "I was wait for this!", "comment_date": "2017-11-03T14:09:33Z", "likes_count": 0}, {"comment_by": "Eurovision Cyan", "comment_text": "Welp...<br><br><br><br><br>Hi...", "comment_date": "2017-11-03T14:09:32Z", "likes_count": 25}, {"comment_by": "Joey Jacob", "comment_text": "NOTIFICATIONS, YESSSSSSSSS", "comment_date": "2017-11-03T14:09:26Z", "likes_count": 0}, {"comment_by": "Geophph", "comment_text": "I\u2019m having a problem with my iPhone where it won\u2019t play sound unless it has headphones in. Anyone know how to fix this problem?", "comment_date": "2017-11-03T14:07:45Z", "likes_count": 0}, {"comment_by": "plop010", "comment_text": "wow, so early, just by coincidence.", "comment_date": "2017-11-03T14:06:51Z", "likes_count": 0}, {"comment_by": "Obito Sigma", "comment_text": "Now to watch all three parts at once!! Great video 3b1b, keep up the great work man! \u03b4.\u03b4", "comment_date": "2017-11-03T14:06:42Z", "likes_count": 11}]