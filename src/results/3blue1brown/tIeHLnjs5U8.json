[{"comment_by": "3Blue1Brown", "comment_text": "Two things worth adding here:  <br>1) In other resources and in implementations, you&#39;d typically see these formulas in some more compact vectorized form, which carries with it the extra mental burden to parse the Hadamard product and to think through why the transpose of the weight matrix is used, but the underlying substance is all the same.<br><br>2) Backpropagation is really one instance of a more general technique called &quot;reverse mode differentiation&quot; to compute derivatives of functions represented in some kind of directed graph form.", "comment_date": "2017-11-03T14:20:15Z", "likes_count": 1435}, {"comment_by": "Eliel Berra", "comment_text": "You have by far the best videos explaining Deep Learning I&#39;ve ever seen! Thank you for taking the time to make this incredibly high quality material :)", "comment_date": "2022-08-22T01:37:23Z", "likes_count": 0}, {"comment_by": "Miko Santos", "comment_text": "Hindi ko pa talaga magets sa ngayon, i&#39;ll be back at this comment in the near future.", "comment_date": "2022-08-09T13:43:35Z", "likes_count": 0}, {"comment_by": "mac", "comment_text": "Amazingly helpful, thanks.", "comment_date": "2022-08-06T20:41:34Z", "likes_count": 0}, {"comment_by": "A G", "comment_text": "I\u2019d like to make another comment for those discouraged by mathematics:<br><br>Embrace the perplexity of it all. Manipulating the mathematical theory is a difficult skill on its own\u2014and the ability to see how that maps onto the physical world, how you can manipulate the theory to make equations for reality\u2014that\u2019s it\u2019s own unique ability and it can take a decade before the headache of thinking about it all becomes less intense. Mathematicians call it \u201cmathematic maturity.\u201d <br><br>Some concepts are clever, and abstract. The fact that they actually work feels like magic\u2014even to the Mathematicians that developed them. It\u2019s what made it so addicting to them, particularly back when there wasn\u2019t much else to do", "comment_date": "2022-08-03T01:28:23Z", "likes_count": 0}, {"comment_by": "jake is the coolest", "comment_text": "FINALLY!! All of this clicked for me!!!!", "comment_date": "2022-07-28T04:03:32Z", "likes_count": 1}, {"comment_by": "Geetansh Verma", "comment_text": "I must say this is one of the best series I have ever seen on youtube for enthusiasts. Thank you very much, man. Appreciated it a lot.", "comment_date": "2022-07-26T23:46:53Z", "likes_count": 0}, {"comment_by": "Mar Athon", "comment_text": "Hello,<br>I have followed this tutorial and the tutorial of Michael Nielsen. He is able to get 95+% accuracy, while I am only able to get 93+% accuracy. Does anyone know why this is? What can should I do? As far as I can tell, our code functions the same way, despite the fact that I am using java.", "comment_date": "2022-07-24T20:31:01Z", "likes_count": 0}, {"comment_by": "\u015e\u00fckr\u00fc ", "comment_text": "What is the target value (y in your equations) used to calculate the penultimate weights?", "comment_date": "2022-07-14T08:24:40Z", "likes_count": 0}, {"comment_by": "Jakub Kahoun", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m06s\">8:06</a>: I hear scream of new ML coders screaming in the void....&quot;Weee dont know what you meeaaaannn&quot; :D", "comment_date": "2022-07-13T08:45:08Z", "likes_count": 0}, {"comment_by": "K.Karthick Kanagaraj", "comment_text": "excellent video content and explanation, Thank you Sir\ud83d\ude00", "comment_date": "2022-07-11T01:09:48Z", "likes_count": 0}, {"comment_by": "Marcio Reverbel", "comment_text": "Wouldn&#39;t it be correct, therefore, to say that the cost Co is equal to the dot product of [a(L) - y] with itself?", "comment_date": "2022-07-06T10:48:28Z", "likes_count": 0}, {"comment_by": "Carson Lawler", "comment_text": "Am I a freak of nature because this mostly makes sense? It\u2019s multivariable calculus with iterations. I probably couldn\u2019t program a machine reader, but it doesn\u2019t sound like he\u2019s speaking hieroglyphics.", "comment_date": "2022-07-04T04:53:50Z", "likes_count": 0}, {"comment_by": "santiago pati\u00f1o", "comment_text": "I have a question, is it possible to calculate the weights and bias throught some metaheuristc search such as tabu search, genetic algorithms, etc? Is the result of stochastic gradient descent the most optimal result or just a really close one? Thanks, I really like your videos.", "comment_date": "2022-06-26T19:42:04Z", "likes_count": 0}, {"comment_by": "Bro Jaxs", "comment_text": "I\u2019m on my 7th maybe 10th watch through, it\u2019s starting to become a lot more concrete.", "comment_date": "2022-06-25T07:13:02Z", "likes_count": 0}, {"comment_by": "Wiatrak Tymoteusz", "comment_text": "Is the derivative with respect to the activation a^(L-1) necessary to compute if the gradient contains only derivatives with respect to weights and biases?", "comment_date": "2022-06-24T17:38:26Z", "likes_count": 0}, {"comment_by": "Areg Ghazaryan", "comment_text": "i&#39;ve seen a + b = x but holy shit", "comment_date": "2022-06-24T00:04:57Z", "likes_count": 0}, {"comment_by": "Mitchell", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m30s\">9:30</a> , doesn&#39;t one also need the partial derivatives of C w.r.t. previous (k-th) activations a^(l-1)_k and current (j-th) biase b_j to really summarize all components of del C?", "comment_date": "2022-06-23T16:46:51Z", "likes_count": 0}, {"comment_by": "Taneli H\u00e4rk\u00f6nen", "comment_text": "Oh man... Just epic.... Have to take some time to digest all this :O", "comment_date": "2022-06-23T13:03:52Z", "likes_count": 0}, {"comment_by": "Radio FREAK", "comment_text": "Life saving playlist.<br>Thank You Sir !", "comment_date": "2022-06-22T20:42:42Z", "likes_count": 0}, {"comment_by": "Rodrigo da silva", "comment_text": "Thank you!", "comment_date": "2022-06-22T19:55:45Z", "likes_count": 0}, {"comment_by": "Vladimir Fokow", "comment_text": "Thank you a lot for this series! It has really helped me get into this topic, and changed my life. Your intuitions have been immensely helpful in my efforts to understand backpropagation. I just can&#39;t overestimate, how great your channel is!", "comment_date": "2022-06-21T20:21:03Z", "likes_count": 31}, {"comment_by": "Mr. Fahrenheit", "comment_text": "For the the derivative of the cost with respect to layer activation L-3, is that a triple sigma enclosing all the previous chain rule derivatives?", "comment_date": "2022-06-16T04:32:23Z", "likes_count": 0}, {"comment_by": "L L", "comment_text": "Insanely good! Thanks so much!", "comment_date": "2022-06-14T13:45:01Z", "likes_count": 0}, {"comment_by": "fossar _", "comment_text": "Had this recommended to me by YouTube for weeks now and glad I&#39;ve finally have time to sit down and watch the miniseries. Excellent videos.", "comment_date": "2022-06-11T20:31:24Z", "likes_count": 0}, {"comment_by": "Peanut", "comment_text": "So I&#39;m trying to recreate this project by programming it in python myself, and I had a question about using ReLU instead of sigmoid as my activation function. If ReLU doesn&#39;t restrict values between 0 and 1 like sigmoid does, what am I supposed to use as my &quot;expected output.&quot; As you describe in this series when using sigmoid, you would just have the correct number be &quot;1&quot; and the rest &quot;0,&quot; but how does that translate when using ReLU when training?", "comment_date": "2022-06-06T06:01:58Z", "likes_count": 0}, {"comment_by": "jawad nasser", "comment_text": "is the cost function accurate in this video serie? it is my understanding that we do not use mean squared error for neural networks correct me if I&#39;m wrong (we use the cross entropy function)", "comment_date": "2022-06-02T23:18:26Z", "likes_count": 0}, {"comment_by": "Hayden Donnelly", "comment_text": "Is that Notch on the patron list at the end?", "comment_date": "2022-05-31T01:42:35Z", "likes_count": 0}, {"comment_by": "Igel Kissen", "comment_text": "You are a hero, thanks for explaining those concepts so well", "comment_date": "2022-05-29T14:28:19Z", "likes_count": 0}, {"comment_by": "Beraul GD", "comment_text": "I\u2019ve watched so many (parts of) videos of people claiming to teach \u201cderivations\u201d of backpropagation, but this right here is a real derivation! Thanks, I genuinely understand this.<br><br>Edit: Oh yeah, I did also do a lot of this on paper for some hours, so that helps too, lol", "comment_date": "2022-05-28T03:48:32Z", "likes_count": 0}, {"comment_by": "Christakxst", "comment_text": "So far the best video I&#39;ve seen on backpropagation", "comment_date": "2022-05-16T21:56:59Z", "likes_count": 0}, {"comment_by": "\u8b1d\u5fd7\u57ce", "comment_text": "ur explanation is really great! just took me about 1-2 hours to understand what the neural network is", "comment_date": "2022-05-15T16:47:52Z", "likes_count": 0}, {"comment_by": "\u0412\u043e\u043b\u043e\u0434\u044f \u0420\u0443\u0449\u0430\u043a", "comment_text": "Oh man, thank you for your work. It really helps a lot", "comment_date": "2022-05-08T18:46:17Z", "likes_count": 0}, {"comment_by": "Yogeshwar Shendye", "comment_text": "Awesome stuff bro!<br>I came here after struggling a lot with understanding the derivation of backprop algorithm and it made clear!", "comment_date": "2022-05-08T04:49:38Z", "likes_count": 0}, {"comment_by": "Lucas Souza", "comment_text": "Thanks!", "comment_date": "2022-05-05T01:22:37Z", "likes_count": 0}, {"comment_by": "Kal", "comment_text": "genius!", "comment_date": "2022-05-03T21:01:26Z", "likes_count": 0}, {"comment_by": "xMarious98", "comment_text": "Incredible.", "comment_date": "2022-05-03T18:06:02Z", "likes_count": 0}, {"comment_by": "Saurabh Sachdev", "comment_text": "&quot;So pat yourself on the back! If all of this makes sense, you have now looked deep into the heart of backpropagation, the work horse behind how neural networks learn.&quot; felt soooo goooooood &lt;3", "comment_date": "2022-05-03T12:35:52Z", "likes_count": 0}, {"comment_by": "Ahmed Azad", "comment_text": "How much brain power do you have? Over 9000?. I only have -1 :P. Genius", "comment_date": "2022-05-02T02:47:25Z", "likes_count": 0}, {"comment_by": "SaySaeqo", "comment_text": "Thanks!", "comment_date": "2022-05-01T00:57:33Z", "likes_count": 0}, {"comment_by": "Mr Five Thumbs", "comment_text": "What a brilliant playlist. Thank you.", "comment_date": "2022-04-30T21:43:33Z", "likes_count": 0}, {"comment_by": "Beck", "comment_text": "I have lost the trace after 404 :D , but the overall conception is understandable. Just the math is frightening :)", "comment_date": "2022-04-22T16:42:41Z", "likes_count": 0}, {"comment_by": "younes shaimi", "comment_text": "This goes far beyond this f*ing 5 minutes videos with ukulele music in background that pretend to explain you stuff .. this is legit. Thanks !!", "comment_date": "2022-04-17T19:33:50Z", "likes_count": 0}, {"comment_by": "\u00d6mer Faruk \u00d6zt\u00fcrk", "comment_text": "literally thank you. I learned the information that I could not learn at school for 5 weeks in a 10-minute video. The animations of the video are absolutely magnificent. Thank you thank you thank you", "comment_date": "2022-04-15T11:08:44Z", "likes_count": 0}, {"comment_by": "Dan Hillman", "comment_text": "If you use the step function instead of the sigmoid function, how do you handle the impulses that come out of the partial derivatives.", "comment_date": "2022-04-08T18:07:39Z", "likes_count": 0}, {"comment_by": "giannisniper96", "comment_text": "Isn&#39;t this the same as the gradient method from the second video? It seems like a fancy way of computing the gradient analytically instead of doing it numerically", "comment_date": "2022-04-05T07:39:52Z", "likes_count": 0}, {"comment_by": "BigBoiBlue", "comment_text": "This is awesome", "comment_date": "2022-04-03T10:38:21Z", "likes_count": 0}, {"comment_by": "easyBob100", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m00s\">4:00</a> Wish you didn&#39;t skip over the actual math there.  It also would have been nice to see how all this actually changes the weights/biases of the network.  I&#39;ve watched/read so many f&#39;n things about backprop &amp; the chain rule, and the only thing I&#39;ve picked up on is how many ways there is to do this, without actually  figuring out how to adjust the weights/biases.  <br><br>My main source is a book called &quot;Fundamentals of Neural Networks&quot;.  It&#39;s an older book, but also goes over the chain rule to show something different from everyone else.  It uses the chain rule to calculate an actual error function to use on the output layer, and then hidden layers.  It then uses that in a different equation to show how to update the weights of the output layer/hidden layers.<br><br>Even the Cost function is different than yours:  E = .5 * sum(target - predicted)**2", "comment_date": "2022-04-01T12:34:32Z", "likes_count": 0}, {"comment_by": "Drygord Spellweaver", "comment_text": "Plot twist: 3blue1brown is an AI trying to train humans to self learn", "comment_date": "2022-03-30T18:24:59Z", "likes_count": 0}, {"comment_by": "selforganisation", "comment_text": "This looks daunting, can&#39;t you just approximate the derivative numerically?", "comment_date": "2022-03-30T17:54:57Z", "likes_count": 0}, {"comment_by": "Syphax", "comment_text": "do you have a Python code source ?", "comment_date": "2022-03-24T19:13:55Z", "likes_count": 0}, {"comment_by": "Roach Dogg JR", "comment_text": "OK, I just remembered I&#39;m way out of my depth with this one", "comment_date": "2022-03-23T20:53:35Z", "likes_count": 0}, {"comment_by": "Susmit Vengurlekar", "comment_text": ".     <i>_n_</i>                <i>_o_</i><br>|                             |<br>m                          n<br>|                             |<br><br>n is the no. of neurons in the first layer and m is the no. Of neurons in the second layer. the n x 1/o matrix contains the results of the activation functions of previous layer.<br>The way I understood is by making the above matrix shape with my arms and explaining to myself", "comment_date": "2022-03-23T18:18:29Z", "likes_count": 0}, {"comment_by": "Sorravit Bunjongpean", "comment_text": "Wow, when you draw it out like this the complex notation is very easy to understand<br>Thank you", "comment_date": "2022-03-21T18:51:40Z", "likes_count": 0}, {"comment_by": "kharnak crux", "comment_text": "recently... i&#39;ve been thinking about &quot;Dimension space&quot;...     and i realized...   that&#39;s the key...  the hard problem of consciousness.. Qualia.... that&#39;s how it can be defined", "comment_date": "2022-03-16T15:08:38Z", "likes_count": 0}, {"comment_by": "kharnak crux", "comment_text": "So... let&#39;s then say,   that accurate firings with shorter work function cost, is to be rewarded by Dopamine.<br>   my goodness..   one could corner a market with this.", "comment_date": "2022-03-16T14:07:48Z", "likes_count": 0}, {"comment_by": "kharnak crux", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=7m57s\">7:57</a>   this... applied to competitive lotka-volterra systems..    it forms a HUGE interaction matrix...  i wonder....<br>what results might an AI arrive at?", "comment_date": "2022-03-16T01:43:59Z", "likes_count": 0}, {"comment_by": "Process", "comment_text": "Linear regressions stacked on top of each other. Very cool!", "comment_date": "2022-03-13T13:03:33Z", "likes_count": 0}, {"comment_by": "Mikolaj P", "comment_text": "Your explanations are truly amazing and easy to follow. Thank you so much!", "comment_date": "2022-03-12T19:13:58Z", "likes_count": 0}, {"comment_by": "Bilal Sedef", "comment_text": "This is a great and very educational video. But I think it needs one more part to show how the weights are updated.", "comment_date": "2022-03-11T19:24:20Z", "likes_count": 2}, {"comment_by": "Cauchy Schwarz", "comment_text": "What confuses me about this substitution strategy is that a derivative is defined via this limit of the tiny nudge you described going to 0. When I have a notation like<br> dell C / dell z where z is itself a <b>function</b> is that I don&#39;t see how you can assure that that function value goes to 0. It makes sense when you are talking about concrete nudges. But it feels like it breaks the formal definition. Especially since dell z appears again in the same formula where it is itself nudged by something else. So if these two dell z are the same, how can you make sure that dell z goes to 0.", "comment_date": "2022-03-09T07:45:54Z", "likes_count": 0}, {"comment_by": "colton murray", "comment_text": "Had one of those OOOOOOOOOh moments, this should be played in every classroom", "comment_date": "2022-03-08T21:34:17Z", "likes_count": 0}, {"comment_by": "Novus", "comment_text": "After much headache I figured out this works and with learning rate. However, what I am still confused is how would you determine y in multi layers. I get the output-&gt;hidden-layer2 would just be 0 for the unwanted outputs, and 1 for the single wanted output. However, after that going from hidden-layer2-&gt;hidden-layer1 how would you determine the y/desired output to calculate loss.<br><br>Edit. Could you set y as 1 for any neuron that are connected with positive weight to the desired output?", "comment_date": "2022-03-08T19:04:52Z", "likes_count": 0}, {"comment_by": "Wiktor_vn_Waffel", "comment_text": "\u0414\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u044e \u0442\u0435\u043c\u0443, \u0441\u043c\u043e\u0442\u0440\u044e \u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0434\u0438 \u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u043e\u0439 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 =D", "comment_date": "2022-03-08T14:10:05Z", "likes_count": 0}, {"comment_by": "Kamil Ch\u0142osta", "comment_text": "You bring clarity on such potentially complex ideas as Prometheus brought all the light to everyone in the mythology. We are forever grateful for your efforts! You are a math artist and this videos are a masterpieces conveying intuitive understanding of such complex ideas to many people.", "comment_date": "2022-03-02T12:59:43Z", "likes_count": 0}, {"comment_by": "A A", "comment_text": "visually branching out formulas should be an academic paper standard \ud83d\udc99", "comment_date": "2022-03-01T16:39:00Z", "likes_count": 0}, {"comment_by": "Doctor Core", "comment_text": "Thanks, I&#39;ve been learning about NN since the 90s and this is the best explanation of the deep math.", "comment_date": "2022-02-23T17:11:27Z", "likes_count": 0}, {"comment_by": "s303", "comment_text": "At time when I just finished my university \u2014 I could not imagine that at one chilly Sunday evening, in almost 15 years after the graduation, I will sit with a bottle of beer, watch math videos, and have so much fun! Thank you!", "comment_date": "2022-02-20T18:58:06Z", "likes_count": 2}, {"comment_by": "Junhan Chen", "comment_text": "Thanks!", "comment_date": "2022-02-19T18:31:16Z", "likes_count": 0}, {"comment_by": "Freaklyfreakboy", "comment_text": "Months of working with deep neural networks and i still have to come back to this video to re-digest the math. Don&#39;t get discouraged if you don&#39;t understand it! As 3blue1brown said it, it is very complex and takes time to digest.", "comment_date": "2022-02-17T17:19:40Z", "likes_count": 0}, {"comment_by": "Cameron Davis", "comment_text": "Hey, awesome video. I\u2019m still confused on one thing. Are we changing the weights of just the first layer, or all the layers? You said that we would be changing the activations of each neuron in each layer by changing their weights and biases, but you would also have to do the same to every layer, and it would seem like you only really need the first layer of weights. Thanks to anyone who can help me clarify this!", "comment_date": "2022-02-16T14:39:43Z", "likes_count": 0}, {"comment_by": "Shakti Singh", "comment_text": "This is beautiful! it&#39;s a poem written in maths. :)", "comment_date": "2022-02-15T07:12:05Z", "likes_count": 0}, {"comment_by": "MAKWELE WISHBERT", "comment_text": "this is good, but I am still wondering how are the weights updated", "comment_date": "2022-02-13T22:16:28Z", "likes_count": 1}, {"comment_by": "Luke Frymire", "comment_text": "Thanks!", "comment_date": "2022-02-11T18:57:43Z", "likes_count": 0}, {"comment_by": "Pedro Arthur Lima", "comment_text": "You guys are the best! awesome videos", "comment_date": "2022-02-08T18:37:05Z", "likes_count": 0}, {"comment_by": "SOUHARDYA SARKAR", "comment_text": "khan academy and 3 blue one brown has made me love mathematics . And i can see that india is far behind in this type of delivery.", "comment_date": "2022-02-02T07:08:37Z", "likes_count": 0}, {"comment_by": "You caN Shine *", "comment_text": "very useful video series...thankyou team", "comment_date": "2022-02-02T03:25:05Z", "likes_count": 0}, {"comment_by": "zafar0132", "comment_text": "I don&#39;t know how you do it 3Blue1Brown....  but give YOUR self a pat on the back... with only a single view of the 1st 3 and two views of the 4th.  I actually understand how neural nets work. congratulations to you for making a complex subject seem so easy!", "comment_date": "2022-01-30T00:01:55Z", "likes_count": 0}, {"comment_by": "Hamid Bluri", "comment_text": "yaaay I reached to the end", "comment_date": "2022-01-29T10:52:15Z", "likes_count": 0}, {"comment_by": "Paris, Mars", "comment_text": "Holy flaming unicorn balls, this is amazing! Conceptually, this worked out to be much simpler than I thought it would be. This motivates me to learn more about deep learning and the math behind it. Thanks for all your work!", "comment_date": "2022-01-26T04:14:19Z", "likes_count": 0}, {"comment_by": "louis boi", "comment_text": "&quot;dont worry if it takes time for ur mind to digest&quot;, me starting to study my ML exam the night before : &quot;oof&quot;", "comment_date": "2022-01-23T20:05:28Z", "likes_count": 0}, {"comment_by": "Ian Liggett", "comment_text": "I decided I wanted to learn more about neural networks a few days ago, and now I almost have a working one! (A very simple one obviously) It would not have been possible without these videos, and even though I don&#39;t understand a lot of the math yet, I have learned more from this than from any other source, and the math is making more sense to me the more times I watch this. (This is the first youtube video I have liked, ever)", "comment_date": "2022-01-23T05:06:14Z", "likes_count": 0}, {"comment_by": "Ioannis-John Mizithras", "comment_text": "aaah yes. Talk dirty to me :3", "comment_date": "2022-01-22T04:59:51Z", "likes_count": 0}, {"comment_by": "Ansam Zedan", "comment_text": "Thank you so much for this series and explanation.", "comment_date": "2022-01-21T13:34:49Z", "likes_count": 0}, {"comment_by": "Sebastian", "comment_text": "Why is the derivative of a(L) wrt z(L) as shown in the video and not just the Sigmoid function?", "comment_date": "2022-01-15T11:28:24Z", "likes_count": 0}, {"comment_by": "Jacob Nielsen", "comment_text": "Thanks! Very intuitive explanation. Be aware that backpropagation is not the learning-mechanism itself. It is a method for computing gradients, and needs to be coupled with an actual learning-algorithm like stochastic gradient descent.", "comment_date": "2022-01-12T12:13:59Z", "likes_count": 0}, {"comment_by": "Ricardo Diaz Rincon", "comment_text": "This is hands down the best explanation I have seen on backprop. Thank you!", "comment_date": "2022-01-11T03:33:41Z", "likes_count": 0}, {"comment_by": "Rob Peters", "comment_text": "If you break this down into quantum mechanics is it simply what electrons have least resistance in firing on the micro processor?", "comment_date": "2022-01-07T07:22:52Z", "likes_count": 0}, {"comment_by": "digitaldog", "comment_text": "Grant,  your video is incredibly helpful but I seem to misunderstand something so could you tell me where I go wrong.\r<br>Before stating what I did for backpropagation I used RELU and softmax as my activation functions.\r<br>\r<br>To find the derivatives of the weights on the last layer you times:\r<br>2(ActivationOutput[j] - OneHot[j]) * derivative of SoftMax * ActivationOutputOfTheLayerBefore[k]\r<br>\r<br>To find the derivatives of ActivationOutput in layer L-1\r<br>The SumOf (2(ActivationOutput[j] - OneHot[j]) * derivative of SoftMax * WeightsBetweenThisActivationOutputAndAllActivations in LayerL )\r<br>\r<br>then repeat this but using derivatives of ActivationOutput in layer L-1 instead of 2(ActivationOutput[j] - OneHot[j]) and RELU derivative instead of SoftMax derivative.\r<br>\r<br>Sorry if I explained this poorly. I can send my code but I don&#39;t think it will help my explanation.\r<br>Please help. I am increadibly confussed.", "comment_date": "2022-01-05T16:12:28Z", "likes_count": 0}, {"comment_by": "Vimal V", "comment_text": "So much of math covered in 10 mins and that too for dumb ppl like me to follow, that is an incredible feat!", "comment_date": "2022-01-05T03:36:25Z", "likes_count": 0}, {"comment_by": "world war_two", "comment_text": "Surprisingly I find this video simpler than the previous one. Thanks a lot 3blue1brown. I&#39;ll consider donating once I get paid. \ud83d\ude0a", "comment_date": "2022-01-04T13:20:00Z", "likes_count": 1}, {"comment_by": "Dhyanesh N panchal", "comment_text": "Can anyone tell me about what is the level of this topic....means at what educational period does we get this....because i am 12th pass out and i can understand it almost 80% only (not with mathematics but with logic)......", "comment_date": "2022-01-04T12:37:33Z", "likes_count": 0}, {"comment_by": "J\u00e4tski.fi", "comment_text": "construct the network, dont try to fiddle with the weights, waste of time", "comment_date": "2021-12-30T11:35:06Z", "likes_count": 0}, {"comment_by": "Santiago Pires", "comment_text": "Please @3Blue1Brown, we need one of backprop in Convolutional Neural Networks!", "comment_date": "2021-12-28T09:57:14Z", "likes_count": 0}, {"comment_by": "PixL", "comment_text": "(French guy trying is best to speak complex math in another language ^^&#39;)<br><br>I LOVE your videos and I&#39;m trying to make my own neural network in C++ (I&#39;m bleeding ^^&#39;), but I still have a question because I&#39;m not sure I have understanded the whole thing and I don&#39;t do a lot of maths: <br>       -So if I have a 2 logic-layers network, the derivative I will use for the layer 1 will be different for the layer 2 since I have a different function ?<br>Because I thought ,when I first saw the video, that each neuron will change his weights just by seeing his &quot;back-neighboors&quot;, like &quot;I need to be at 1, so I need to move up all my low-weight&quot;, and vice-versa. It was less scary cause In my head to improve each neuron we just needed to know the back-neighboors : not the WHOLE CHAIN ! And i loved it because it was automatic, I could add thousands of layers without having to calculate the derivative myself.<br><br>Thanks to anyone who got the courage to read it all and even more to respond ! Sorry if I have done some english errors ^^&#39; &lt;3", "comment_date": "2021-12-25T23:51:44Z", "likes_count": 0}, {"comment_by": "Mythically Moist", "comment_text": "Thank you so much for making video series on all related topics, I&#39;m trying to go from not having done algebra in two years to being able to make these sons of bitches and all that&#39;s saving me is your videos and a can-do attitude", "comment_date": "2021-12-25T20:49:57Z", "likes_count": 0}, {"comment_by": "OfficialSensotix", "comment_text": "Maybe I am missing something but why is it ( a(L) -Y) squared<br><br>Why is it squared?", "comment_date": "2021-12-24T14:20:24Z", "likes_count": 1}, {"comment_by": "Hetty", "comment_text": "i was following and following and then i wasn&#39;t LMAO", "comment_date": "2021-12-21T22:17:50Z", "likes_count": 0}, {"comment_by": "Kateryna", "comment_text": "Thank you so much for this series. Such an interesting topic.", "comment_date": "2021-12-19T07:47:46Z", "likes_count": 0}, {"comment_by": "Chxnge", "comment_text": "Hey guys, I&#39;m quite confused as to why the derivative dz^(L)/dw^(L) worked out to a^(L-1). When I attempted to do the math, I seem to get the derivative as being a^(L-1) + w^(L). I got this by following the product rule for derivatives.<br><br>Any help with this would be greatly appreciated.", "comment_date": "2021-12-18T23:22:59Z", "likes_count": 0}, {"comment_by": "Justin DeWitt", "comment_text": "Something thats been stumping me in my own implementation:<br><br>If I use the ReLU function instead of sigmoid, I where you compute the derivative of sigmoid, I would compute the derivative of ReLU which is defined to be 1, when z_j is positive, and 0 when its negative, how is it possible to get a negative value in my del vector.<br><br>Basically wheneverr z_j is negative, I have 0 as one of my terms in the product given at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m28s\">9:28</a> which makes the whole term zero. As a result, many of my values in the del vector are 0.", "comment_date": "2021-12-14T11:40:20Z", "likes_count": 0}, {"comment_by": "tehpson", "comment_text": "Thnaks you for this serie :)", "comment_date": "2021-12-13T15:27:27Z", "likes_count": 0}, {"comment_by": "Virkutisss", "comment_text": "Why do we need to minimize cost function in machine learning, what&#39;s the purpose of this? Yeah, I understand that there will be less erorrs etc., but I need to understand it from fundamental perspective. Why don&#39;t we use global maximum for example?", "comment_date": "2021-12-12T20:48:15Z", "likes_count": 0}, {"comment_by": "Utkarsh Agiwal", "comment_text": "beautiful!", "comment_date": "2021-12-12T05:47:04Z", "likes_count": 0}, {"comment_by": "Ashkan Kiafard", "comment_text": "The fact that I can understand what you&#39;re talking about shows that your teaching is flawless!", "comment_date": "2021-12-10T10:06:49Z", "likes_count": 0}, {"comment_by": "\u5e74\u7cd5", "comment_text": "It&#39;s beautiful", "comment_date": "2021-12-06T21:10:07Z", "likes_count": 0}, {"comment_by": "Fandango // JepZ", "comment_text": "That was way easier than I feared", "comment_date": "2021-12-02T23:37:25Z", "likes_count": 0}, {"comment_by": "Manuel Bravo", "comment_text": "Excelente explicaci\u00f3n!!!!!!", "comment_date": "2021-11-21T13:23:22Z", "likes_count": 0}, {"comment_by": "Potato Furyy", "comment_text": "Why am I here", "comment_date": "2021-11-19T14:44:57Z", "likes_count": 0}, {"comment_by": "xiaohan zhang", "comment_text": "suddenly I know how this &#39;black magic&#39; works, I&#39;m having trouble understanding this topic, you have no idea how helpful this video is, thanks a lot!!!", "comment_date": "2021-11-19T10:08:20Z", "likes_count": 0}, {"comment_by": "Ben Piro", "comment_text": "I am currently going through Michael Nielson&#39;s &quot;Neural Networks and Deep Learning&quot; book. This video helps to clear up and visualize the chapter on back propagation a lot. Thank you for making this video series.", "comment_date": "2021-11-14T22:03:34Z", "likes_count": 0}, {"comment_by": "Matthew Hutchinson", "comment_text": "does every connection have it&#39;s own weight, or does every neuron have it&#39;s own weight? If we use the numbers AI from the last 3 videos, and we take the last 2 layers. Theres 10 neurons that show the output, and theres 16 neurons in the layer behind. Is there 160 weights (1 for each connection), or is there 16 weights (1 for each neuron). <br><br>Based on the matrix from the previous videos, i assume they are 160 weights between these 2 layers. It also seems more logical for there to be a weight per connection based on how backpropagation works.", "comment_date": "2021-11-12T07:48:04Z", "likes_count": 0}, {"comment_by": "Manuel Mapa", "comment_text": "I&#39;m in the process of becoming a data scientist and this video potentially saved my career. Thank you!", "comment_date": "2021-11-11T04:28:38Z", "likes_count": 0}, {"comment_by": "\ube44\uc815\uc0c1\uc218", "comment_text": "The best", "comment_date": "2021-11-10T16:17:02Z", "likes_count": 0}, {"comment_by": "Rohan Srivastava", "comment_text": "Thank you so much for this amazing series!", "comment_date": "2021-11-09T08:56:39Z", "likes_count": 0}, {"comment_by": "AlpinsteF", "comment_text": "Great presentation! Very helpful. Although, could you clarify if the indices at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m30s\">9:30</a> are consistent? if you sum over j in the &#39;next&#39; layer, you would have k in the current layer. Instead of w_jk^(l+1), would it be w_ij^(l+1) and sum over i for this next layer? Thanks!", "comment_date": "2021-11-03T23:27:54Z", "likes_count": 3}, {"comment_by": "Doaa Ahmed", "comment_text": "Best explaintion ever for neural network ... thanks a lot", "comment_date": "2021-11-03T21:48:00Z", "likes_count": 0}, {"comment_by": "kailasa nischal", "comment_text": "how can i go deep into it??<br>resources of courses please!!", "comment_date": "2021-11-03T10:14:08Z", "likes_count": 0}, {"comment_by": "226 - Farhan Nafis R - STEI-K", "comment_text": "It took me 2 whole years to finally fully understand this but its worth all the wait", "comment_date": "2021-11-02T03:42:41Z", "likes_count": 0}, {"comment_by": "Emil Kuban", "comment_text": "that&#39;s great.", "comment_date": "2021-10-30T03:12:38Z", "likes_count": 0}, {"comment_by": "Kamran Kerim", "comment_text": "I can&#39;t thank you enough Sir!", "comment_date": "2021-10-22T14:25:01Z", "likes_count": 0}, {"comment_by": "arun gandhi", "comment_text": "Making complex things simple, that&#39;s what a genius does", "comment_date": "2021-10-20T22:34:36Z", "likes_count": 0}, {"comment_by": "Piotr Rywczak", "comment_text": "\ud83d\ude2d", "comment_date": "2021-10-19T14:24:00Z", "likes_count": 0}, {"comment_by": "Shibasis Patnaik", "comment_text": "Thanks!", "comment_date": "2021-10-16T10:47:02Z", "likes_count": 0}, {"comment_by": "Nima Jafari", "comment_text": "YOU ARE THE BEST GRANT", "comment_date": "2021-10-16T08:31:38Z", "likes_count": 0}, {"comment_by": "Rajiv Kumar", "comment_text": "Thank you very much", "comment_date": "2021-10-15T11:40:37Z", "likes_count": 0}, {"comment_by": "Atinesh", "comment_text": "Those animations \ud83d\udc4c", "comment_date": "2021-10-15T09:43:25Z", "likes_count": 0}, {"comment_by": "jaydays", "comment_text": "Literally bless your soul", "comment_date": "2021-10-14T21:27:44Z", "likes_count": 0}, {"comment_by": "Ahmed Mohamed", "comment_text": "I&#39;m in love with your content.", "comment_date": "2021-10-13T17:28:06Z", "likes_count": 0}, {"comment_by": "Tobias Johannesson", "comment_text": "I will be back, give me a few weeks and I will not only follow, but I will also (hopefully) understand that time around!", "comment_date": "2021-10-08T10:07:35Z", "likes_count": 0}, {"comment_by": "libertyintegritytruth", "comment_text": "this explanation is just genius. wow.. thank you", "comment_date": "2021-10-07T18:37:00Z", "likes_count": 0}, {"comment_by": "Kai Christensen", "comment_text": "Dear Grant,<br><br>A year ago, I decided I wanted to learn Machine Learning and how to use it to make cool stuff. I was struggling with some of the concepts, so I went to YouTube and re-discovered this series on your channel.<br><br>Out of all the courses I&#39;ve tried and all the hours of other content I&#39;ve sat through, your videos stand out like a ray of sunshine. I just got my first full-time job as a Machine Learning Engineer, and I can confidently say it would never have happened without this series.<br><br>Your channel may have affected the course of my life more than almost any other. Thanks for all your hard work!", "comment_date": "2021-10-03T03:39:17Z", "likes_count": 498}, {"comment_by": "\u00d3\u00fbth M\u00e2n\u0113", "comment_text": "My brain is fried.", "comment_date": "2021-10-01T16:20:09Z", "likes_count": 1}, {"comment_by": "Rembau Times", "comment_text": "Thanks for the video. It was very well crafted and I like the &#39;Pi&#39; teacher and students. (Kind of reminds me of the Office Assistant in Windows 97, but done right in the video.", "comment_date": "2021-09-30T07:53:59Z", "likes_count": 0}, {"comment_by": "Hello_WorlD!", "comment_text": "\uac10\uc0ac\ud569\ub2c8\ub2e4.", "comment_date": "2021-09-19T09:49:26Z", "likes_count": 0}, {"comment_by": "Ben Zuckier", "comment_text": "I know it&#39;s nitpicking, but you refer to the partial symbol a bunch of times as &quot;del&quot;. The partial symbol, to my knowledge, has no better name than &quot;partial&quot; while del is the same symbol as nabla.", "comment_date": "2021-09-13T14:50:51Z", "likes_count": 1}, {"comment_by": "Max Bardelang", "comment_text": "this video makes me feels like understanding the chain rules is easier in the context of a neural network than it is from a pure mathematical background!", "comment_date": "2021-09-12T15:24:34Z", "likes_count": 0}, {"comment_by": "Me & The Plant", "comment_text": "Just brilliant. Thanks for this", "comment_date": "2021-09-10T07:15:00Z", "likes_count": 0}, {"comment_by": "Vadrevu Dheeraj", "comment_text": "Very very very very very helpful.", "comment_date": "2021-09-09T09:22:03Z", "likes_count": 0}, {"comment_by": "Karson Sewlochan", "comment_text": "One of the most well done educational series I&#39;ve ever seen, thank you.", "comment_date": "2021-09-05T22:27:14Z", "likes_count": 0}, {"comment_by": "WeinSim", "comment_text": "After calculating the derivative, by how much should the weights / biases be changed? Say a the derivative of a certain weight is -0.5. how much should I subtract from that weight? like by what factor should I multiply the derivative to get an optimal result? (I&#39;m stuck at an 82% accuracy rn with the exact model as in the video)", "comment_date": "2021-09-02T19:13:06Z", "likes_count": 1}, {"comment_by": "Nobre Engenheiro", "comment_text": "That&#39;s the end? WTF", "comment_date": "2021-09-01T11:14:37Z", "likes_count": 2}, {"comment_by": "Pavel Tverdunov", "comment_text": "almost 300 guys missed thumbs up button!", "comment_date": "2021-08-28T21:33:51Z", "likes_count": 0}, {"comment_by": "Adrian D.", "comment_text": "Wow you are helping me so much with your Videos for my exams. Thank u so much!!!!", "comment_date": "2021-08-27T15:41:12Z", "likes_count": 0}, {"comment_by": "Macknight Xu", "comment_text": "so this video is the detailed calculation of the previous video?", "comment_date": "2021-08-26T13:23:46Z", "likes_count": 0}, {"comment_by": "Shanmukh Chandra Yama", "comment_text": "Man it&#39;s really whole lot of stuff, I can&#39;t digest the notations and complex expressions in one go. As usual the explanation is phenomenal\ud83d\udc4c", "comment_date": "2021-08-25T11:58:10Z", "likes_count": 0}, {"comment_by": "Johnny Schmegma", "comment_text": "magic, got it", "comment_date": "2021-08-23T20:11:45Z", "likes_count": 0}, {"comment_by": "Mikhail", "comment_text": "Actually, we can even find a derivative of sigmoid function, which is equal to (e^(-x))/(e^(-x)+1)^2, and plug Z(L) instead of x to make it mathmaticaly complete", "comment_date": "2021-08-21T15:08:27Z", "likes_count": 0}, {"comment_by": "Splines", "comment_text": "Thank you so much @3Blue1Brown for all your efforts, this is truly amazing.<br><br>I was quite struggling to grasp the yellow box at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m25s\">9:25</a>. I think the choice of j as running index for the sum is confusing since this j is different from the one above (where we calculate del C / del w_jk^(L)). So, let&#39;s instead choose m as a running index for the sum.<br>Then: Shouldn&#39;t the first factor in the sum read w_mj^(L+1) instead of w_mk^(L+1). Otherwise, it wouldn&#39;t go together with the dimensions. Am I missing something here?", "comment_date": "2021-08-18T09:17:05Z", "likes_count": 0}, {"comment_by": "Bharghava K", "comment_text": "In the future when computing hardware and software get so good that most topics in math can be animated this good and be made as public resources, I bet we can expect more students in Math and it&#39;s related fields.", "comment_date": "2021-08-18T05:46:12Z", "likes_count": 0}, {"comment_by": "Shaun", "comment_text": "@3Blue1Brown you are PICASSO of Math and Data Science. Love and Respect.", "comment_date": "2021-08-16T17:59:49Z", "likes_count": 0}, {"comment_by": "Omar Montalvo Torrico", "comment_text": "More Artificial Intelligence pleace \ud83d\ude4f\ud83d\ude4f", "comment_date": "2021-08-16T12:40:22Z", "likes_count": 0}, {"comment_by": "simay kaz\u0131c\u0131", "comment_text": "Thanks", "comment_date": "2021-08-16T12:19:02Z", "likes_count": 0}, {"comment_by": "Aditi Paretkar", "comment_text": "One of the best series on Deep Learning .. watched all four videos and all were totally worth it. Thankyou for all the time and effort that goes into making these videos", "comment_date": "2021-08-10T11:42:39Z", "likes_count": 0}, {"comment_by": "Daniel Konstantinovsky", "comment_text": "do you include derivatives of C_0 w.r.t. all weights and biases in the network in the gradient or only w.r.t. the weights and biases connecting the final layer to layer L-1?", "comment_date": "2021-08-08T18:25:45Z", "likes_count": 0}, {"comment_by": "Abhijith VK", "comment_text": "This one channel has taught me more on mathematics than everything else I have ever gone through. Thanks a lot for such insightful contents. You make the world a better place &lt;3", "comment_date": "2021-08-07T21:46:35Z", "likes_count": 0}, {"comment_by": "Aryan Parekh", "comment_text": "I&#39;m finally understanding everything perfectly and it just makes so so happy, thank you", "comment_date": "2021-08-06T16:32:03Z", "likes_count": 0}, {"comment_by": "\u0645\u0639\u06cc\u0646 \u06a9\u0633\u0631\u0627\u0626\u06cc", "comment_text": "\u0642\u062f\u0631\u062a \u0631\u0627 \u0628\u0647 \u0638\u0646 \u06a9\u0633\u06cc \u062a\u0642\u062f\u06cc\u0645 \u0646\u06a9\u0631\u062f\u0647 \u0627\u062f\u0645 \u0634\u062f\u0646 \u0631\u0627 \u0686\u0631\u0627", "comment_date": "2021-08-03T07:52:48Z", "likes_count": 0}, {"comment_by": "ThatKyleGuy", "comment_text": "It&#39;s really sad to me that this free video does a WAY better job of teaching this concept than my overpaid grad school professor ever did.", "comment_date": "2021-08-01T01:35:11Z", "likes_count": 0}, {"comment_by": "Abdallah Youssef", "comment_text": "\u0627\u0644\u0631\u0627\u062c\u0644 \u062f\u0647 \u062c\u062f\u0639", "comment_date": "2021-07-29T01:11:05Z", "likes_count": 0}, {"comment_by": "Daniel Mewes", "comment_text": "Thanks!", "comment_date": "2021-07-28T20:00:54Z", "likes_count": 0}, {"comment_by": "JTProductions3", "comment_text": "3 weeks into my deep learning subject at university and I had no idea.  Saw these videos and I thought.  LIFESAVER!  Thanks so much man.  So good!", "comment_date": "2021-07-27T10:12:26Z", "likes_count": 0}, {"comment_by": "Swphsil", "comment_text": "It will be very cool if you could continue this series and add more videos on explaining the working of different optimizers, cost functions etc", "comment_date": "2021-07-26T17:08:19Z", "likes_count": 1}, {"comment_by": "Aderox", "comment_text": "My, not even graduated from high school: interesting. I don&#39;t understand a single word.", "comment_date": "2021-07-21T15:34:27Z", "likes_count": 0}, {"comment_by": "Sameer Yadav", "comment_text": "is it completed?", "comment_date": "2021-07-20T19:26:26Z", "likes_count": 0}, {"comment_by": "Dipesh Patil", "comment_text": "i learnt the chain rule in 2 year of my engineering, but understood today how beautiful it is.", "comment_date": "2021-07-19T06:15:10Z", "likes_count": 0}, {"comment_by": "allyourcode", "comment_text": "If I understand correctly, the point of backpropagation is that it much faster than the naive method of calculating the gradient of the loss/cost. I don&#39;t really see why it should matter whether you go forwards or backwards. Seems to me, as long as you do not recalculate anything, you&#39;ll always do the same amount of work, at least, that is my intuition. Why is this so much faster??", "comment_date": "2021-07-19T02:56:29Z", "likes_count": 0}, {"comment_by": "Kolja M", "comment_text": "I don&#39;t understand why the books cant explain this stuff this easy yet precise, great job!", "comment_date": "2021-07-16T10:53:14Z", "likes_count": 0}, {"comment_by": "asd9433", "comment_text": "I like the way you explain it. Can you please create series on  RL , GNN (other AI)?", "comment_date": "2021-07-16T07:42:33Z", "likes_count": 0}, {"comment_by": "bram", "comment_text": "I had to watch this series again, it is great!", "comment_date": "2021-07-13T08:43:17Z", "likes_count": 0}, {"comment_by": "kilroy1964", "comment_text": "Got it. So now, how much do we change the weights and biases?", "comment_date": "2021-07-10T11:34:13Z", "likes_count": 0}, {"comment_by": "Dawood Abuswai", "comment_text": "You are great man &lt;3", "comment_date": "2021-07-10T01:05:06Z", "likes_count": 0}, {"comment_by": "popi", "comment_text": "The hard part is how connect the dot/neuron and finding neuron itself", "comment_date": "2021-07-09T23:31:46Z", "likes_count": 0}, {"comment_by": "popi", "comment_text": "It was basic how to create neural network engine", "comment_date": "2021-07-09T23:30:17Z", "likes_count": 0}, {"comment_by": "popi", "comment_text": "Tracing backward from front to behind", "comment_date": "2021-07-09T23:28:34Z", "likes_count": 0}, {"comment_by": "jimmyt1988", "comment_text": "Amazing.<br>Questions:<br>1. (a(L) - y)^2. Is the square due to the requirement of measuring the area of this activation to cost difference (ie, it relates to calculus)?<br>2. <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=3m48s\">3:48</a>, you make the deltaC0 a non subset 0. Did you do this to avoid confusion of the question of which layer the cost came from?", "comment_date": "2021-07-06T17:06:59Z", "likes_count": 0}, {"comment_by": "Hydrochloric Acid", "comment_text": "I&#39;ve been trying to pick at this series of videos for years at this point, but the relevant math was just out of my grasp. Just finished year 1 of college and went through a proper calculus course, and the fact that I can now understand this is incredibly satisfying", "comment_date": "2021-07-05T10:53:28Z", "likes_count": 0}, {"comment_by": "\u8c22\u751f", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=2m53s\">2:53</a>,\u201chow sensitive the cost function is to small changes in our weight w^(L), or phrased differently, what\u2019s the derivative of C with respect to w^(L)\u201dThis sentence has revolutionized my understanding of derivatives .  thank you very much indeed. YOU ARE always Awesome!", "comment_date": "2021-06-30T09:14:31Z", "likes_count": 0}, {"comment_by": "Sanjit Rao", "comment_text": "If you can learn the math behind something, you already have a fundamental understanding of that thing.", "comment_date": "2021-06-27T17:07:37Z", "likes_count": 0}, {"comment_by": "Mehedi Hasan", "comment_text": "I&#39;m watching the videos of this channel first and it just blow my mind. You&#39;ve made a complex matter really simple.You&#39;re genius \ud83d\ude0d\ud83d\ude0d", "comment_date": "2021-06-26T19:56:40Z", "likes_count": 0}, {"comment_by": "TheMalitias", "comment_text": "Is there a reason all neurons use the same activation function?", "comment_date": "2021-06-18T03:36:11Z", "likes_count": 0}, {"comment_by": "Alexandra", "comment_text": "With your explanation, all these partial derivatives finally make sense. Thank you!", "comment_date": "2021-06-16T14:38:51Z", "likes_count": 0}, {"comment_by": "Go Better", "comment_text": "It makes so much sense now! Thanks!", "comment_date": "2021-06-13T13:02:31Z", "likes_count": 0}, {"comment_by": "Manjunath Hadimani", "comment_text": "I have confusion with z(L) expression, z(L) = w(L)a(L-1) but as per Andrew NG course it&#39;s z(L) = w(L-1)a(L-1), please clarify this. Andrew NG machine learning course week 4", "comment_date": "2021-06-13T06:20:55Z", "likes_count": 0}, {"comment_by": "\u0422\u0430\u043c\u0435\u0440\u043b\u0430\u043d \u041c\u0443\u0441\u0442\u0430\u0444\u0430\u0435\u0432", "comment_text": "Thanks a LOT!", "comment_date": "2021-06-11T12:09:54Z", "likes_count": 0}, {"comment_by": "sourabh deokar", "comment_text": "why are we calculating the partial derivatives of the cost function wrt to the activations, if the algorithm is updating only the weights and biases?", "comment_date": "2021-06-10T19:23:38Z", "likes_count": 0}, {"comment_by": "J R", "comment_text": "I love this! Grant you are prodigy! Your visualizations and pedagogy is truly unmatched, helping me get through my NN class.", "comment_date": "2021-06-10T17:38:53Z", "likes_count": 0}, {"comment_by": "Noel Gomariz", "comment_text": "what a beautiful series to get started into the matter, thanks &lt;3", "comment_date": "2021-06-07T12:16:21Z", "likes_count": 0}, {"comment_by": "Hariharan Nair", "comment_text": "You are a god", "comment_date": "2021-06-07T05:51:49Z", "likes_count": 0}, {"comment_by": "Anto Vrdoljak", "comment_text": "It really can`t get any better than this.  Awesome! This is truly the peak of learning methodology and didactics!", "comment_date": "2021-05-29T07:50:09Z", "likes_count": 0}, {"comment_by": "Luna Peragine", "comment_text": "You explained this in a way that was unbelievably easy to understand, THANK YOU!!!", "comment_date": "2021-05-27T20:41:05Z", "likes_count": 0}, {"comment_by": "Brauggi the bold", "comment_text": "Ok, so turns out Backpropagation is just a really complicated way of saying &quot;Compute the partial derivative&quot;. Why do you need to say anything more than that?", "comment_date": "2021-05-26T15:45:16Z", "likes_count": 1}, {"comment_by": "Vaddagani Shiva", "comment_text": "Is chapter 5 coming?", "comment_date": "2021-05-13T18:26:52Z", "likes_count": 1}, {"comment_by": "Nicholas Kryger-Nelson", "comment_text": "I would love if anyone could answer my question, does this mean that if you have a net work with many layers, the partial derivative of cost with respect to a parameter in the first layer (like w^1) would be a super long expression that builds on the chain rule.", "comment_date": "2021-05-13T04:30:20Z", "likes_count": 0}, {"comment_by": "Ian Bryant", "comment_text": "Okay but do you take the derivative of the summation of the cost function? Cause before it was just 2(a(L) - y).", "comment_date": "2021-05-08T17:54:05Z", "likes_count": 0}, {"comment_by": "Neil Lyons", "comment_text": "Great video, thank you.", "comment_date": "2021-05-07T11:23:19Z", "likes_count": 0}, {"comment_by": "Ministry of good Ideas", "comment_text": "could someone tell me where the &quot;automatic differentiation&quot; part is in this back propagation?<br>Is it just the dissolving of the gradient layer by layer?", "comment_date": "2021-05-05T22:52:27Z", "likes_count": 0}, {"comment_by": "Linsu Han", "comment_text": "It&#39;s good to visual the vanishing gradient problem here at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m30s\">4:30</a> :<br>- if the Activation is Sigmoid, the derivative is ranges between [0, 0.25]<br>- if the Activation is ReLU, the derivative is either 1, or 0<br>With enough Sigmoid activations stacked on top of each other you can see that the upper limit of gradients approaches (.25)^n -&gt; 0", "comment_date": "2021-05-04T21:58:40Z", "likes_count": 6}, {"comment_by": "Ahmed Syed", "comment_text": "thank you , this was amazing intuition into the algorithm", "comment_date": "2021-04-21T15:25:28Z", "likes_count": 0}, {"comment_by": "Yonit Lev", "comment_text": "thank you SO MUCH for publishing this video! The illustrations, explanation and your calm voice made it so easy to understand! I first hear of backprop yesterday and the professor didn&#39;t go into the math of the cost function derivative one step back from the output layer and your explanation really helped me get a better sense of understaning what&#39;s happening here. I can not thank you enough - bless you!", "comment_date": "2021-04-21T05:19:33Z", "likes_count": 0}, {"comment_by": "\u0415\u0433\u043e\u0440 \u0410\u0431\u0440\u043e\u0441\u0438\u043c\u043e\u0432", "comment_text": "If I ever believe in any god, ideas that you give would be somewhere among the reasons", "comment_date": "2021-04-20T17:25:36Z", "likes_count": 0}, {"comment_by": "Ciro Garc\u00eda", "comment_text": "I didn&#39;t understand a thing, but I&#39;ll come back when I actually have the math knowledge to follow along!", "comment_date": "2021-04-20T15:55:54Z", "likes_count": 1}, {"comment_by": "Ohi Bokth", "comment_text": "Thank you so much for making whats very complicated to something slightly less complicated. Spending time watching this videos is the best way i can waste my time ;)", "comment_date": "2021-04-16T23:02:11Z", "likes_count": 0}, {"comment_by": "boerewors1123", "comment_text": "Would you make a video of how multiple &quot;soft-max&quot; functions combine to represent complex equations?", "comment_date": "2021-04-16T16:01:39Z", "likes_count": 0}, {"comment_by": "Obv Andrew", "comment_text": "Your video made it so easy to understand and visualise. Thank you so much!", "comment_date": "2021-04-14T09:26:24Z", "likes_count": 0}, {"comment_by": "Igor Santarek", "comment_text": "It took me few days to understand it at least a little bit. After few day I understood it in the level that allowed me to create and compute derivatives for specific problem (network that learns to solve AND) with 2 hidden layers (each 1 neuron), 2 inputs and 1 output. I calculated everything on paper and then written a program in python (without using matrices because I wanted to understand it deeply) and it works!<br>RESULTS FOR &quot;AND&quot; TEST:<br>SET: (0.0, 0.0, 0.0) -&gt; 0.011851194413328438<br>ERROR: 0.0001404508090225072<br>SET: (0.0, 1.0, 0.0) -&gt; 0.05425714202833884<br>ERROR: 0.002943837461083333<br>SET: (1.0, 0.0, 0.0) -&gt; 0.055500092001344106<br>ERROR: 0.00308026021215766<br>SET: (1.0, 1.0, 1.0) -&gt; 0.9101530551266752<br>ERROR: 0.008072473503070263", "comment_date": "2021-04-13T10:17:40Z", "likes_count": 0}, {"comment_by": "catchingphotons", "comment_text": "Unarguably one of the best &quot;tutorial&quot; videos of all times! The carefully taken logical steps of understanding, the animations, the visualizations, the tempo, the examples... boggles my mind! This is a masterpiece! <br>Greetings<br>-Chris", "comment_date": "2021-04-12T13:32:56Z", "likes_count": 0}, {"comment_by": "Programming & software design", "comment_text": "&quot;All of these are just numbers right?&quot; -3b1b<br>No, this is madness!", "comment_date": "2021-04-11T09:07:45Z", "likes_count": 2}, {"comment_by": "Shekhar Kumar", "comment_text": "Simply Amazing ! Best of best explanation to convert your imagination in to realtime graphics. Thanks a lot it cleas so many doubts.", "comment_date": "2021-04-10T14:53:57Z", "likes_count": 0}, {"comment_by": "Nguy\u1ec5n Cao Nh\u00e2n", "comment_text": "You really saved my life, the explanations, the animations inside the video, the drawings, ... everything is just perfect! Thank you so much!", "comment_date": "2021-04-08T13:58:58Z", "likes_count": 0}, {"comment_by": "George Timothy Clooney", "comment_text": "should Y always be 1 or zero in theory?", "comment_date": "2021-04-07T21:24:48Z", "likes_count": 0}, {"comment_by": "Eric Wang", "comment_text": "Math did not exist until this guy.", "comment_date": "2021-04-07T03:26:51Z", "likes_count": 0}, {"comment_by": "TheOraware", "comment_text": "when i saw first time , it was not clear to me. Second time i started to write and follow exactly what Grant explained , why writing word to word making more sense to me? it takes lot of time how can i avoid this and understand thoroughly anything without writing?", "comment_date": "2021-04-05T05:18:22Z", "likes_count": 0}, {"comment_by": "Craig Schanz", "comment_text": "With all of these computations, how many floating point operations or FLOPS would computing one whole gradient once be, given any amount of layers, neurons per layer, and weighs connecting each neuron?", "comment_date": "2021-04-04T01:43:46Z", "likes_count": 0}, {"comment_by": "Peter Bliznak", "comment_text": "could.t find any more crystal clear explanation anywhere else ..... thanks", "comment_date": "2021-04-02T20:52:51Z", "likes_count": 0}, {"comment_by": "Quantum Leap", "comment_text": "once you understand it, it seems so trivial but when you haven&#39;t it seems so insurmountable", "comment_date": "2021-04-01T16:39:55Z", "likes_count": 0}, {"comment_by": "Claudio Bustamante", "comment_text": "Thank you so much for this, your work is very valuable!", "comment_date": "2021-03-29T21:48:15Z", "likes_count": 0}, {"comment_by": "\ub300\ud559\uc6d0\uc0dd \uc0b4\uc544\ub0a8\uae30", "comment_text": "\ubaa8\ub4e0 \uc911\ud559\uad50\uc758 \uc218\ud559\uacfc\uc815\uc744 \ud3d0\uae30 \ud558\uace0 \uadf8\uc758 \uac15\uc758\ub97c \ud55c\uad6d\uc758 \ud544\uc218 \uad50\uacfc\ubaa9\uc73c\ub85c \uc9c0\uc815\ud574\uc57c \ud55c\ub2e4.", "comment_date": "2021-03-29T12:46:53Z", "likes_count": 0}, {"comment_by": "Brendan Moran", "comment_text": "Why do you square the difference between the desired output and activation value? Is this just conventionally used to find the absolute value of the difference, or is the square serving some other purpose?", "comment_date": "2021-03-27T06:49:54Z", "likes_count": 0}, {"comment_by": "Daniel Simamora", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=5m27s\">5:27</a> <br>How do you (or those who invented this Neural Network) know that averaging is enough? I mean, why use arithmetic mean? Why not other mean like reciprocal mean, weighted mean, and geometric mean??", "comment_date": "2021-03-24T05:40:34Z", "likes_count": 0}, {"comment_by": "Janekk", "comment_text": "this series became too hard on episode 3", "comment_date": "2021-03-20T11:46:48Z", "likes_count": 0}, {"comment_by": "Zilong Zhao", "comment_text": "your video should be shown in every university&#39;s lesson, the animation makes the calculation just so easy to understand.", "comment_date": "2021-03-19T01:31:49Z", "likes_count": 3}, {"comment_by": "Bozorg", "comment_text": "Thank you, Thank you, Thank you", "comment_date": "2021-03-18T11:56:14Z", "likes_count": 0}, {"comment_by": "Plug OFF", "comment_text": "\u041a\u0440\u0430\u0441\u0430\u0432\u0430! Good job!", "comment_date": "2021-03-14T02:26:07Z", "likes_count": 0}, {"comment_by": "Jordi A.", "comment_text": "Great job man", "comment_date": "2021-03-12T14:57:19Z", "likes_count": 0}, {"comment_by": "no way", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m28s\">9:28</a> I understand the concept entirely - but you&#39;ve used &quot;j&quot; for both an index of w and an index of the sum? I&#39;m struggling to implement this in python.", "comment_date": "2021-03-09T20:40:45Z", "likes_count": 0}, {"comment_by": "\u041d\u0438\u043a\u0438\u0442\u0430 \u0411\u0443\u043b\u0433\u0430\u0440\u0443", "comment_text": "Please video about convolutional neural networks", "comment_date": "2021-03-09T18:14:41Z", "likes_count": 0}, {"comment_by": "Yajnesh Talapady", "comment_text": "Samajh nahi aaya par sun ke achha laga \ud83e\udd14", "comment_date": "2021-03-08T20:58:43Z", "likes_count": 0}, {"comment_by": "Veix Quadron", "comment_text": "this thinker put so many math teachers out of a job", "comment_date": "2021-03-04T11:13:38Z", "likes_count": 0}, {"comment_by": "AguanteBarenboim", "comment_text": "Amazing explanation!", "comment_date": "2021-03-01T01:14:11Z", "likes_count": 0}, {"comment_by": "Gauthier Lechevalier", "comment_text": "You are pretty much giving university level courses to us mere-humans!! Thank you so much!!! you really are a great teacher", "comment_date": "2021-02-26T16:37:41Z", "likes_count": 0}, {"comment_by": "arturr5", "comment_text": "i dont know, and I don&#39;t care to know", "comment_date": "2021-02-25T16:17:30Z", "likes_count": 0}, {"comment_by": "RH S", "comment_text": "Honey, how is lerning algebra?<br>I found a better one, algebra can wait!", "comment_date": "2021-02-25T15:00:42Z", "likes_count": 0}, {"comment_by": "Hannah Do", "comment_text": "great video, thanks! :)", "comment_date": "2021-02-24T11:24:53Z", "likes_count": 0}, {"comment_by": "Victor Esteban Arce Cubillo", "comment_text": "I&#39;m sec 56. I wanted to answer something obvious. Fourth variables is exactly the same of fourth fourth contantas by variables.", "comment_date": "2021-02-21T07:02:41Z", "likes_count": 0}, {"comment_by": "SpicyMelon", "comment_text": "I have spent about 2 and a half years trying to implement this into code from the video. It never works and nobody in the comments seems to have ever tried to implement it either. I am coding on unity with C# . The backpropagation step always fails. <br><br>Here are my main questions:<br> - when finding the error of each node value and the desired nodes value in the last layer, why do we square the results. When I do this the derivative calculations now cant determine if they need to decrease or increase cause the result is always positive.... what am I doing wrong here?<br> - When finding the cost, why do we sum up the errors but then never use it.<br> - for the chain rule, do we multiply the individual &quot;chains&quot; together?<br><br><br>Basically evetime I attempt this project from scratch (about 6 times at this point) I always fail at the backpropagation step, specifically in the derivative section. The main thing I need to know is how do I find what values to change each weight and bias such that the cost goes down.... duh. All this video makes soooooo much sense but in practice it never ever works....D:", "comment_date": "2021-02-19T00:54:13Z", "likes_count": 0}, {"comment_by": "IMateme", "comment_text": "what is the application /program used for creating this wonderful lesson?? I mean , adobe premier? or davinci resolve ... or any other ?", "comment_date": "2021-02-16T15:20:26Z", "likes_count": 0}, {"comment_by": "Francesco La Porta", "comment_text": "Amazing video", "comment_date": "2021-02-13T20:22:54Z", "likes_count": 0}, {"comment_by": "Igor Szemela", "comment_text": "that helps af, really appreciate your  work", "comment_date": "2021-02-13T16:16:51Z", "likes_count": 0}, {"comment_by": "Stefan Baumann", "comment_text": "Extremely well explained Grant - thank you!", "comment_date": "2021-02-10T18:26:01Z", "likes_count": 0}, {"comment_by": "Jappa", "comment_text": "Awesome intrduction to neural networks. Thanks a lot!", "comment_date": "2021-02-08T16:35:49Z", "likes_count": 0}, {"comment_by": "\u041d\u0438\u043a\u0438\u0442\u0430 \u0411\u0443\u043b\u0433\u0430\u0440\u0443", "comment_text": "I want more", "comment_date": "2021-02-06T07:02:23Z", "likes_count": 0}, {"comment_by": "DruidPath", "comment_text": "Thanks I got a bit confused with the notation on other explanations but because of all your graphics it was very easy to understand:)", "comment_date": "2021-02-05T01:07:23Z", "likes_count": 0}, {"comment_by": "Paola Estrada", "comment_text": "So I&#39;ve watched all of your videos in this neural network series and they&#39;re very clear. But how would you make a neural network that recognises the partial interaction of molecules? How would you make a neural network that &quot;knows&quot; the difference between an acid and an aldehyde?", "comment_date": "2021-02-04T05:31:17Z", "likes_count": 0}, {"comment_by": "Kai Christensen", "comment_text": "I&#39;m taking Machine Learning by Andrew Ng on Coursera right now, and just got stuck on backpropagation. Thank you thank you thank you thank you Grant, you have no idea how incredibly helpful your videos are and how much your channel has inspired me through the years.", "comment_date": "2021-02-03T05:33:22Z", "likes_count": 114}, {"comment_by": "HARRISON FORD", "comment_text": "Nice video, learned a lot, thank you soooooo muuuuch!", "comment_date": "2021-02-01T02:06:18Z", "likes_count": 0}, {"comment_by": "Vedant", "comment_text": "Man u deserve a Nobel Prize for teaching Machine Learning with this simplicity.", "comment_date": "2021-01-30T12:33:31Z", "likes_count": 13}, {"comment_by": "Arkajit Maity", "comment_text": "We had a test on this in Class 9. We had to program one. This a good video.", "comment_date": "2021-01-23T06:26:32Z", "likes_count": 1}, {"comment_by": "Hudson Van", "comment_text": "This is the best video ever~", "comment_date": "2021-01-22T14:18:11Z", "likes_count": 1}, {"comment_by": "furry", "comment_text": "\u042f \u043f\u043e\u0441\u043b\u0435 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430 \u0442\u0443\u0442", "comment_date": "2021-01-19T13:37:11Z", "likes_count": 0}, {"comment_by": "Arthur", "comment_text": "This NN series was amazing, thank you so much! I&#39;ve watched a lot of these kinds of videos, read several online articles, and am following a popular online course. This series is the best resource by far, so helpful.", "comment_date": "2021-01-18T08:43:35Z", "likes_count": 0}, {"comment_by": "The Mathguy", "comment_text": "you are extermely op. You are OP.", "comment_date": "2021-01-18T00:27:51Z", "likes_count": 0}, {"comment_by": "Nobu Taka", "comment_text": "Also why does the notation change from capital &quot;L&quot; superscript to lowercase &quot;l&quot; superscript without explanation?", "comment_date": "2021-01-17T20:44:24Z", "likes_count": 0}, {"comment_by": "Nobu Taka", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m25s\">9:25</a> you have a box around a term which seems to be expanded in the bigger yellow box below but the superscript changes from &quot;l&quot; to &quot;l+1&quot;.  How can there be an &quot;l+1&quot; if you are backpropagating from the final layer?", "comment_date": "2021-01-17T20:33:29Z", "likes_count": 0}, {"comment_by": "Francis Valcke", "comment_text": "Amazing video. Simply unexcelled math videos. Thank you so very much. Teaching needs a serious makeover. If everything was brought as clearly as this, we would have a lot more successful people on the planet.", "comment_date": "2021-01-15T21:03:07Z", "likes_count": 0}, {"comment_by": "Justin White", "comment_text": "@<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=2m25s\">2:25</a> after watching many other videos.. This can be simplified in understanding that cost = (the result you want) - (what the calculation actually is) squared.<br><br>So here you want result y and z(L) is how that is calculated, so you get the difference.", "comment_date": "2021-01-14T17:17:36Z", "likes_count": 0}, {"comment_by": "Andrea Severini", "comment_text": "This is gold \ud83d\ude0d\ud83d\ude0d", "comment_date": "2021-01-11T16:44:48Z", "likes_count": 0}, {"comment_by": "mr frozen97-hasSisAndBro", "comment_text": "Thankful to my high school which taught us jaw dropping calculas, so that now we understand this ... At least close to reality", "comment_date": "2021-01-10T03:51:19Z", "likes_count": 0}, {"comment_by": "Eugene Crystal", "comment_text": "When I was studying chain rule I found an answer here, in Calculus series, and it was like - oh thank you Grant!<br>Now I&#39;m stuck with Andrew Ng&#39;s backpropagation - and once again, I understood it here. But the algorithms are very different. I clearly understand yours, and still have no idea with Andrew&#39;s :)", "comment_date": "2021-01-07T09:55:56Z", "likes_count": 0}, {"comment_by": "kazenokize2", "comment_text": "I just watched this video going back and forth several times and I am at a point where I am not sure if it made sense or not, but I still feel like it helped. Thank you so much for this series!", "comment_date": "2021-01-06T00:15:36Z", "likes_count": 0}, {"comment_by": "Heshumi", "comment_text": "I have never thought of a chain rule this way! Now I know the intuition behind it. Thank you very much", "comment_date": "2021-01-05T11:03:20Z", "likes_count": 0}, {"comment_by": "Peter Franz", "comment_text": "Crazy this is just free", "comment_date": "2021-01-04T22:31:54Z", "likes_count": 0}, {"comment_by": "Daniel R", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m28s\">9:28</a> shouldn&#39;t the Sum iterate over all Neurons in the Next Layer? <br>Looks to me like the indizes are messed up<br><br>Thus Summing over Layer l+1(via index &quot;i&quot;) would note: <br>            Sum(i = 0, ...) over  w_{k, i} * o(z_{i}) * d(C)/d(a_i)<br><br>My understanding is that a &#39;nudge&#39; to w_{j,k} would influence the Cost function in dependency to all Neurons in the next Layer. (That are connected via weights from a_{j})<br><br>If thats correct I got it.<br>Great Video!", "comment_date": "2021-01-04T15:47:04Z", "likes_count": 0}, {"comment_by": "Tobi", "comment_text": "Remove the background music", "comment_date": "2021-01-01T13:13:23Z", "likes_count": 0}, {"comment_by": "Astronaut", "comment_text": "It helped-A-LOT.. loved it<br>My honest thanks for this series", "comment_date": "2020-12-30T17:27:27Z", "likes_count": 0}, {"comment_by": "Vishnu Krishnaprasad", "comment_text": "I was struggling to wrap my mind around the math of this. Beautifully explained, thank you &lt;3<br>Subscribed!", "comment_date": "2020-12-27T04:58:53Z", "likes_count": 2}, {"comment_by": "Sanskar Shrivastava", "comment_text": "I bow down to you", "comment_date": "2020-12-26T09:49:43Z", "likes_count": 0}, {"comment_by": "Jude Davis", "comment_text": "Please do more videos on this. On the math of this", "comment_date": "2020-12-23T12:00:31Z", "likes_count": 0}, {"comment_by": "naiita bolsen", "comment_text": "im to dumb for this, but then why am i so wise. but then why cant my brain keep the plates spinning?", "comment_date": "2020-12-22T13:36:35Z", "likes_count": 0}, {"comment_by": "Mulan Szechuan", "comment_text": "Thx :D", "comment_date": "2020-12-22T01:17:44Z", "likes_count": 0}, {"comment_by": "Cuber Shil", "comment_text": "I have a question<br><br>For the partial derivative of the layer (L-2) what is y/desired out put going to equal", "comment_date": "2020-12-21T12:20:58Z", "likes_count": 0}, {"comment_by": "owais farooqui", "comment_text": "and when I asked my math teacher, why should I learn calculus, that person said because &quot;to pass the test&quot;. that didn&#39;t make a lot of sense back then.", "comment_date": "2020-12-20T11:42:21Z", "likes_count": 0}, {"comment_by": "Shokhrukh Abduahadov", "comment_text": "So, how learning change occurs, i mean, when we get to the first layer in bp, what to do next and what to do this deltas?", "comment_date": "2020-12-12T07:25:15Z", "likes_count": 0}, {"comment_by": "colorlace", "comment_text": "Best explanation I&#39;ve ever seen of the chain rule in a video that isn&#39;t about the chain rule", "comment_date": "2020-12-09T02:50:42Z", "likes_count": 2}, {"comment_by": "Aidar O' Sullivan", "comment_text": "For some reason this part is now only 144p, what a shame", "comment_date": "2020-12-08T16:57:25Z", "likes_count": 0}, {"comment_by": "Altern 8", "comment_text": "This stuff is absolutely bloody brilliant. Shame I didn&#39;t have this guy as a maths teacher when I was younger.", "comment_date": "2020-12-08T06:40:47Z", "likes_count": 0}, {"comment_by": "Telman S.", "comment_text": "nice animations!", "comment_date": "2020-12-07T13:10:33Z", "likes_count": 0}, {"comment_by": "Austin Segal", "comment_text": "This is the best channel on this app.", "comment_date": "2020-12-05T19:45:24Z", "likes_count": 0}, {"comment_by": "Cameron Duffy", "comment_text": "Have you made a video lecture on convolutional networks and LSTM&#39;s?", "comment_date": "2020-12-03T23:48:41Z", "likes_count": 0}, {"comment_by": "aditya kumar", "comment_text": "looks like your brain has good set of weights and biases.", "comment_date": "2020-12-03T11:20:53Z", "likes_count": 1}, {"comment_by": "K Bisht", "comment_text": "Holy shit. This is perhaps the most beautiful thing I have seen", "comment_date": "2020-12-01T01:28:26Z", "likes_count": 0}, {"comment_by": "Matthew Booth", "comment_text": "Thank you so much for making these videos!  Your approach to teaching math is nothing short of brilliant (and elegant).", "comment_date": "2020-11-28T12:14:36Z", "likes_count": 0}, {"comment_by": "tuseroni", "comment_text": "i am confused by your chain rule notation, i don&#39;t know how to convert it into lagrange&#39;s notation of f&#39;(g(x))g&#39;(x)", "comment_date": "2020-11-27T19:16:38Z", "likes_count": 0}, {"comment_by": "Ramzi Bualuan", "comment_text": "Amazing videos!! Thank you!.<br>Quick question (out of curiosity): what did you use in order to create the great animations?", "comment_date": "2020-11-27T01:52:15Z", "likes_count": 0}, {"comment_by": "Kin Chan", "comment_text": "I wish I was good at English!", "comment_date": "2020-11-26T16:27:32Z", "likes_count": 0}, {"comment_by": "ang Zh", "comment_text": "This video is so amazing.", "comment_date": "2020-11-22T15:10:31Z", "likes_count": 0}, {"comment_by": "M_ K_", "comment_text": "Hi all!<br><br>Other resources give the derivative of the mean squared error as (\u0177-y), compared to 2(\u0177-y) as given in this video. <br><br>Does this difference stem from the trick of adding a 2 (as in Nielsen\u00b4s book) at the definition of the mean squared error: mse = 1/2n sum((\u0177-y)2)? <br>What\u00b4s the reason using the &quot;half mean squared error trick&quot; (1/2n); is it done to make the computation faster?<br><br>Looking forward to your feedback :-)", "comment_date": "2020-11-22T11:47:15Z", "likes_count": 0}, {"comment_by": "ZerdaGdir", "comment_text": "I feel my brain hitting my skull from this video, you put light on all of that information these papers were trowing in there", "comment_date": "2020-11-21T01:25:37Z", "likes_count": 0}, {"comment_by": "Tong Lu", "comment_text": "I come back to this video every few weeks to refresh my memory about how backpropagation works.", "comment_date": "2020-11-20T21:18:06Z", "likes_count": 0}, {"comment_by": "Aleksandr Teplyuk", "comment_text": "I think in @<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m30s\">9:30</a> is a mistake in formula. Missing a sum.", "comment_date": "2020-11-20T17:14:11Z", "likes_count": 0}, {"comment_by": "Aleksandr Teplyuk", "comment_text": "Why on <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m18s\">8:18</a> used \u04210 instead of just C?", "comment_date": "2020-11-20T16:48:34Z", "likes_count": 0}, {"comment_by": "Ethan Hofton", "comment_text": "you are a god, simply put. Thank you so much! helped a alot", "comment_date": "2020-11-18T18:23:23Z", "likes_count": 0}, {"comment_by": "VIBaJ 16", "comment_text": "I will never understand how a video like this gets dislikes, other than dislike bots disliking every video they find...", "comment_date": "2020-11-18T09:50:12Z", "likes_count": 0}, {"comment_by": "Daniel Minchev", "comment_text": "Super helpful!!!", "comment_date": "2020-11-15T09:32:06Z", "likes_count": 0}, {"comment_by": "Mohammad Patel", "comment_text": "minute <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=6m14s\">6:14</a> is peak, breaks the ice. Yes yes, likin it!!! Thanks", "comment_date": "2020-11-12T10:41:06Z", "likes_count": 0}, {"comment_by": "Yohaan Vakil", "comment_text": "unbelievable. unbelievable. you are the best teacher on youtube.", "comment_date": "2020-11-07T05:03:04Z", "likes_count": 0}, {"comment_by": "W Lulu", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m26s\">9:26</a> the subscript is wrong for w, z and a in layer L+1?", "comment_date": "2020-11-06T15:28:47Z", "likes_count": 0}, {"comment_by": "lyntonbr", "comment_text": "Can we get a Chapter 5 where is shown a practical example/exercise?", "comment_date": "2020-11-06T10:28:25Z", "likes_count": 0}, {"comment_by": "Frederik Brok Brandi", "comment_text": "This is amazing. Thank you so much", "comment_date": "2020-11-05T12:56:37Z", "likes_count": 0}, {"comment_by": "charleshwankong", "comment_text": "Thanks Grant for making the material so accessible at such depth.<br><br>Just for clarity, @<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m32s\">9:32</a> should there be double summation for both j and k? The single summation safely renders J relative to the layer, but I think there could be problems by affixing k to one matrix?<br><br>Concretely, say you have a 6 layer network such that:<br>dim(weights(L-2)) = 3x3; <br>dim(weights(L-5)) = 50x40; <br>you&#39;re interested in computing Cost w.r.t. the weight at (25, 25) in layer L-5. <br><br>There would be no k=25 in the weights at layer L-2.", "comment_date": "2020-11-03T22:31:27Z", "likes_count": 1}, {"comment_by": "Przemys\u0142aw Pierzynowski", "comment_text": "I have got the question. How to compute derivative of sigmoid function? It is partial derivative? So, if it is true this partial derivative of a function is with respect to what?", "comment_date": "2020-10-31T10:36:00Z", "likes_count": 1}, {"comment_by": "Ansh Gupta", "comment_text": "Please make a series on Convolutional Neural Networks", "comment_date": "2020-10-26T05:50:25Z", "likes_count": 0}, {"comment_by": "Stany Devito", "comment_text": "All I need now is a video on actually coding something like this. I doubt it&#39;s that difficult as long as you understand the math.", "comment_date": "2020-10-24T03:18:24Z", "likes_count": 0}, {"comment_by": "Kaynbock Mehr", "comment_text": "Ok so I have a verbal exam on neural networks and AI + optimasition in general in 3 days and I am pretty sure this cut my prep time pretty much in half...", "comment_date": "2020-10-23T19:21:17Z", "likes_count": 0}, {"comment_by": "picky potato", "comment_text": "I watched the video around 30 times and now i made a self driving car(though with reinforcement learning) not gradient descent", "comment_date": "2020-10-23T03:44:02Z", "likes_count": 0}, {"comment_by": "Aria Ranjbar", "comment_text": "I love you grant thank you for this. i will pay you one day. i hope", "comment_date": "2020-10-20T07:32:32Z", "likes_count": 0}, {"comment_by": "gauda82", "comment_text": "I&#39;m watching this in 2020. @3Blue1Brown, please make more videos on this topic!! I assume that this series is not complete yet", "comment_date": "2020-10-19T13:42:28Z", "likes_count": 0}, {"comment_by": "nomaan husain", "comment_text": "On the 4th of the series! Just hit the subscribe button", "comment_date": "2020-10-18T15:33:45Z", "likes_count": 0}, {"comment_by": "Anwarul Bashir Shuaib", "comment_text": "It&#39;s not fair :( Please upload more videos on deep learning! How about CNN, RNN, sequence models?", "comment_date": "2020-10-18T15:14:45Z", "likes_count": 0}, {"comment_by": "zoklev", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=5m27s\">5:27</a>", "comment_date": "2020-10-17T12:25:38Z", "likes_count": 0}, {"comment_by": "OOOVincentOOO", "comment_text": "Does the network teaching become more efficient if you define an extra &quot;&quot;no number&quot;  N output?\r<br>0,1,2,3,4,5,6,7,8,9,N\r<br>Then you show the network selected also pictures that are non numbers (like checkboard, four quadrants etc)? Then you might maybe hit a minima more quickly?", "comment_date": "2020-10-16T12:43:07Z", "likes_count": 0}, {"comment_by": "Xin Yi", "comment_text": "Is there anybody can explain why weight between a_k^(L-1) and a_k^(L) is w_jk^(L)? Still confused...\ud83d\ude2d Help me please!!", "comment_date": "2020-10-15T02:23:35Z", "likes_count": 0}, {"comment_by": "Aryan Raina", "comment_text": "Best deep learning series i found on internet even Andrew Ng&#39;s course is not this good. The way you covered the maths part in this video was awesome. Thanks!", "comment_date": "2020-10-13T10:53:08Z", "likes_count": 0}, {"comment_by": "Chinmay", "comment_text": "Thanks a lot for all that you do Grant !", "comment_date": "2020-10-12T21:09:28Z", "likes_count": 0}, {"comment_by": "tiago l", "comment_text": "I&#39;m proud of myself for understanding most of that.<br>Great video!", "comment_date": "2020-10-11T19:43:19Z", "likes_count": 2}, {"comment_by": "A Joule Thief", "comment_text": "Why are we putting it through a non leanier function", "comment_date": "2020-10-07T11:54:09Z", "likes_count": 1}, {"comment_by": "James Jin", "comment_text": "All of the animation you used are so simple yet at the same time so illuminating. I bet people would appreciate your videos even more when at the end of the video you zoom out to show all of your visual aid in a nicely summarized flow chart / spatial diagram.", "comment_date": "2020-10-07T01:01:44Z", "likes_count": 1}, {"comment_by": "Nelson C\u00e1rdenas Bola\u00f1o", "comment_text": "Really awesome. Thanks for your hard work", "comment_date": "2020-10-04T21:58:33Z", "likes_count": 0}, {"comment_by": "Etienne Orio", "comment_text": "You explain things a LOTS better than any teacher out there, well done man", "comment_date": "2020-10-02T22:57:27Z", "likes_count": 0}, {"comment_by": "Shivang Raisurana", "comment_text": "the number of view per video in this series keeps decreasing... 7mill...3.5.. and now 1.2", "comment_date": "2020-10-02T07:54:32Z", "likes_count": 0}, {"comment_by": "venkatanaresh suddula", "comment_text": "Extra ordinary video ever sir", "comment_date": "2020-10-01T15:40:50Z", "likes_count": 0}, {"comment_by": "Thiago Crepaldi", "comment_text": "Amazing video! Thank you! It would be even nicer with a small numeric example, starting from the target value y and going backwards until propagation ends. Mapping what are inputs and outputs in a matrix/vector are is confusing sometimes and an actual example is that necessary repetition exercise to grab hold of it", "comment_date": "2020-09-29T00:23:37Z", "likes_count": 0}, {"comment_by": "Aditya Pratap Singh", "comment_text": "well . just awesome .", "comment_date": "2020-09-25T08:49:52Z", "likes_count": 1}, {"comment_by": "Praykshee Saini", "comment_text": "love the channel and thank you for the playlist,loved it.. just couldnt understand how did the relation of del(C)/del(ajl) come in <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m25s\">9:25</a>", "comment_date": "2020-09-21T10:41:15Z", "likes_count": 0}, {"comment_by": "Savit Bharadwaj", "comment_text": "amazing", "comment_date": "2020-09-18T15:23:29Z", "likes_count": 0}, {"comment_by": "WistrelChianti", "comment_text": "I feel like I want to cry. After spending <b>ages</b> watching this and hammering the back button more times than I care to estimate... I was thrown in front of the truck at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m30s\">8:30</a> and squashed into the ground at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m20s\">9:20</a>. This feels like the &quot;bad ending&quot; in some sort of visual novel. It can&#39;t end this way... I will have to play the game again.", "comment_date": "2020-09-16T21:38:58Z", "likes_count": 0}, {"comment_by": "Mizar", "comment_text": "I am speechless. This channel undoubtedly contains the best pedagogical scientific material on Youtube, and possibly in the world. Thanks for making these videos, your skills and passion are unreachable!", "comment_date": "2020-09-15T14:53:46Z", "likes_count": 0}, {"comment_by": "GyoungJin Gim", "comment_text": "back propagation\uc744 \uc774\ub807\uac8c \uba85\ub8cc\ud558\uac8c \uc124\uba85\ud55c \uc601\uc0c1\uc740 \ucc98\uc74c\uc785\ub2c8\ub2e4. \ub300\uac10\ub3d9 \u3160 You are genius!!", "comment_date": "2020-09-15T13:30:30Z", "likes_count": 1}, {"comment_by": "Shraga Mildiner", "comment_text": "This video series was so good that I subscribed to the channel. No idea why I hadn&#39;t yet", "comment_date": "2020-09-14T15:50:08Z", "likes_count": 0}, {"comment_by": "Sweardog", "comment_text": "Is it me or did someone forget to subscript the zero in that one del c / del a term in the chain rule equation?", "comment_date": "2020-09-07T23:38:35Z", "likes_count": 0}, {"comment_by": "MI982", "comment_text": "I think I get this. I&#39;m scared.", "comment_date": "2020-09-04T23:17:26Z", "likes_count": 0}, {"comment_by": "Aditya", "comment_text": "Now someone just needs to explain the compact vectorized form....", "comment_date": "2020-09-02T10:50:45Z", "likes_count": 0}, {"comment_by": "Okey!", "comment_text": "i didnt understand :D", "comment_date": "2020-09-02T08:11:55Z", "likes_count": 0}, {"comment_by": "Orie", "comment_text": "Neural networks make much more sense to me now after watching the series, I am really curious about the global minima, would love to see some content from you about it. Amazing, as always!", "comment_date": "2020-08-31T23:13:49Z", "likes_count": 0}, {"comment_by": "JP Deka", "comment_text": "Brilliant lecture... I understood Backpropagation a lot better after you went full mathematical in this video... :)", "comment_date": "2020-08-30T17:31:50Z", "likes_count": 0}, {"comment_by": "Fady Algyar", "comment_text": "Perfect! you made it really simple", "comment_date": "2020-08-29T12:49:35Z", "likes_count": 0}, {"comment_by": "Rishi", "comment_text": "I want more of these \ud83d\ude0d \ud83d\ude4f", "comment_date": "2020-08-28T18:05:17Z", "likes_count": 0}, {"comment_by": "Samir EL ZEIN", "comment_text": "waiting for your videos on reinforcement learning", "comment_date": "2020-08-26T19:27:00Z", "likes_count": 0}, {"comment_by": "kushal shah", "comment_text": "Can you please explain the derivative of sigmoid function in detail. It is skipped and jumped to answer but its complicated would love to see your technique.", "comment_date": "2020-08-24T17:51:44Z", "likes_count": 0}, {"comment_by": "Ninos Hermis", "comment_text": "how do these formulas fit within the Convolution Integral???", "comment_date": "2020-08-24T00:38:38Z", "likes_count": 0}, {"comment_by": "Surjayan Ghosh", "comment_text": "I didn&#39;t quite understand how the cost got back propagated from L-1 to L-2. <br><br>To know how much to nudge the weights connected to the node in the last layer(L) we calculate the cost as (y- a(L)) and see how to change the weight such that the cost is lowered.<br><br>So, now to similarly tweak the weights for the nodes in  L-1 layer. How do I calculate the Cost ?? I don&#39;t have a &quot;y&quot; at this layer ?<br><br>Where am I lost? \u2639\ufe0f", "comment_date": "2020-08-22T21:53:30Z", "likes_count": 0}, {"comment_by": "Matthew Haythornthwaite", "comment_text": "If anyone is interested, I worked through the chain rule for the differential of the cost function w.r.t the weight in the second layer down. Two additional terms are added to make everything cancel as they should. It shows how as you progress down the layers, more partial differentials are added to the equation from all the variables above, making it more unstable and hence more susceptible to the exploding or vanishing gradient problem. <br><br>dC/dw(L-1) = dz(L-1)/dw(L-1) * da(L-1)/dz(L-1) * dz(L)/da(L-1) * da(L)/dz(L) * dC/da(L)", "comment_date": "2020-08-22T15:15:05Z", "likes_count": 32}, {"comment_by": "dogeun Kim", "comment_text": "Is there a web version of your videos. I mean in text and images. thanks", "comment_date": "2020-08-20T13:33:21Z", "likes_count": 0}, {"comment_by": "bArda26", "comment_text": "you are beautiful", "comment_date": "2020-08-20T05:13:52Z", "likes_count": 0}, {"comment_by": "Oheey", "comment_text": "\u6211\u8111\u888b\u70b8\u4e86", "comment_date": "2020-08-16T03:53:27Z", "likes_count": 0}, {"comment_by": "Aneesh Prasobhan", "comment_text": "I can&#39;t believe more than 1 million people reached video 4. Damn, humans are pretty smart.", "comment_date": "2020-08-15T18:18:31Z", "likes_count": 1}, {"comment_by": "Dylan Campopiano", "comment_text": "You make me understand what people mean when they say math is beautiful", "comment_date": "2020-08-11T14:29:39Z", "likes_count": 0}, {"comment_by": "Danish Nazir", "comment_text": "More power to you! , This is some next level explanation everything is clear atleast for me now thanks once again", "comment_date": "2020-08-10T20:19:08Z", "likes_count": 0}, {"comment_by": "Felix Paul", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=5m44s\">5:44</a> \\partial C_0. Not \\partial C0. Just to let you know that I watched this video like a quadrillion times. Don&#39;t know why youtube doesn&#39;t count it.", "comment_date": "2020-08-10T19:00:55Z", "likes_count": 1}, {"comment_by": "steelcitysi", "comment_text": "This was the best animated description I&#39;ve come across. I hope you can continue on this topic more. Especially interested in the jump to CNNs, and the intuition for the effects of changing the number of layers and number of nodes in the hidden layers.", "comment_date": "2020-08-10T04:04:36Z", "likes_count": 0}, {"comment_by": "Bishal Thapaliya", "comment_text": "What a video ! Loved it . This makes life more easier.", "comment_date": "2020-08-09T06:40:10Z", "likes_count": 0}, {"comment_by": "Jessica FB", "comment_text": "Thank you for this excellent series! It makes it so much easier to conceptualize. Now, off to code! :)", "comment_date": "2020-08-05T22:27:26Z", "likes_count": 0}, {"comment_by": "matbmp", "comment_text": "Is there a typo around minute <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m00s\">9:00</a>? It&#39;s written &quot;Sum over layer L&quot;, but the upper index of sum is n(L-1). Or am I missing something? I paused the video earlier, without the text under and couldn&#39;t get it.", "comment_date": "2020-07-31T07:33:13Z", "likes_count": 1}, {"comment_by": "N", "comment_text": "This 10 min video is pure gold. Lays down the math in an easy to understand, intuitive manner.", "comment_date": "2020-07-30T19:11:40Z", "likes_count": 0}, {"comment_by": "Boldizsar Zopcsak", "comment_text": "Excellent video", "comment_date": "2020-07-30T09:01:48Z", "likes_count": 0}, {"comment_by": "Sarvesh _77", "comment_text": "Only one word for you, and I can&#39;t stress more, &quot;GOD&quot;<br>I feel privileged to comment this too", "comment_date": "2020-07-24T17:11:57Z", "likes_count": 0}, {"comment_by": "GAMING R DRAGO", "comment_text": "eead", "comment_date": "2020-07-24T15:59:56Z", "likes_count": 0}, {"comment_by": "Ishaan Zharotia", "comment_text": "Can you make a video on why overfitting/overtraining can happen?", "comment_date": "2020-07-22T15:38:16Z", "likes_count": 5}, {"comment_by": "Bharath BN", "comment_text": "&quot;take a bow &quot; for your explination...................well explained", "comment_date": "2020-07-19T04:33:23Z", "likes_count": 0}, {"comment_by": "Itamar Erenberg", "comment_text": "thenk you so much, finally i understand this subject after years i trying to understand this.", "comment_date": "2020-07-18T23:35:05Z", "likes_count": 0}, {"comment_by": "SHArK K", "comment_text": "Best vdeo i watch till today", "comment_date": "2020-07-18T18:02:15Z", "likes_count": 0}, {"comment_by": "\uc544\uce68\uc740\uc721\uac1c\uc7a5", "comment_text": "Sir, I have a question.<br>What is meaning of the number of layer and the number of Neurons per layer?<br>ex) They are affect to Convergence rate of weight?(\rLearning speed) or Resolution limit? or The limitation of Prediction accuracy? or other things?<br>I don&#39;t understand what is advantage of many layer and many number of neurons.<br>and If you know textbook about this issue, please recommend to me. <br>Thank you for your videos and reading my question.", "comment_date": "2020-07-17T14:21:47Z", "likes_count": 0}, {"comment_by": "Surt McGert", "comment_text": "I am slightly confused, you said that the activation influences the cost function through multiple paths so you add them together... but the bias also influences the cost through different paths but you dont add that up", "comment_date": "2020-07-15T09:10:11Z", "likes_count": 0}, {"comment_by": "Anjel Patel", "comment_text": "Beautiful. Absolutely Gold.", "comment_date": "2020-07-11T14:56:05Z", "likes_count": 0}, {"comment_by": "ashim karki", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m55s\">8:55</a> &quot;add those up&quot; made my day", "comment_date": "2020-07-06T11:33:22Z", "likes_count": 1}, {"comment_by": "Daniel Morgan", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m30s\">4:30</a>, how do you calculate the derivative of sigmoid(z^L)?", "comment_date": "2020-07-04T20:20:07Z", "likes_count": 2}, {"comment_by": "OriFl", "comment_text": "i fucking love you no homo", "comment_date": "2020-07-01T17:01:14Z", "likes_count": 1}, {"comment_by": "v pramod", "comment_text": "Wowww... Machine learning is easy\ud83d\ude1c\ud83d\ude1c", "comment_date": "2020-07-01T13:18:31Z", "likes_count": 0}, {"comment_by": "VoyTech Movie", "comment_text": "Thanks for this video!!!!!!! :)", "comment_date": "2020-06-29T20:54:19Z", "likes_count": 1}, {"comment_by": "Lala", "comment_text": "wow I can/not/ thank you enough for this", "comment_date": "2020-06-29T11:05:19Z", "likes_count": 0}, {"comment_by": "Chadi Amani", "comment_text": "Hey <br>I have just a noticed a small error on derivate of costfunction to a  ! d C / d a ! <br>C does not equal (a-y)\u00b2 ! but it equal -Y*log(a)-(1-y)log(1-a) ! if we do the derivate it will give the result  a-y <br>Thank your for videos", "comment_date": "2020-06-29T08:39:34Z", "likes_count": 0}, {"comment_by": "Chadi Amani", "comment_text": "Once i work i will support you on patreon ! thank you for your awesome videos &lt;3", "comment_date": "2020-06-29T02:01:27Z", "likes_count": 0}, {"comment_by": "\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0418\u0432\u0430\u043d", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=2m18s\">2:18</a> So, how do I calculate value what want to change w^(L) and how to get the cost for a^(L-1) to spread the change further?", "comment_date": "2020-06-26T16:45:15Z", "likes_count": 12}, {"comment_by": "nawmee rahman", "comment_text": "I love 3B1B!", "comment_date": "2020-06-26T11:07:32Z", "likes_count": 0}, {"comment_by": "J. Ignacio G\u00f3mez", "comment_text": "Thank you very much ! this is the first video of yours that i&#39;ve seen and i&#39;m amazed. You&#39;ve got a follower in Argentina!", "comment_date": "2020-06-24T23:58:03Z", "likes_count": 0}, {"comment_by": "einemailadressenbesitzer einemailadressenbesitzer", "comment_text": "Hi, your video is not clarifying how the y(label) for the previous layer is calculated resp. how the error is propagated backwards. So the previous y is calculated by multiplying |x-y|^2 it by the weight, right?", "comment_date": "2020-06-24T09:11:27Z", "likes_count": 0}, {"comment_by": "Pavan Vishwanath", "comment_text": "The best intuition of the backprop ever!", "comment_date": "2020-06-23T17:33:03Z", "likes_count": 0}, {"comment_by": "Vikas Kumar Bansal", "comment_text": "Can someone give me the list of 180 people who downvoted this video? Want to ban their youtube account!", "comment_date": "2020-06-22T17:47:23Z", "likes_count": 0}, {"comment_by": "Miya Sanchez", "comment_text": "You saved my life.", "comment_date": "2020-06-21T10:48:49Z", "likes_count": 0}, {"comment_by": "divyanshu vyas", "comment_text": "If i get a Job or a Position, just so that I get to share my success story- 80% of my speech will be &#39;thank you 3Blue1Brown&#39;. <br>You are a blessing! Cheers.", "comment_date": "2020-06-19T19:11:23Z", "likes_count": 0}, {"comment_by": "Ramya Loganathan", "comment_text": "why am I so late to this channel! Grateful forever!", "comment_date": "2020-06-19T13:23:23Z", "likes_count": 0}, {"comment_by": "Animiles", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=1m54s\">1:54</a> Yeah, some people really have a bias towards BL xD", "comment_date": "2020-06-17T13:40:14Z", "likes_count": 0}, {"comment_by": "blaze", "comment_text": "beautifully explained!", "comment_date": "2020-06-16T13:49:46Z", "likes_count": 0}, {"comment_by": "Rohan Walia", "comment_text": "This is extremely well explained. Your work is exemplary!", "comment_date": "2020-06-14T23:36:34Z", "likes_count": 0}, {"comment_by": "Rohan Walia", "comment_text": "Why is the cost function square of the difference of the activation and the output?", "comment_date": "2020-06-13T18:20:45Z", "likes_count": 0}, {"comment_by": "Shreya Jain", "comment_text": "You gave me some feel on deep learning. thanks", "comment_date": "2020-06-12T18:57:01Z", "likes_count": 0}, {"comment_by": "\uc548\uc7ac\ud615", "comment_text": "Thanks a lot. Now I got some intuition. Please keep doing your great works.", "comment_date": "2020-06-10T04:09:21Z", "likes_count": 0}, {"comment_by": "Alexander Johansson", "comment_text": "This is such an intuitive explanation of backpropagation! Simply amazing, thank you! &lt;3", "comment_date": "2020-06-09T08:53:46Z", "likes_count": 0}, {"comment_by": "One Sun", "comment_text": "You should have called a(L) &#39;y hat&#39;. Why complicate things by being different than 99% of deep learning notation to people trying to learn?", "comment_date": "2020-06-08T17:43:03Z", "likes_count": 0}, {"comment_by": "Sai Nandan Desetti", "comment_text": "Stunningly beautiful...<br>The best part of the series (for me, obviously) is that the beauty of this series does NOT make it very easy to understand.<br>No. Each video may need multiple views. But these videos are so beautifully made that you&#39;d want to watch them again and again, not with the frustration of getting your head over a concept but with the thrill of unravelling a mystery...<br><br>So for creating such excitement in me, thank you.", "comment_date": "2020-06-08T04:13:19Z", "likes_count": 0}, {"comment_by": "Shivam Shrivastava", "comment_text": "Mr. Grant please make videos on CNN and LSTM, the world seriously needs videos of yours on these topics...PLEASEEEEE.<br>Thanks for your <br>astonishing<br>eye-opening<br>hallucinatory<br>mind-altering<br>mind-boggling<br>overwhelming<br>psychedelic<br>staggering<br>stunning<br>wonderful explanation to the world.", "comment_date": "2020-06-07T11:17:11Z", "likes_count": 1}, {"comment_by": "Omkar Mali", "comment_text": "You are genius.", "comment_date": "2020-06-07T05:54:53Z", "likes_count": 0}, {"comment_by": "PenguinMaths", "comment_text": "Absolutely amazing. Thank you for making this complicated topic so accessible.", "comment_date": "2020-06-07T02:32:01Z", "likes_count": 0}, {"comment_by": "Eliot Khachi", "comment_text": "Amazing series. Only you could make learning Neural Nets this easy. I remember watching a couple 2 hour MIT lectures that barely introduced Neural Nets b/c the course gets caught up in the notation instead of focusing on helping you understand what you&#39;re modeling. Many professors these days teach backwards, starting from the already developed, very structured, extensive math instead of starting from the concepts that gave rise to the math. I&#39;m already quite familiar with calculus and linear algebra and I can&#39;t wait to start coding a Neural Net. Although, I&#39;m new to Python and I&#39;m not sure how to read the .gz image files into an array of inputs ranging from 0 to 1 in pixel intensity.", "comment_date": "2020-06-03T21:33:02Z", "likes_count": 0}, {"comment_by": "Dhiraj Kumar Sahu", "comment_text": "You are a gift to humanity....I thank God, for such a valuable gift", "comment_date": "2020-06-03T11:49:55Z", "likes_count": 0}, {"comment_by": "Ilias P", "comment_text": "i can&#39;t express my gratitude for this amazing seires.<br>thank you.", "comment_date": "2020-06-03T11:12:09Z", "likes_count": 0}, {"comment_by": "Josiah McKay", "comment_text": "I&#39;ve never set foot in a calculus class, but i understood (the concept at least) of everything you just said. The visuals definitely helped a lot.", "comment_date": "2020-06-03T04:06:57Z", "likes_count": 0}, {"comment_by": "rafeang", "comment_text": "Thanks for this series! I have a question: So after knowing all the derivatives that determine each component in the gradient how does the network &quot;tune&quot; itself to minimize it&#39;s cost (step downhill?) Does it just adjust all the weights and biases according to the magnitude of their derivatives x the learning rate?", "comment_date": "2020-06-03T03:18:38Z", "likes_count": 0}, {"comment_by": "Bradley Dennis", "comment_text": "I just finished up calc iii this semester and I have never felt happier with myself for being able to apply my new knowledge than this episode. I also don&#39;t think I have ever been more excited to hear calc iii topics being brought up in a field I am trying to teach myself currently. Thank you for making such a simple to understand series!", "comment_date": "2020-05-31T04:48:24Z", "likes_count": 29}, {"comment_by": "Ayan Dogra", "comment_text": "Amazing video <br>Cleared lot of doubts I had about taking derivatives in backpropagation", "comment_date": "2020-05-29T09:38:53Z", "likes_count": 0}, {"comment_by": "Ashish Adhikari", "comment_text": "When will you release the next video on Neural Networks?", "comment_date": "2020-05-29T06:52:21Z", "likes_count": 0}, {"comment_by": "Samar Emara", "comment_text": "This is amazing! Thank you so much! I feel like I understand deep learning very well now thanks to your simple, deep and fun explanation", "comment_date": "2020-05-28T08:42:04Z", "likes_count": 0}, {"comment_by": "Surt McGert", "comment_text": "is error and cost the same? because from all of my other resources on this topic I have never come across the cost function. I thought you use the error to backpropogate...", "comment_date": "2020-05-27T17:10:00Z", "likes_count": 0}, {"comment_by": "arjun", "comment_text": "Thnx a lot brother!", "comment_date": "2020-05-25T14:58:37Z", "likes_count": 1}, {"comment_by": "joao pedro", "comment_text": "I liked your series very much, it was good to get know more about the details behind neural networks", "comment_date": "2020-05-24T12:01:42Z", "likes_count": 0}, {"comment_by": "Liakpin Lekbo", "comment_text": "I truely loved this video! Thank you sir &lt;3", "comment_date": "2020-05-23T19:02:23Z", "likes_count": 0}, {"comment_by": "Hardik Bhati", "comment_text": "Can u please do a video on Generative Adversarial Networks (GANs), it will be great help", "comment_date": "2020-05-23T11:46:22Z", "likes_count": 0}, {"comment_by": "MrDvbnhbq", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m57s\">4:57</a>", "comment_date": "2020-05-22T23:47:56Z", "likes_count": 1}, {"comment_by": "Yuichi YH", "comment_text": "I watched and tried to make a program...<br>in the end my program had less than %50 of accuracy", "comment_date": "2020-05-22T21:37:02Z", "likes_count": 0}, {"comment_by": "Todian Mishtaku", "comment_text": "Great. Absolutely worth it.", "comment_date": "2020-05-22T21:29:09Z", "likes_count": 0}, {"comment_by": "C.U.Senthil Kumar", "comment_text": "Simply wow.....no further comment needed to applause your content.", "comment_date": "2020-05-21T20:48:56Z", "likes_count": 0}, {"comment_by": "Jackisaboss1208", "comment_text": "If you get a chance I&#39;d love to see you explain recurrent neural networks! Would be very helpful for me", "comment_date": "2020-05-19T17:58:31Z", "likes_count": 0}, {"comment_by": "M. Ali", "comment_text": "Impressive video , it helps to understand the components of the gradient vector mathematically and the gradient vector itself at an abstract level. But doesn&#39;t delve into the question of how we would use that gradient vector  to make the adjustments.", "comment_date": "2020-05-19T17:56:41Z", "likes_count": 0}, {"comment_by": "TruthAloneTriumphs", "comment_text": "You can see how much damage is done by teachers who have little interest in their subject. They kill the interest and the enthusiasm and students lose their love for the subject. How can anyone not love this stuff? I am a 58 year old Master&#39;s in CS.", "comment_date": "2020-05-19T03:58:05Z", "likes_count": 0}, {"comment_by": "jaime caballero", "comment_text": "These videos are references in state of the art Deep Learning books for understanding concepts such as this. That tells a lot about how good they are.", "comment_date": "2020-05-19T03:20:32Z", "likes_count": 0}, {"comment_by": "Rohit Datla", "comment_text": "u r not just teaching NN concept but how to think, break down and understand any complex problem and digest, U R AWESOME!!!!!", "comment_date": "2020-05-17T18:48:36Z", "likes_count": 0}, {"comment_by": "Black_Wind", "comment_text": "And how about finding the global extremum of the cost function ? Because with the gradient descent you fall directly into a local extremum, which is an issue for the algorithm&#39;s performance, no ? Is there a way to implement some other algorithms such as simulated annealing, or even some type of genetic evolution ?", "comment_date": "2020-05-17T06:42:33Z", "likes_count": 0}, {"comment_by": "Maggie Liuzzi", "comment_text": "Amazing!!", "comment_date": "2020-05-15T03:38:44Z", "likes_count": 1}, {"comment_by": "Jasper Butcher", "comment_text": "Man here i am looking through the comments to answer questions, but 99% of them are complements to how amazing these videos are...", "comment_date": "2020-05-14T22:04:18Z", "likes_count": 0}, {"comment_by": "Andrei Margeloiu", "comment_text": "Fabulous explanation!", "comment_date": "2020-05-14T08:12:56Z", "likes_count": 0}, {"comment_by": "Nick C", "comment_text": "While youtube explains better than my professor...", "comment_date": "2020-05-12T16:55:12Z", "likes_count": 0}, {"comment_by": "Abhinav Choudhary", "comment_text": "I watched this neural networks series....it&#39;s really great, it was very well explained.<br>I have a question though....I tried to run your code after I downloaded it from GitHub I ran it on VScode as well as Jupyter....but it didn&#39;t run. Please tell me the IDE that you used to execute this code.", "comment_date": "2020-05-11T16:25:48Z", "likes_count": 0}, {"comment_by": "\u6e5b\u7136\u884c\u8005", "comment_text": "This is Great !", "comment_date": "2020-05-11T02:51:44Z", "likes_count": 0}, {"comment_by": "Arjun Raja", "comment_text": "Finally I can find Old Zealand with backpropogation", "comment_date": "2020-05-10T23:00:15Z", "likes_count": 0}, {"comment_by": "Shilan Savan", "comment_text": "A very informative and clear explanation. Keep up the good job. Thank you \ud83d\ude0a", "comment_date": "2020-05-10T21:31:14Z", "likes_count": 0}, {"comment_by": "Srivatsan R", "comment_text": "wait is that it? I was hoping for more videos :( But still this was one of the best explanations of NN that i have ever seen :)", "comment_date": "2020-05-09T07:26:10Z", "likes_count": 0}, {"comment_by": "low_homie", "comment_text": "Its brilliant,but i genuinely wanted the log loss cost function", "comment_date": "2020-05-08T19:35:14Z", "likes_count": 0}, {"comment_by": "3StepsAhead", "comment_text": "OMFG... brilliant, this is totally enlightening, thank you so so so much", "comment_date": "2020-05-08T14:50:36Z", "likes_count": 0}, {"comment_by": "cd412", "comment_text": "So helpful to explain the chain rule that way. Great job!", "comment_date": "2020-05-08T14:48:27Z", "likes_count": 0}, {"comment_by": "Carlos Pegueros", "comment_text": "I think you forgot to change the superscript of a^(L) to a^(L-1) in the second and third derivatives when applying the chain rule around minute <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=6m13s\">6:13</a> .", "comment_date": "2020-05-06T18:56:34Z", "likes_count": 0}, {"comment_by": "Kevin", "comment_text": "Honestly this channel doesn&#39;t deserve a dislike button. It took me days to figure out one video(at the beginning),but the concepts remain still in my head. This channel taught us that maths is not just changing numbers, but its conceptual and intuitive just like science. Grant if you are ever read this, please know that you are one of the very few people that change the world. I just dont have words for you man, great job is an understatement for you. I promise once i earn enough i will contribute to your channel", "comment_date": "2020-05-06T10:09:10Z", "likes_count": 2}, {"comment_by": "Taylor Jewell", "comment_text": "CNN AND LSTM VIDS PLEASE, THE PEOPLE NEED THEM", "comment_date": "2020-05-05T00:52:59Z", "likes_count": 0}, {"comment_by": "Will be free", "comment_text": "If you train a neural network to do a specific task, you may find that you don&#39;t have enough layers in the network to give a definitive answer.<br><br><br>Do you need to start over from the beginning with the training if you make changes in the network, or can you salvage something of the work previously put in?", "comment_date": "2020-05-04T15:45:09Z", "likes_count": 0}, {"comment_by": "Chirag Palan", "comment_text": "after seeing this neural networks looks like 1+1 to me!!! Amazing Boss.....  Keep uploading more videos... Is there any place where I can see deep course on machine learning by you ?", "comment_date": "2020-05-04T09:50:11Z", "likes_count": 1}, {"comment_by": "Thiyagu Tenysen", "comment_text": "I came here after Andrew Ng&#39;s week 5 in coursera and you blew my mind", "comment_date": "2020-05-02T17:26:20Z", "likes_count": 284}, {"comment_by": "No one", "comment_text": "Thanks a lot !!!", "comment_date": "2020-05-01T20:54:43Z", "likes_count": 0}, {"comment_by": "Eun Ju Lee", "comment_text": "You are so much better than my professor. Was so lost with the assignment and now I kind of have the lead to follow", "comment_date": "2020-04-30T08:05:57Z", "likes_count": 0}, {"comment_by": "Lucas Andres Costa", "comment_text": "Welp, my brain is completly fried now", "comment_date": "2020-04-29T04:09:24Z", "likes_count": 0}, {"comment_by": "Ich Bins", "comment_text": "There is a typo in the sum in <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m30s\">9:30</a> (in the yellow box)   - j will be contracted in this case", "comment_date": "2020-04-28T23:14:15Z", "likes_count": 1}, {"comment_by": "Undergrad", "comment_text": "Holy shit bro, this video is the best I have seen on Backprop calculus!", "comment_date": "2020-04-28T22:49:54Z", "likes_count": 0}, {"comment_by": "WeinSim", "comment_text": "Thanks so much for all these awesome videos! If an hour ago you&#39;d shown me these formulas I&#39;d have been like &quot;what the hell I&#39;m never gonna understand any of this&quot;, but now, after watching this video (and the other 3 as well), and also taking out a pen + piece of paper to write everything down, I feel confident that now I understand it. It&#39;s like speeking / reading in another language, and you make it so simple and accessible to understand. thatk you", "comment_date": "2020-04-28T20:06:01Z", "likes_count": 0}, {"comment_by": "Zhe Xiang", "comment_text": "Sir, i cant thank you enough of how simply and clearly you explained this. Makes university professors look bad tbh. Thank you so much!", "comment_date": "2020-04-28T15:23:28Z", "likes_count": 0}, {"comment_by": "Rokia lamrani alaoui", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m29s\">9:29</a>, is there an error in the index j and k, or the j in aj is the same as in the wjk inside the sum ? thanks", "comment_date": "2020-04-27T23:01:45Z", "likes_count": 0}, {"comment_by": "Mangilipally Laxmi Narayana", "comment_text": "I can&#39;t thank you enough for the explanation. I got stuck at back propagation for almost a week now. Liked and Subscribed. Thanks a lot again", "comment_date": "2020-04-27T16:35:42Z", "likes_count": 0}, {"comment_by": "\u9648\u6d1e\u5929", "comment_text": "My brain exploded.", "comment_date": "2020-04-24T13:37:01Z", "likes_count": 0}, {"comment_by": "Michael Wolfgang", "comment_text": "Brilliant videos! Made me understand everything after the second episode! <br><br>Although I still don&#39;t get it why you&#39;d need a formula for each weight and bias when you could just tweak them a little, then calculate the derivative of the cost function and start propagating. Maybe, I&#39;ve been a little too quick saying I understood everything after the second episode, or a formula for each weight and bias just makes it faster a bit...<br><br>Also, having watched all 4 episodes I still don&#39;t understand by what fraction of \u2207C should we change the weights and biases, what is the multiplier of \u2207C that we should use for the gradient descent. An arbitrary small number? Then the training session will go forever, I guess. I searched the web for the answer and stumbled across a spooky formulae for an adaptive multiple. If that&#39;s the only way, I dive right into understanding it. Otherwise, can somebody tell me what is a good multiplier that is safe enough to use?<br><br>Thanks again, this series is just perfect!!!", "comment_date": "2020-04-23T14:17:41Z", "likes_count": 0}, {"comment_by": "Rahul Singh", "comment_text": "bro how do u make such sexy presetations?", "comment_date": "2020-04-23T07:56:41Z", "likes_count": 0}, {"comment_by": "SnortsOfHappiness", "comment_text": "ANYTHING IS POSSIBLE.", "comment_date": "2020-04-22T03:58:40Z", "likes_count": 0}, {"comment_by": "Youngsoo Choy", "comment_text": "This is the best mathematical explanation about the backpropagation of neural network. I&#39;ve watched other coursera courses twice, but nothing can be compared to this well-visualized and easy to understand explanation.", "comment_date": "2020-04-20T16:40:58Z", "likes_count": 0}, {"comment_by": "Shidharth Routh", "comment_text": "The explanations were truly detailed and Concise, just like they should be... since I am slow learner and it\u2019s definitely a stretch but I would really appreciate if you could teach this with the help of a real life example with numbers .... thank you so much for this awesome video \ud83d\ude0a", "comment_date": "2020-04-19T21:51:03Z", "likes_count": 0}, {"comment_by": "Rohit Venkat Gandhi Mendadhala", "comment_text": "This is what we call &quot;God Level Explanation&quot;. I did not find a video or resource which explained better than this. Thank you very much 3B1B.", "comment_date": "2020-04-17T22:29:12Z", "likes_count": 0}, {"comment_by": "Decapoli", "comment_text": "You are a genious, amazing content, thanks!", "comment_date": "2020-04-17T16:39:32Z", "likes_count": 0}, {"comment_by": "Ranit Chatterjee", "comment_text": "I can&#39;t explain how good you&#39;re for your explanation!!!", "comment_date": "2020-04-15T21:20:41Z", "likes_count": 0}, {"comment_by": "Mohamed Saleh", "comment_text": "If anybody knows please and can help me because I&#39;m stuck ....How to do backpropagation on multiple input samples ?? Do we take average of derivatives or what is done ??", "comment_date": "2020-04-15T20:35:56Z", "likes_count": 1}, {"comment_by": "Farid Jafarov", "comment_text": "After watching this video, I went to your patreon account and become a patreon. I read and watch backpropagation from all other resources, but all of them use complex notations that you need to be a master&#39;s degree at maths. But this one was clear, thank you so much for all your marvelous content!", "comment_date": "2020-04-13T19:19:48Z", "likes_count": 0}, {"comment_by": "Fahd Ciwan", "comment_text": "ahhh .... the mind orgasm after ur brain finally swallows the concept ... priceless!!", "comment_date": "2020-04-12T20:23:28Z", "likes_count": 0}, {"comment_by": "MADHUKUMAR S 17BEE0117", "comment_text": "Why can&#39;t you collaborate with ben eater and work on building breadboard perception or stochastic gradient descent hardware circuit?", "comment_date": "2020-04-12T19:21:55Z", "likes_count": 0}, {"comment_by": "Shailesh Dagar", "comment_text": "So, in a vast neural network, the weights, biases and the activation values closer to the input layer when nudged must influence some activation value(s) in the next layer which influence all the activation values in the next layer and on and on like a chain reaction till the output layer. And Backpropagation helps us calculate and these effects. Am I right about this?", "comment_date": "2020-04-11T17:39:38Z", "likes_count": 0}, {"comment_by": "M\u0101rti\u0146\u0161 Mickus", "comment_text": "Thank You very much! You helped me to understand it quite a lot but eventually, I somehow came up with a few of my own methods as well as how to calculate all this... Interesting (I&#39;ll give it a try to program it).", "comment_date": "2020-04-11T09:24:08Z", "likes_count": 0}, {"comment_by": "Dimitrios Eskitzis", "comment_text": "oh boy !!", "comment_date": "2020-04-11T07:39:08Z", "likes_count": 0}, {"comment_by": "Dimitrios k", "comment_text": "The diagram on the left which disampigues the relation among the several variables led to the following conclusion:<br>fuck, you are a genious!", "comment_date": "2020-04-09T18:49:22Z", "likes_count": 0}, {"comment_by": "vladi21k", "comment_text": "Great explanation. Please also cover automatic differentiation!", "comment_date": "2020-04-07T18:57:33Z", "likes_count": 0}, {"comment_by": "Austin Ellis-Mohr", "comment_text": "I am continually impressed by your videos. Your essence of linear algebra lit up my imagination which such a geometric interpretation of the subject, and I have used your general methods many times in teaching others (both calculus and linear algebra). This is, in my opinion, the most beautiful series yet as it concisely describes a sort of mystified topic. The analogies you draw and even the notation you use is clear, informative, and friendly. I just want to say thank you and let you know that even outside of people watching your videos, your imagination and passion for teaching affects a lot of people\u2019s learning in a variety of subjects. Again, just thank you and keep being a great teacher for literally millions.", "comment_date": "2020-04-07T07:05:58Z", "likes_count": 0}, {"comment_by": "samuelec", "comment_text": "I keep coming back here every time I have doubts about the mechanics of back propagation. I need to visualise concepts to understand, so for me your video so well done. I can&#39;t say anything else than thank you.", "comment_date": "2020-04-06T17:51:34Z", "likes_count": 0}, {"comment_by": "Ashraf El Droubi", "comment_text": "3blue1brown You are a gift to us who love learning math, Thank you  so much!", "comment_date": "2020-04-06T16:38:28Z", "likes_count": 0}, {"comment_by": "Chlo\u00e9 Tordjeman", "comment_text": "Thank you, it&#39;s very clear.", "comment_date": "2020-04-05T10:35:20Z", "likes_count": 0}, {"comment_by": "Julian Ferry", "comment_text": "For anyone still slightly confused, I highly recommend this chapter of the Stanford CV course: <a href=\"https://cs231n.github.io/optimization-2/\">https://cs231n.github.io/optimization-2/</a><br>This video got me most of the way there, but this Stanford optimisation chapter helped clear things up entirely.", "comment_date": "2020-04-05T08:53:29Z", "likes_count": 0}, {"comment_by": "Nicholas Ziglio", "comment_text": "This explanation is so beautiful! Thank you &lt;3", "comment_date": "2020-04-05T02:04:45Z", "likes_count": 0}, {"comment_by": "Joey Picano", "comment_text": "Uhh Ohh,    This Quarantine has awoken something in me", "comment_date": "2020-04-03T05:11:53Z", "likes_count": 0}, {"comment_by": "Alexander Weaver", "comment_text": "This a a great explanation but I have one question.  If we have multiple neurons in each layer, would dc/db where b is the bias from the hidden to output layers be the sum of (dz/db)*(da/dz)*(dc/da)  for every output z and a because the bias affects every output neuron.", "comment_date": "2020-04-02T14:01:56Z", "likes_count": 1}, {"comment_by": "RandomGuy_PT", "comment_text": "How do you decide how many hidden layers it should have?<br>Also, should i try to make an algorithm with only 2 neurons per layer?", "comment_date": "2020-03-30T18:14:45Z", "likes_count": 0}, {"comment_by": "Emenike Anigbogu", "comment_text": "Best explanation on youtube", "comment_date": "2020-03-26T11:55:58Z", "likes_count": 0}, {"comment_by": "Sky Shoesmith", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m56s\">8:56</a> I don&#39;t understand this bit. That expression in the sum cancels down to dCost/dActivation. Why are we summing that expression when it&#39;s already the thing we&#39;re looking for?", "comment_date": "2020-03-26T09:03:39Z", "likes_count": 1}, {"comment_by": "Eugene Martynov", "comment_text": "Great  job done! You must be devoted to math. I&#39;m a little more than 50 years old, fan of math. And I&#39;m fascinated.", "comment_date": "2020-03-24T09:33:59Z", "likes_count": 0}, {"comment_by": "blackblather", "comment_text": "Thx,  it was very helpful", "comment_date": "2020-03-23T11:56:18Z", "likes_count": 0}, {"comment_by": "TonyGamer", "comment_text": "It&#39;s amazing to me just how many of your courses you have done intersect in this one field, linear algebra, calculus, differential equations, etc. are all present within this one topic.", "comment_date": "2020-03-11T05:59:10Z", "likes_count": 0}, {"comment_by": "1xaps", "comment_text": "Nobel prize in teaching", "comment_date": "2020-03-10T14:50:59Z", "likes_count": 0}, {"comment_by": "DrDress", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m03s\">9:03</a> Isn&#39;t the sum supposed to run to n_L and not n_(L-1)?", "comment_date": "2020-03-09T12:55:12Z", "likes_count": 0}, {"comment_by": "Ikshul Bethur", "comment_text": "What are the more complex and modern forms of machine learning?<br>Could someone point me in the right direction pweese? &lt;333", "comment_date": "2020-03-09T10:28:10Z", "likes_count": 0}, {"comment_by": "Viking Lee", "comment_text": "Chinese subtitle contributors are respectable, while some words translation still causes confusion.", "comment_date": "2020-03-05T10:17:31Z", "likes_count": 0}, {"comment_by": "Prakriti Mishra", "comment_text": "Wakarimashta ^_^ This is my favorite channel. Watching your videos doesn&#39;t even feel like learning. Its like relaxing productively.", "comment_date": "2020-03-05T09:51:31Z", "likes_count": 0}, {"comment_by": "samuelec", "comment_text": "Best explanation ever! thank you", "comment_date": "2020-02-29T22:25:01Z", "likes_count": 0}, {"comment_by": "Andre Korenak", "comment_text": "I know I&#39;m starting a little deep here...but where would I even go to begin to understand the lexicon for these symbols given my lack of mathematics background?", "comment_date": "2020-02-26T02:18:00Z", "likes_count": 0}, {"comment_by": "Lewis Chen", "comment_text": "AI: Let me take a look at the picture. \ud83e\udd14... It looks like garbage. Let me see if this garbage in my mind looks like the picture. \ud83e\uddd0... ... Oh my god! It\u2019s a goddam 3!", "comment_date": "2020-02-25T06:31:07Z", "likes_count": 0}, {"comment_by": "The Terrible Animator", "comment_text": "Small question, if i have a Neural network with one input layer, one mid layer, and one output. In them i have wheights wich i can derive and calculate, but what about bias, does every layer have their own bias, or only one. If it is the first case and every layer has a different bias its fine but what if every layer has the same bias, do i have to derive every layers bias and use that, or perhaps the closest bias to the output?<br><br><br>How do i find a way to close in on the correct bias if there are multiple calculations for bias on every layer?", "comment_date": "2020-02-24T23:36:35Z", "likes_count": 0}, {"comment_by": "sagar desai", "comment_text": "Great explanation. Thank you for the mathematical version of it. :)", "comment_date": "2020-02-23T14:42:40Z", "likes_count": 0}, {"comment_by": "SHArK K", "comment_text": "at last  i understood", "comment_date": "2020-02-22T19:04:16Z", "likes_count": 0}, {"comment_by": "Carlos Cerritos Lira", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m29s\">9:29</a> I am pretty sure the formula for dC/da_j^(l) should not include the k index. Because we should be eable to find it without knowing k.", "comment_date": "2020-02-19T03:45:47Z", "likes_count": 0}, {"comment_by": "Watts Field", "comment_text": "You&#39;re videos are of mad quality! I&#39;m upgrading my debit card to credit so I can become a Patreon, cuz damn.<br>I think we often take amazing Youtube videos for granted. Creators like 3b1b give us so much. It&#39;s only right to give back a little sometimes.", "comment_date": "2020-02-18T17:45:03Z", "likes_count": 4}, {"comment_by": "Royce Melborn", "comment_text": "Is an episode 5 coming?", "comment_date": "2020-02-17T20:26:09Z", "likes_count": 0}, {"comment_by": "vijay patneedi", "comment_text": "Now, pat yourself on the back...!", "comment_date": "2020-02-16T16:42:43Z", "likes_count": 0}, {"comment_by": "Avi Mehenwal", "comment_text": "I still do not understand neural network that well but I must say it\u2019s the best resource on the topic.<br>Feynman technique in action. Thankyou for helping us learn.", "comment_date": "2020-02-16T15:09:18Z", "likes_count": 0}, {"comment_by": "mjlr1000", "comment_text": "You make this look incredibly simple. Congratulations on the series. Truly outstanding.", "comment_date": "2020-02-16T02:52:30Z", "likes_count": 0}, {"comment_by": "Shashwat Rathod", "comment_text": "Awesome series! Thank you!", "comment_date": "2020-02-15T14:09:54Z", "likes_count": 0}, {"comment_by": "Harsha P", "comment_text": "This actually made it easy to learn about back propagation rather than racking my mind on those equations \ud83d\ude02", "comment_date": "2020-02-13T05:43:03Z", "likes_count": 0}, {"comment_by": "AlienRenders", "comment_text": "For backpropagation, since the derivative of sigmoid uses the sigmoid, you don&#39;t need to keep the pre-sigmoid result, right? You can just do (s * (1-s)). But for anything else (like RELU), you need to keep the temporary Z values before relu is applied, correct? Because you need to apply the relu derivative on those Z values, right?", "comment_date": "2020-02-11T04:20:56Z", "likes_count": 0}, {"comment_by": "Yoshi", "comment_text": "Why can&#39;t i like this video more than once???", "comment_date": "2020-02-10T06:17:48Z", "likes_count": 1}, {"comment_by": "Marcel K", "comment_text": "Is there a reason why you denoted the cost function C0 with a zero? I think I understood everything, but C0 confuses me a bit :D", "comment_date": "2020-02-06T16:26:28Z", "likes_count": 0}, {"comment_by": "NovaHorizon", "comment_text": "Gah I&#39;m so confused. It all lies in the fact that I only have basic algebra for math knowledge, so this calculus is a lot to learn :\\", "comment_date": "2020-02-04T23:02:46Z", "likes_count": 0}, {"comment_by": "rigel eisenheim", "comment_text": "best channel ever", "comment_date": "2020-02-02T19:52:23Z", "likes_count": 0}, {"comment_by": "Paris Mollo", "comment_text": "This was just so..... beautiful.", "comment_date": "2020-02-01T16:22:14Z", "likes_count": 0}, {"comment_by": "Leelahs Playhouse", "comment_text": "Ohm my gosh you should try this application!  Browse for: androidcircuitsolver/app.html", "comment_date": "2020-01-26T19:35:11Z", "likes_count": 0}, {"comment_by": "Sam Ozturk", "comment_text": "Hey, first of all your videos are great. It makes learning process extremely easy and fast. <br><br><br>Quick question: I was wondering how you make those animations? I need to make presentations constantly at work, so It would be easier if I can explain things as you do to a non-techinal person(in this case my manager).", "comment_date": "2020-01-26T11:13:27Z", "likes_count": 2}, {"comment_by": "A person ", "comment_text": "OMG, this is such a good and simple explanation, I was so lost before I saw it,<br>I have only one question though:<br>From what I&#39;ve realized, using the chain rule you also use the cost function for the neuron&#39;s value and the Y you wanted it to be, it works perfectly when you perform it on the last layer,  but then when you go back on other layers it is a little different... and the formula just doesn&#39;t work for me on the other layers properly, can someone please explain to me what changes there?", "comment_date": "2020-01-25T01:44:03Z", "likes_count": 0}, {"comment_by": "Meganton", "comment_text": "Oh man, that very last formula at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m35s\">9:35</a> really confused me for about 2 hours, until i realized, that the OR really means OR, so that i can ignore the weird part above it and just use the dC/dw formula with (a-y)...", "comment_date": "2020-01-24T01:14:25Z", "likes_count": 0}, {"comment_by": "Shyam Sunder", "comment_text": "if I get anywhere in this field, you&#39;ll have the credit for actually starting me up...", "comment_date": "2020-01-19T17:57:39Z", "likes_count": 2}, {"comment_by": "DeliciousNoodles", "comment_text": "Wow this video made all my doubts from the first 3 videos clear! Longest 10 min video I&#39;ve ever watched!", "comment_date": "2020-01-16T11:46:48Z", "likes_count": 1}, {"comment_by": "Anush kumar", "comment_text": "This was the best explanations of backpropagation I have ever seen. Really enjoyed the video :)", "comment_date": "2020-01-12T13:50:30Z", "likes_count": 0}, {"comment_by": "Martin Martin", "comment_text": "Great explanation! I like how you really focus on giving why the formulas actually help and to make this stuff as intuitive as possible. This is the best inteoduction on backpropagation that I have seen so far!", "comment_date": "2020-01-11T15:05:20Z", "likes_count": 0}, {"comment_by": "LimitedWard", "comment_text": "Absolutely brilliant explanation! I took a course on deep learning in college, but ended up auditing it in the end because I couldn&#39;t grasp the concepts well enough to pass the tests. You just took the entire first unit of the course, which took several weeks, and condensed it into 4 easily digestible videos that anyone can understand!", "comment_date": "2020-01-10T06:00:44Z", "likes_count": 18}, {"comment_by": "SHArK K", "comment_text": "Thanks lord for the info<br>\u0905\u0926\u094d\u0935\u093f\u0924\u0940\u092f!!!!", "comment_date": "2020-01-07T11:00:08Z", "likes_count": 3}, {"comment_by": "Mohammed Ansari", "comment_text": "it is still best channel in 2020", "comment_date": "2020-01-06T16:37:45Z", "likes_count": 0}, {"comment_by": "N8 Programs", "comment_text": "How is the error for the neurons in the hidden layers calculated? I know how to solve for the gradient, but I&#39;m not sure how to compute the &quot;cost&quot; for each of the neurons in the hidden layers. Does anyone know?", "comment_date": "2020-01-04T23:25:25Z", "likes_count": 1}, {"comment_by": "Andrew Kinsella", "comment_text": "Thank you \ud83d\ude0a", "comment_date": "2020-01-01T21:01:08Z", "likes_count": 0}, {"comment_by": "Dhruv", "comment_text": "I&#39;ve worked with AI for 2 years now. I have never seen anyone explain this as succinctly and aptly as you have. This video is legitimate gold. Going to show this to anyone who needs an explanation in future!", "comment_date": "2019-12-31T13:59:04Z", "likes_count": 7}, {"comment_by": "Mike_o", "comment_text": "i watched this video like 10 times at least and, Finally! finally! i was able to build my own neural network with back prop, in c#<br>and it actually works!<br><br>Btw, i found out that if my layers have a lot of neurons the System often over shoting, and deviding the Cost for each neuron by how many neurons,<br>are in that layer help to fix that issue, i mean i can have the same training value for any type of neural network and it wont over shoot at all.<br>but in the video you just said to add the costs for each neuron together rather then getting the average cost for each neuron.... <br>hmm...", "comment_date": "2019-12-29T13:20:25Z", "likes_count": 5}, {"comment_by": "Manoj Pawar SJ", "comment_text": "When will you make video about CNN RNN", "comment_date": "2019-12-28T17:58:49Z", "likes_count": 0}, {"comment_by": "A Random IITIAN", "comment_text": "What did it cost ?<br><br><br><br><br>just a function.", "comment_date": "2019-12-27T16:20:12Z", "likes_count": 1}, {"comment_by": "aulas4you  Tutorial ", "comment_text": "great content! Very good class!", "comment_date": "2019-12-27T09:19:33Z", "likes_count": 0}, {"comment_by": "Hans Dieter", "comment_text": "I would definitely hire you as a teacher. The content is easy to grasp, although I am not a native english speeker.", "comment_date": "2019-12-21T12:25:05Z", "likes_count": 0}, {"comment_by": "Zauber Flecks", "comment_text": "You are the best lecturer that ever existed", "comment_date": "2019-12-19T13:50:14Z", "likes_count": 0}, {"comment_by": "Archit Jain", "comment_text": "Solve a numberical", "comment_date": "2019-12-14T19:59:14Z", "likes_count": 0}, {"comment_by": "Alex Jones", "comment_text": "Beautiful! Thank you so much", "comment_date": "2019-12-12T22:42:05Z", "likes_count": 0}, {"comment_by": "flashgordon", "comment_text": "looks like the A.I. researchers are wide of the mark on how intelligence works.", "comment_date": "2019-12-09T04:38:15Z", "likes_count": 0}, {"comment_by": "Muhammed Aydo\u011fan", "comment_text": "thatssss pretty cool and helped me a lot.  Thankssss. (Im parsssselmouth)", "comment_date": "2019-12-08T16:56:38Z", "likes_count": 0}, {"comment_by": "Kreavita", "comment_text": "i made a neural network of this kind in c++ from scratch and let it train on the mnist dataset with mini batches (100 samples) but it is training for 3 hours already and didnt even made it to the half of the dataset. is this normal or is there likely an error in the code? im running it on one thread and my cpu isnt the best (haswell i5)", "comment_date": "2019-12-08T15:57:45Z", "likes_count": 0}, {"comment_by": "Salamanka", "comment_text": "I watched this video again, do not agree with you, I think the formula of <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=03m45s\">03:45</a> was meaningless, so the whole video completely meaningless after that.", "comment_date": "2019-12-07T07:58:36Z", "likes_count": 0}, {"comment_by": "Salamanka", "comment_text": "Shit, this episode screws me", "comment_date": "2019-12-07T07:11:31Z", "likes_count": 0}, {"comment_by": "Kevin Oduor", "comment_text": "backpropagation is hard for me to understand.why not just discard the &quot;wrong &quot; neurons.", "comment_date": "2019-12-04T20:25:57Z", "likes_count": 0}, {"comment_by": "William Romero-\u00c1uila", "comment_text": "Wombat-_l", "comment_date": "2019-12-03T00:38:28Z", "likes_count": 0}, {"comment_by": "qwert 9203", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m28s\">9:28</a> sums up the whole thing", "comment_date": "2019-12-02T14:25:53Z", "likes_count": 13}, {"comment_by": "Pedro Rodrigues", "comment_text": "Thank you!", "comment_date": "2019-12-02T00:34:30Z", "likes_count": 0}, {"comment_by": "Aditya Mishra", "comment_text": "Clicky stuff!!!", "comment_date": "2019-12-01T09:13:37Z", "likes_count": 0}, {"comment_by": "Ben Yonas", "comment_text": "My only question is what do you do with the partial derivatives. Where do you go from there?", "comment_date": "2019-12-01T06:15:54Z", "likes_count": 0}, {"comment_by": "Tinil0", "comment_text": "That feeling when you never took past college algebra and yet spend hours watching 3Blue1Brown &lt;.&lt;", "comment_date": "2019-11-28T06:00:18Z", "likes_count": 0}, {"comment_by": "Joseph Song", "comment_text": "best explanation ever!", "comment_date": "2019-11-26T00:06:46Z", "likes_count": 0}, {"comment_by": "Thush Ish", "comment_text": "Sorry just 1 quick question: <br><br><br>If many instances of the training data is used to calculate the cost function, how do we know the value of the activation of the previous layer?<br><br><br>I&#39;m approaching this as a way to change the weight matrix from the cost and the output, so if that it incorrect, please let me know.<br>Thank you for your support!", "comment_date": "2019-11-25T01:19:11Z", "likes_count": 0}, {"comment_by": "Open Ocean", "comment_text": "This saved my life and made so much sense! Why can&#39;t more teachers be ACTUAL teachers like you instead of just assuming you know everything?!", "comment_date": "2019-11-25T01:03:02Z", "likes_count": 0}, {"comment_by": "Qutub V", "comment_text": "This is unbelievably good! Thank you so much.", "comment_date": "2019-11-24T00:06:49Z", "likes_count": 0}, {"comment_by": "Aron Highgrove", "comment_text": "Too many colors, too much movements. You need to cut it down to the essential ideas, and leave further &quot;enrichements&quot; to other videos or additional notes.", "comment_date": "2019-11-19T01:09:01Z", "likes_count": 0}, {"comment_by": "CiniCraft", "comment_text": "<b>MY_BRAIN.EXE HAS STOPPED RESPONDING, CHECK THE ERROR LOG FOR MORE DETAILS</b>", "comment_date": "2019-11-18T23:22:35Z", "likes_count": 0}, {"comment_by": "Logic Facts", "comment_text": "born to be useless moron explains simple things as complicated and messy as possible. this is rule valid for internet as general", "comment_date": "2019-11-18T21:39:04Z", "likes_count": 1}, {"comment_by": "Justin Reusnow", "comment_text": "Fantastic series. Please consider making one on convolutional neural networks as well! Your style of delivery would make it more perceptible than most sources.", "comment_date": "2019-11-17T02:42:09Z", "likes_count": 0}, {"comment_by": "Sriprad Potukuchi", "comment_text": "I finally worked my way through the series! It really helps to understand the underlying mechanics of neural nets rather than just copy-pasting formulas into your code. I am making my own neural net with this knowledge. Wish me luck!<br>Thank you, Grant, for these amazing videos.", "comment_date": "2019-11-15T13:22:07Z", "likes_count": 0}, {"comment_by": "agpxnet", "comment_text": "Great explanation, compliments.", "comment_date": "2019-11-14T19:25:16Z", "likes_count": 0}, {"comment_by": "Alex Talbot", "comment_text": "Completely understood the logic, but today I find out just how bad my maths skills are.", "comment_date": "2019-11-14T16:28:35Z", "likes_count": 0}, {"comment_by": "Omar CHIDA", "comment_text": "My brain neurons stopped firing after watching this series xD.<br>Just a joke you did great job explaining it and I understand it very well. Thanks for making these videos.", "comment_date": "2019-11-11T02:40:55Z", "likes_count": 0}, {"comment_by": "Krebzonide", "comment_text": "I just learned chain rule in high school last week and I&#39;m glad it has a real life application.", "comment_date": "2019-11-07T23:50:41Z", "likes_count": 1}, {"comment_by": "Jar Juice Machine", "comment_text": "Number of times I have watched this video before understanding<br>\u2193", "comment_date": "2019-11-06T15:55:55Z", "likes_count": 4}, {"comment_by": "Omar Alsabbagh", "comment_text": "Amazing Explanation.", "comment_date": "2019-11-05T04:52:53Z", "likes_count": 0}, {"comment_by": "Giorgio Acquati", "comment_text": "This is easily my favorite youtube channel!  Why not continue the series on something like convolutional neural  networks?", "comment_date": "2019-11-02T12:34:22Z", "likes_count": 1}, {"comment_by": "Analytical1", "comment_text": "Can someone explain to me how to continue updating the weights? To update the weights before the output layer you do  ErrorFunc&#39; * ActivationFunc&#39; * a(L-1) = nudges to W(L)<br><br>From the video:<br>\u2202C/\u2202a(L) * \u2202a(L)/\u2202z(L) * \u2202z(L)/\u2202w(L)<br><br><br>How do you extend the chain? Do you multiply that chain with W(L-1), OR W(L-1) and a(L-2) as well since the previous update ended in multiplying\u00a0by a(L-1).<br><br>So:<br>ErrorFunc&#39; * ActivationFunc&#39; * a(L-1) * W(L-1) * a(L-2) = nudges to W(L-1)<br>In notation:<br>\u2202C/\u2202a(L) * \u2202a(L)/\u2202z(L) * \u2202z(L)/\u2202w(L) *\u00a0 \u2202a(L-2)/\u2202z(L-1) * \u2202z/\u2202w(L-1)", "comment_date": "2019-10-31T15:45:03Z", "likes_count": 1}, {"comment_by": "UUMatter_010", "comment_text": "Im currently in 11. Grade learning Calculus and this Video gave me an awesome &quot;WTF it makes sense&quot; Moments, had to think about it for some time tho. Awesome Video!", "comment_date": "2019-10-23T17:59:51Z", "likes_count": 1}, {"comment_by": "kim khanh Le", "comment_text": "Woww, now I understand", "comment_date": "2019-10-19T10:59:08Z", "likes_count": 1}, {"comment_by": "Karthik", "comment_text": "Awesome !, This channel is unbelivably good. Thanks a lot man", "comment_date": "2019-10-14T00:54:43Z", "likes_count": 0}, {"comment_by": "Keith Wallace", "comment_text": "this was really good", "comment_date": "2019-10-12T09:31:14Z", "likes_count": 0}, {"comment_by": "JAM JAM", "comment_text": "I love this channel. Thank you", "comment_date": "2019-10-12T03:29:14Z", "likes_count": 0}, {"comment_by": "Chris Hansen", "comment_text": "Commenting to help you with the youtube algorithm because these videos are great", "comment_date": "2019-10-06T21:04:55Z", "likes_count": 0}, {"comment_by": "Ophir Gal", "comment_text": "I would be super interested in a video of you explaining how you make your videos :)", "comment_date": "2019-10-05T19:19:06Z", "likes_count": 0}, {"comment_by": "Gell\u00e9rt T\u00f3th", "comment_text": "I understand what happens on the first level, but then what? We have the derivative for the a(l-1)-s and we recalculate another cost func, but now with (dC/da(l-1))^2 instead of (a(l)-y)^2?", "comment_date": "2019-10-02T04:26:03Z", "likes_count": 0}, {"comment_by": "Pranav Chaturvedi", "comment_text": "Congratulations fellow learner on making this far. You are/are going to be a good Machine Learning Engineer.(I am just telling that myself.)", "comment_date": "2019-10-01T18:21:20Z", "likes_count": 0}, {"comment_by": "Ahmed Elsayes", "comment_text": "No words can express my admiration with your work", "comment_date": "2019-10-01T06:56:13Z", "likes_count": 0}, {"comment_by": "Erika Gutierrez", "comment_text": "best series!! Thanks for this material!", "comment_date": "2019-09-30T01:51:12Z", "likes_count": 0}, {"comment_by": "Shis", "comment_text": "Need more videos on AI \ud83d\ude03", "comment_date": "2019-09-27T18:57:53Z", "likes_count": 1}, {"comment_by": "Arnold Marsh", "comment_text": "Help a lot when I doing my ML homework. thanks &lt;3", "comment_date": "2019-09-27T14:19:20Z", "likes_count": 0}, {"comment_by": "Marvin J", "comment_text": "brilliantly clear! love it! It really helps!", "comment_date": "2019-09-27T13:38:08Z", "likes_count": 0}, {"comment_by": "VinTechTalk", "comment_text": "going through your video is like meditation... a blissful experience.. thank you so much!", "comment_date": "2019-09-26T06:13:58Z", "likes_count": 0}, {"comment_by": "Noah Kupinsky", "comment_text": "Hey for all of you getting discouraged because you don\u2019t understand this - that was me last year. I went and taught myself derivatives and came back to try again and suddenly I understand everything. It\u2019s such an amazing feeling to see that kind of work pay off. Don\u2019t give up kiddos", "comment_date": "2019-09-21T06:50:51Z", "likes_count": 462}, {"comment_by": "Nepali", "comment_text": "Just about an hour ago, I was totally alien to AI guys especially when they said &quot;machine learns&quot;. Hats off to your selflessness making even a medico able to understand how a machine actually learns, relatively easier when I compared it to natural neural networks in our nervous system. Mark my words, with raging AI in healthcare, your videos will be a connecting link for someone away from AI, to know about how AI works.", "comment_date": "2019-09-17T12:39:45Z", "likes_count": 2}, {"comment_by": "basir sedighi", "comment_text": "i watched, i learned , i became a patreon  <b>*</b> meme of Fry saying &quot;have my money &quot;***", "comment_date": "2019-09-15T17:27:46Z", "likes_count": 1}, {"comment_by": "Virajkumar Patel", "comment_text": "Thank you for such a great video!", "comment_date": "2019-09-14T20:47:06Z", "likes_count": 0}, {"comment_by": "Danil Kutny", "comment_text": "I have been trying to understand, how to host a &#39;hello_world&#39; python server for about a week and I still don&#39;t understand. I has watched your 4 videos a few times and make my own neural network that can understand the world. Man, I wish people who teaches were at least 10% as good in teaching as you", "comment_date": "2019-09-13T21:26:51Z", "likes_count": 0}, {"comment_by": "David Okao", "comment_text": "This is so beautiful", "comment_date": "2019-09-13T02:37:54Z", "likes_count": 0}, {"comment_by": "Louis Emery", "comment_text": "I wish I saw this video much earlier since I&#39;m good at chain rule and also optimization problems. I attended a lecture on neural networks in 1984. I didn&#39;t really understand how one can determine weights without fitting. Looked to me like back propagation was a swindle until I saw this video. Now I can show my friends using a couple of lines on the whiteboard.", "comment_date": "2019-09-12T06:31:56Z", "likes_count": 0}, {"comment_by": "Mrrajender2801", "comment_text": "Many guys claim to know. Some guys actually know. But only one guy actually knows and can explain to his grandma as well with very beautiful animations. You are that ONE !!!", "comment_date": "2019-09-11T16:01:25Z", "likes_count": 33}, {"comment_by": "aam dae", "comment_text": "Hope to see soon one more episode on the same thing in matrix notation which would make it more sensible to relate to actual implementation", "comment_date": "2019-09-05T23:01:29Z", "likes_count": 0}, {"comment_by": "Gaurang Mohta", "comment_text": "This is the best explaination to Chain Rule I&#39;ve ever heard!!", "comment_date": "2019-09-05T17:44:23Z", "likes_count": 1}, {"comment_by": "Juuso Korhonen", "comment_text": "I have a problem with my neural net, which I build from scratch following these videos. Most of the time my net gives pretty sure answers like [0.999987, 0.000323] (until now I&#39;ve only tested with self-created data like input [1,2,3,4] should give [1,0] as an output), but sometimes with different initializations of weights and biases the training ends with feedforward now giving some strange answers like [0.004, 0.000003]. There&#39;s still clear distinction with the probability of the right answer and the wrong answer, but it is nowhere near the optimal sure answer [1,0]. What is going on here? Is it that my gradient descent finds an local minimum which gives as an output the said [0.004, 0.000003] and gets stuck there? Is this a common problem with neural nets? Should I try to find initial configurations for which the local minimum gives the least error?", "comment_date": "2019-09-04T09:48:26Z", "likes_count": 0}, {"comment_by": "Gustavo Rocha", "comment_text": "You are genius.", "comment_date": "2019-08-30T21:19:44Z", "likes_count": 0}, {"comment_by": "Zachary Thatcher", "comment_text": "That unexplained little formula addition at the end <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m30s\">9:30</a> showing what the partial derivative of the cost function with respect to the current node and layer when the current layer is not the output later really messed with me. In typical notation, that&#39;s lower case Delta. Correct?", "comment_date": "2019-08-27T19:28:46Z", "likes_count": 0}, {"comment_by": "Jonathan Kane", "comment_text": "Are there anymore coming in the series? I found this very helpful.", "comment_date": "2019-08-22T17:09:53Z", "likes_count": 0}, {"comment_by": "Sanjay Thorat", "comment_text": "@3Blue1Brown, @<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m25s\">9:25</a>, I think wl+1 should be just wl. Please confirm", "comment_date": "2019-08-22T07:26:01Z", "likes_count": 1}, {"comment_by": "Sreesha Srinivasan Kuruvadi", "comment_text": "This is epic !", "comment_date": "2019-08-18T09:20:34Z", "likes_count": 0}, {"comment_by": "Christopher Herrera", "comment_text": "Wow, this took a long time to get my head around fully, but I was finally able to understand it enough to implement my own version of backpropagation from scratch thanks to this video! Neural networks are something I&#39;ve wanted to get into for a while and I&#39;m really grateful for these wonderful in-depth explanations!", "comment_date": "2019-08-13T09:38:38Z", "likes_count": 1}, {"comment_by": "Jeffrey Hubbard", "comment_text": "So with stochastic gradient descent would you only change some of the weight values each iteration in the training phase.", "comment_date": "2019-08-12T03:46:10Z", "likes_count": 0}, {"comment_by": "Achraf Saad", "comment_text": "At like <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m25s\">8:25</a>, why is C0 being the sum going from j = 0 to nL-1 being the number of neurons of the last hidden layer, and then j being used in the output&#39;s subscript (Aj)? is that a mistake or am I missing something", "comment_date": "2019-08-12T02:47:25Z", "likes_count": 0}, {"comment_by": "Ricardo Solano", "comment_text": "Amazing stuff. Just... GREAT. I Cannot thank you enough!", "comment_date": "2019-08-11T22:05:25Z", "likes_count": 0}, {"comment_by": "Alex Berk", "comment_text": "Maybe one day i will be smart enough to understand it all", "comment_date": "2019-08-10T23:02:20Z", "likes_count": 0}, {"comment_by": "Dream Worker 65524", "comment_text": "You easily won one more subscriber.", "comment_date": "2019-08-08T10:16:53Z", "likes_count": 0}, {"comment_by": "Grunion Shaftoe", "comment_text": "This video doesn&#39;t actually suggest how one chooses a value to add to the weights, and the propagation seems to move forward to the first layer only - how are alterations added to the second and third layers ?", "comment_date": "2019-08-07T10:55:00Z", "likes_count": 0}, {"comment_by": "Sergey Piterman", "comment_text": "This is SO MUCH computation. But amazing explanation, can&#39;t wait to implement.", "comment_date": "2019-08-06T22:01:47Z", "likes_count": 0}, {"comment_by": "\u984f\u632f\u5b87", "comment_text": "\u975e\u5e38\u512a\u79c0\u7684\u8b1b\u89e3", "comment_date": "2019-08-05T02:33:28Z", "likes_count": 0}, {"comment_by": "Jagdeepak Rawat", "comment_text": "Hey , can you make a video for rnn? It would be of great help.", "comment_date": "2019-08-04T17:47:07Z", "likes_count": 0}, {"comment_by": "Luis Ka", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m10s\">4:10</a> haha i laughed for no reason at the thoo", "comment_date": "2019-08-03T13:17:39Z", "likes_count": 0}, {"comment_by": "TN Inventor", "comment_text": "now i know the basic i will let it sink then watch how to code it using python", "comment_date": "2019-08-02T17:56:20Z", "likes_count": 0}, {"comment_by": "Muhammad Sarim Mehdi", "comment_text": "you are not a good teacher. You failed to extrapolate from the simple example to the general one. You should give an example with a 2 layer network with the cost function and activation function and then show, explicitly by writing down, how the derivatives go all the way back.", "comment_date": "2019-08-01T20:31:14Z", "likes_count": 0}, {"comment_by": "duncpol", "comment_text": "Is there any learning material available on the internet for a simple neural net which goes step by step, computing actual values (cost functions, derivations) for all weights, biases and iterations? That would be very practical.", "comment_date": "2019-07-31T21:55:26Z", "likes_count": 0}, {"comment_by": "Samia Zaman", "comment_text": "Thank you. Love it.", "comment_date": "2019-07-29T02:24:14Z", "likes_count": 0}, {"comment_by": "Billy Kotsos", "comment_text": "This video is art", "comment_date": "2019-07-28T22:48:12Z", "likes_count": 0}, {"comment_by": "Access2Music", "comment_text": "Thank you so so so much for making this series. Within an hour, I feel that I have learned a good deal about Neural Networks. You are amazing!", "comment_date": "2019-07-28T11:26:09Z", "likes_count": 0}, {"comment_by": "Michael Corley", "comment_text": "It&#39;s great that you started it backwards but when I started to program it I realized something. What is a^(L-1) when computing the first layer? Is it simply the value from the inputs?<br><br><br>z^((L))=w^((L)) a^((L-1))+b^((L))\r<br>a^((L))=f(z^L)", "comment_date": "2019-07-22T19:59:48Z", "likes_count": 0}, {"comment_by": "brunon554", "comment_text": "Wow, so clear, thanks :D<br>I&#39;m not sure why the derivative of z(L) is a(L-1) though :/<br>Could anyone explain me? :)", "comment_date": "2019-07-22T11:54:35Z", "likes_count": 0}, {"comment_by": "Hasindu Piyumantha", "comment_text": "Great video series... Thanks a lot for explaining something very complex so nicely...", "comment_date": "2019-07-18T16:05:00Z", "likes_count": 0}, {"comment_by": "Subramanya M", "comment_text": "Thanks for the wonderful lectures. Expecting more lectures in this field..", "comment_date": "2019-07-15T08:25:05Z", "likes_count": 1}, {"comment_by": "mukund holo", "comment_text": "this is math at the best and art too at the highest. the grace of the animation, the subtle music, the perfectly paced narration and the wonderful colour scheme! math and art or let&#39;s say math is art!", "comment_date": "2019-07-10T06:14:04Z", "likes_count": 0}, {"comment_by": "what? no", "comment_text": "Without this I never would&#39;ve been able to make my first neural network, even though all it did was learn how to respond to Rock Paper Scissors when it already knows which one you&#39;re going to put (basically overfitting is the goal)", "comment_date": "2019-07-09T07:09:45Z", "likes_count": 0}, {"comment_by": "arli", "comment_text": "is the cost function here the loss function or the averaged loss function?", "comment_date": "2019-07-07T17:30:01Z", "likes_count": 0}, {"comment_by": "Prashamsh Takkalapally", "comment_text": "One of the best lectures I have ever heard. Great explanation of NN, cost functions, activation functions etc. Now I understand NN far far better...(P.S. I saw previous videos Part 1, 2,3 as well)", "comment_date": "2019-07-07T04:30:10Z", "likes_count": 6}, {"comment_by": "Guilherme Viveiros", "comment_text": "This 4-video series was very helpful and your explanations are awesome! <br>Thank you!", "comment_date": "2019-07-07T01:28:01Z", "likes_count": 0}, {"comment_by": "Samarth Singla", "comment_text": "The amount of help you are providing is nothing short of amazing.", "comment_date": "2019-07-05T01:03:39Z", "likes_count": 8}, {"comment_by": "Ivens Lima", "comment_text": "Simply fantastic!!!!", "comment_date": "2019-07-01T20:25:50Z", "likes_count": 0}, {"comment_by": "Srijan", "comment_text": "So, I get that the desired output(y) for the layer of neurons in the output layer can either be a 0 or a 1. But, what is the desired output(y) when calculating the gradients for the second to last layer of neurons? What activation do we actually desire for the layer behind the activation layer?", "comment_date": "2019-07-01T03:35:07Z", "likes_count": 2}, {"comment_by": "Saif Ul Islam", "comment_text": "It has taken me about 3-4 days worth time to understand all of these 4 lectures, lectures which are in total, no longer than 1 hour and 30 minutes. <br><br><br><br>And I feel proud.", "comment_date": "2019-06-28T17:13:35Z", "likes_count": 217}, {"comment_by": "Dinoel D", "comment_text": "Hello. I have a question. I watched a lot of videos and I can&#39;t figure out a thing about neural networks.<br><br><br>Is the BIAS common for all the neurons of a layer or does every neuron have it&#39;s own bias?<br><br><br>In some schematics the bias is like a neuron with activation (1)  and it has different weights when connecting to every neuron of the layer. In other schematics, the bias has one value as activation  and there are no weights(so the bias is equal for all the neurons of that layer)<br>Thank you very much!", "comment_date": "2019-06-27T11:39:10Z", "likes_count": 0}, {"comment_by": "Ashish` Tiwari", "comment_text": "Holy mother of God!!!<br>How did I intuitively understand such complex thing!!!!??<br>Grant,Sir!!! You are god to me", "comment_date": "2019-06-26T18:28:53Z", "likes_count": 0}, {"comment_by": "Jules Sci", "comment_text": "That gradient, nudge and the little number line is just genius.", "comment_date": "2019-06-24T04:52:45Z", "likes_count": 0}, {"comment_by": "Mr. rubi1000", "comment_text": "So good! Love your video!", "comment_date": "2019-06-22T11:41:03Z", "likes_count": 0}, {"comment_by": "Gustavo Oliveira", "comment_text": "This is guys is amazing! Can&#39;t stop watching! Thank you again!", "comment_date": "2019-06-20T20:08:53Z", "likes_count": 1}, {"comment_by": "Vikash Kumar Chaurasia", "comment_text": "Great tutorial thanks for this series", "comment_date": "2019-06-20T15:23:59Z", "likes_count": 0}, {"comment_by": "Theodore Chandra", "comment_text": "I should put your name in the acknowledgement part of my thesis", "comment_date": "2019-06-20T09:24:29Z", "likes_count": 2}, {"comment_by": "David whyte", "comment_text": "I there a paper I can use?", "comment_date": "2019-06-18T10:42:08Z", "likes_count": 0}, {"comment_by": "Roger Van Brunt", "comment_text": "nicely put I now have a very good feeling how nets are trained", "comment_date": "2019-06-17T22:11:53Z", "likes_count": 0}, {"comment_by": "Ahmad Alghooneh", "comment_text": "That one of the worst ways to learn Backprop, the idea comes from chain Rule and the rest is linear algebra. better with the book in description.", "comment_date": "2019-06-17T18:07:13Z", "likes_count": 0}, {"comment_by": "Ananya Roy", "comment_text": "Please make videos on deep reinforcement learning, Q-learning, DQN. It&#39;s the only channel that explains all the maths with the greatest visuals behind BP and basic DL.", "comment_date": "2019-06-17T03:49:34Z", "likes_count": 0}, {"comment_by": "Bleard Loshaj", "comment_text": "I admire you!<br>It would help a lot, if you tell us how you manage to gain such an understanding from books. By that I mean, what does it take to be able to make those videos? Does everything you know come from books? Do you have a good professor? Do you experiment with visualization tools to achieve the geometrical interpretation? Or are you only gifted to gain such kind of understanding?", "comment_date": "2019-06-16T16:36:15Z", "likes_count": 0}, {"comment_by": "Gibe Mass", "comment_text": "You are some sort of sorcerer. The chain rule was finally elucidated for me in about 20 seconds. <br>So concise and intuitive indeed.<br>That was one thing, for whatever reason, I couldn&#39;t get my head around as a younger student. <br>Big props man. Love this channel.", "comment_date": "2019-06-16T00:30:36Z", "likes_count": 0}, {"comment_by": "peppercakeman", "comment_text": "One hundred percent subscribed. Amazing stuff, thanks for putting in the effort.", "comment_date": "2019-06-13T04:31:41Z", "likes_count": 0}, {"comment_by": "maxim25o2", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m44s\">4:44</a> I have stil problem to calculate weight.  I don&#39;t understand how calculate all delta for sum of &quot;z(L)&quot; Please help.", "comment_date": "2019-06-11T08:13:52Z", "likes_count": 0}, {"comment_by": "Prachi Sharma", "comment_text": "You are the best to learn from with no weights and bias attached!!!", "comment_date": "2019-06-10T08:06:09Z", "likes_count": 1}, {"comment_by": "Lucas Fernandez Piana", "comment_text": "Brillant, thanks for this work!", "comment_date": "2019-06-09T16:17:27Z", "likes_count": 0}, {"comment_by": "PowerOfTheMirror", "comment_text": "How do you determine the desired value of a neuron in the hidden layer?", "comment_date": "2019-06-09T08:51:12Z", "likes_count": 5}, {"comment_by": "Meenakshi CS", "comment_text": "Please, may I have some more?<br>Deep Learning? Convolutional Neural Networks?", "comment_date": "2019-06-09T07:51:43Z", "likes_count": 0}, {"comment_by": "Yann Chevrier-Foundy", "comment_text": "What&#39;s up with the number of layers?", "comment_date": "2019-06-09T04:46:29Z", "likes_count": 0}, {"comment_by": "jtgdyt2", "comment_text": "Nice, but none of it makes it any more clear how I would write the code to do this. This did not even cover the adjustment of weights.", "comment_date": "2019-05-31T08:04:27Z", "likes_count": 0}, {"comment_by": "Vruksha Nikam", "comment_text": "This all aligns so well with Andrew Ng&#39;s ML course!", "comment_date": "2019-05-30T06:42:44Z", "likes_count": 0}, {"comment_by": "pankaj joshi", "comment_text": "I&#39;d trained dozens of models in CNN. but now I&#39;ve understand what I did there. Awesome content with such a beautiful soundtrack. \ud83d\ude0a", "comment_date": "2019-05-29T10:43:40Z", "likes_count": 0}, {"comment_by": "Peter Janosky", "comment_text": "What an amazing video series. Everything was so well explained that I, having just learned this math, was able to follow along. I wish I could do more to support this channel.", "comment_date": "2019-05-28T05:25:55Z", "likes_count": 0}, {"comment_by": "J\u00fcrgen Ruut", "comment_text": "I attempted to create my own neural network from scratch based on this video series... However this last video still jumps over a few bits which likely proved to be my downfall... since the neural network really just does not work unfortunately.", "comment_date": "2019-05-27T18:22:13Z", "likes_count": 0}, {"comment_by": "Richard", "comment_text": "Thank you very much for your amazing explanations. I&#39;d been buckling down a lot for understanding back propagation. While all the explanations I found failed to make me clearly understand the topic, your explanation just worked wonders! Thanks again for so nice videos!", "comment_date": "2019-05-27T13:48:53Z", "likes_count": 1}, {"comment_by": "SetTheCurve", "comment_text": "I don&#39;t recall hearing this video say anything about what value we should modify any one weight and bias by. At least one single example using values would have been nice.", "comment_date": "2019-05-27T01:02:50Z", "likes_count": 0}, {"comment_by": "Prasad Madhale", "comment_text": "That Backup diagram does help in calculating backprop", "comment_date": "2019-05-24T01:26:28Z", "likes_count": 1}, {"comment_by": "Floris Bollen", "comment_text": "When propagating backwards do you just take the activation of the next layer as if they were y? Or do you have to add the notches or what?", "comment_date": "2019-05-22T16:50:57Z", "likes_count": 0}, {"comment_by": "Vitor Thom\u00e9", "comment_text": "Dude, how the fuck you do your animations and which program you use to create this videos??? Thanks for the explanation btw, and you are crazy xD", "comment_date": "2019-05-21T21:10:56Z", "likes_count": 1}, {"comment_by": "David whyte", "comment_text": "God bless you", "comment_date": "2019-05-21T18:03:56Z", "likes_count": 0}, {"comment_by": "Koke Iks", "comment_text": "Why is cost = (a-y)^2 why is there to power of two", "comment_date": "2019-05-21T12:59:10Z", "likes_count": 0}, {"comment_by": "projunder", "comment_text": "I just watch these to feel smart", "comment_date": "2019-05-19T10:25:30Z", "likes_count": 0}, {"comment_by": "surinder Dhawan", "comment_text": "At 5.09 change in a(L) / change in Z(L) should be equal to sigmoid why it giving derivative of sigmoid", "comment_date": "2019-05-18T08:23:23Z", "likes_count": 0}, {"comment_by": "Jester Shoeman", "comment_text": "Incredible video! Thank you!", "comment_date": "2019-05-14T16:11:37Z", "likes_count": 0}, {"comment_by": "LSun YQ", "comment_text": "\u5341\u5206\u611f\u8c22\u4f60\u4e3a\u6211\u4eec\u5e26\u6765\u8fd9\u4e48\u597d\u7684\u4e00\u4e2a\u89c6\u9891<br>Thank you very much for bringing me such a good course.", "comment_date": "2019-05-14T12:16:18Z", "likes_count": 0}, {"comment_by": "TheWindRider", "comment_text": "I noticed in one of the previous videos, you showed that this was just the beginning of a series of neural network topics, with convolutional neural networks and LSTM listed down the path.<br><br><br>Do you plan on making a future video series on more specific types of neural networks like CNNs and LSTM?", "comment_date": "2019-05-12T05:35:37Z", "likes_count": 2}, {"comment_by": "Billy Kotsos", "comment_text": "All that, to find a fucking minimum...<br>The idea though is simple.", "comment_date": "2019-05-11T21:13:22Z", "likes_count": 0}, {"comment_by": "Callam Ingram", "comment_text": "This was incredible enlightening but I still don\u2019t now where the variable \u2018b\u2019 for bias is coming from is that set at the beginning to correct for errors or...?", "comment_date": "2019-05-09T05:20:47Z", "likes_count": 0}, {"comment_by": "\u05d2\u05d1\u05e8\u05d9\u05d0\u05dc \u05d7\u05d3\u05d0\u05d3", "comment_text": "Great series! one question though, if I got it right, after this video we now know how to calculate the gradient vector (each component is calculated by averaging all the partial derivatives with respect to the corresponding variable in the cost function). My question is this, once we have the gradient vector, how exactly do we determine how to change the weights and biases to reduce the cost function?<br><br><br>Any help would be appreciated.", "comment_date": "2019-05-06T15:15:50Z", "likes_count": 0}, {"comment_by": "PinGuitar", "comment_text": "Ugghhh... my brain ... maybe I&#39;ll just Netflix and chill.", "comment_date": "2019-05-06T05:22:47Z", "likes_count": 0}, {"comment_by": "Alex watermen", "comment_text": "So just to be clear if I were taking dc_0/da(L-2), What would be my equation?", "comment_date": "2019-05-06T02:43:57Z", "likes_count": 0}, {"comment_by": "Jakob Lindskog", "comment_text": "This is easily the best channel on yt", "comment_date": "2019-05-04T14:32:03Z", "likes_count": 0}, {"comment_by": "Kenzi Jeanis", "comment_text": "<a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a> is a godsend. This video explains the high level concepts, but fails to detail some of the underlying work that goes on. I spent at least an hour beating my head against a wall re-watching portions of this video while trying to figure out how to calculate `y` for hidden layers.", "comment_date": "2019-05-04T02:32:42Z", "likes_count": 0}, {"comment_by": "VeryLazyAngel", "comment_text": "This is just fabulous. You sir, are genius.", "comment_date": "2019-05-01T00:58:46Z", "likes_count": 0}, {"comment_by": "amIndian", "comment_text": "This is the clearest explanation of backprop and the symbols used! Great job", "comment_date": "2019-04-26T06:06:46Z", "likes_count": 0}, {"comment_by": "Philip Abraham", "comment_text": "Mr 3Blue1Brown,<br>Are you an alien from an advanced civilization from a faraway galaxy, send here to earth to enlighten us dumb earthlings on the idea that mathematics is truly beautiful, and actually can be learned by any one who cares to learn?? <br>If you are an alien, then please don&#39;t go back, stay here on earth as long as you can. For we surely need you here on earth to  wake us out of our mathematical and scientific ignorance. Our educational systems here on earth has truly failed us and drive millions of us to dislike mathematics and science. You are our only hope!!!", "comment_date": "2019-04-24T21:41:57Z", "likes_count": 0}, {"comment_by": "Rodwan Bakkar", "comment_text": "amazing!!! you are from different world!!!! please do a video about gradient boosting!", "comment_date": "2019-04-23T14:00:37Z", "likes_count": 0}, {"comment_by": "Isaac A.", "comment_text": "Is this the power of a god?", "comment_date": "2019-04-23T01:55:19Z", "likes_count": 0}, {"comment_by": "Thiago Machado", "comment_text": "Oh my.. I just noted that my first AI implementation was a single neuron on a single layer hahahah :z", "comment_date": "2019-04-22T23:58:13Z", "likes_count": 1}, {"comment_by": "CH Ho", "comment_text": "Always come back to this series when I need to refresh basic NN concepts.  It would great also if you could include the math behind NN, like the concepts from this book: <a href=\"https://explained.ai/matrix-calculus/index.html\">https://explained.ai/matrix-calculus/index.html</a>.", "comment_date": "2019-04-22T06:30:09Z", "likes_count": 0}, {"comment_by": "SirHoneyBadger", "comment_text": "What about the weights between the input layer and the first hidden layer? How would they be changed. I&#39;m confused as &#39;z&#39; of a layer is calculated by taking the product of the weight between that layer and the previous and the activation of the node in the previous layer.<br><br>But then what would the activation be of an input node? Just the input value? Or the sigmoid value from the input value?", "comment_date": "2019-04-21T22:25:21Z", "likes_count": 0}, {"comment_by": "yolo mein", "comment_text": "How do you know the desired output of a neuron in the hidden layer?", "comment_date": "2019-04-21T01:19:02Z", "likes_count": 1}, {"comment_by": "Banke A.", "comment_text": "Thank you for breaking this down. I truly enjoyed watching this :)", "comment_date": "2019-04-20T23:59:40Z", "likes_count": 0}, {"comment_by": "Agustri Berry", "comment_text": "Ohmegood thiss is so helping.....", "comment_date": "2019-04-19T10:47:38Z", "likes_count": 0}, {"comment_by": "Simon Mar\u00e9chal", "comment_text": "How do you derivate ReLu ?", "comment_date": "2019-04-18T14:45:15Z", "likes_count": 0}, {"comment_by": "Mahery Ranaivoson", "comment_text": "One of the clearest concept behind BP.  My 6 hours (with tons of doubt) class has been cleared in 10 min. Keep going", "comment_date": "2019-04-16T12:53:39Z", "likes_count": 0}, {"comment_by": "Nufosmatic", "comment_text": "Very effective animation. Or did I mean affective? Thank you.", "comment_date": "2019-04-15T13:53:39Z", "likes_count": 0}, {"comment_by": "Mingqian Ye", "comment_text": "I had to watch this video 10+ times in order to get my head around the math.", "comment_date": "2019-04-14T21:14:01Z", "likes_count": 0}, {"comment_by": "Wentao Qiu", "comment_text": "Best explanation there is, thank you so much for making the series.", "comment_date": "2019-04-14T18:56:39Z", "likes_count": 0}, {"comment_by": "Mukul Paras Potta", "comment_text": "Make more videos on this topic if possible!! People would love to learn from you!", "comment_date": "2019-04-10T15:13:08Z", "likes_count": 0}, {"comment_by": "Dharma Teja Nuli", "comment_text": "God Bless You man! I have tried watching many videos about backpropagation but this series made my conceptual understanding and intuition super clear. Thanks a lot. You have no idea how happy I am right now to have understood the concept of backpropagation.", "comment_date": "2019-04-09T20:32:59Z", "likes_count": 0}, {"comment_by": "hupa1a", "comment_text": "Wow ! That&#39;s a perfect explanation! Thank you so much!", "comment_date": "2019-04-08T17:10:18Z", "likes_count": 0}, {"comment_by": "Thomas Klein", "comment_text": "Thank you very much for this awesome video. I have a question: If I&#39;m not totally stupid, the framed formula in <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m26s\">9:26</a> is wrong. The first problem is, that you use &quot;j&quot; as summation index but j is already defined. Secondly I think that the indices of w in the equation are wrong. I think, it should be <br>\u03a3_i w_ij \u03c3&#39;(z_i) \u2202C/\u2202a_i <br>(so w_ij instead of w_ik)<br>Is that correct or am I just too dumb?", "comment_date": "2019-04-07T22:46:04Z", "likes_count": 1}, {"comment_by": "Guy1524", "comment_text": "Hmm, I wonder why the view count goes down from &gt;1M to ~600K for this chapter :P", "comment_date": "2019-04-07T21:34:44Z", "likes_count": 0}, {"comment_by": "TheDiscoMole", "comment_text": "Most intuitive and visually stimulating explanation of backpropogation out there. And Most importantly, without misleading mistakes, mr &quot;sumthegradientwrttoactivationvector&quot;", "comment_date": "2019-04-05T14:05:42Z", "likes_count": 0}, {"comment_by": "time crystal", "comment_text": "Thank you for starting your indices at 0.", "comment_date": "2019-04-05T12:09:46Z", "likes_count": 38}, {"comment_by": "TheOnlyEpsilonAlpha", "comment_text": "Srye but I still don&#39;t get it \ud83d\ude1e", "comment_date": "2019-04-02T14:45:11Z", "likes_count": 0}, {"comment_by": "Mario Shtjefni", "comment_text": "Well you didn&#39;t really explain much of the backpropagation algorithm, or how the gradient vector in a complex neural network looks like.", "comment_date": "2019-03-31T02:03:21Z", "likes_count": 0}, {"comment_by": "agapeloa", "comment_text": "So good, thank you !<br>When do you think to make the next vid\u00e9o ? :) pleaaaaase !", "comment_date": "2019-03-30T15:36:34Z", "likes_count": 0}, {"comment_by": "Gary Chen", "comment_text": "you must have spent a lot of time to accomplish such an excellent tutorial!!", "comment_date": "2019-03-28T11:53:47Z", "likes_count": 0}, {"comment_by": "Wasiim", "comment_text": "For anyone who is having trouble understanding it. It&#39;s easier to understand if you&#39;re familiar with computing gradients for multiple linear regression problems, it&#39;s basically the same idea. In a multiple linear regression problem you would be dealing with a vector of partial derivatives to compute the gradient, but here, since there&#39;s multiple layers, it&#39;s a matrix of partial derivatives.", "comment_date": "2019-03-28T05:55:21Z", "likes_count": 0}, {"comment_by": "Harshit Madan", "comment_text": "Please make more videos on ML.", "comment_date": "2019-03-26T19:59:33Z", "likes_count": 0}, {"comment_by": "Shikhar Bakhda", "comment_text": "I did not pat myself on the back, so...", "comment_date": "2019-03-26T10:21:26Z", "likes_count": 0}, {"comment_by": "Daniel Wildegger", "comment_text": "Amazing series, thank you so much for that", "comment_date": "2019-03-24T15:54:43Z", "likes_count": 0}, {"comment_by": "\u00d8rjan Solli", "comment_text": "If all weights are zero, will it stop the backpropogation? How can it carry the cost backwards when multiplying with weights=0? I&#39;m loogin at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=6m25s\">6:25</a>. I coded it an ran it on my computer and ended up with all weights set to zero.", "comment_date": "2019-03-21T21:02:55Z", "likes_count": 0}, {"comment_by": "Tom\u00e1s Enrique", "comment_text": "I love you!", "comment_date": "2019-03-18T23:47:14Z", "likes_count": 0}, {"comment_by": "lone WOLF", "comment_text": "the world is nothing without great people like you.....Good Job!", "comment_date": "2019-03-18T01:01:12Z", "likes_count": 0}, {"comment_by": "Sacation", "comment_text": "Awesome series! Even though i already had quite a intuitive feeling about the concepts of Deep learning, your videos just always make complex subjects click in my mind, it sort of forms the right connections between the neurons in my mind i suppose so ;)<br>Even without any advanced math knowledge i was able to follow your math, so thanks for choosing to keep your examples as simple as possible! <br>I&#39;m gonna make my own network from scratch in code some time, to see if i truly understand it throughly.", "comment_date": "2019-03-16T11:26:54Z", "likes_count": 4}, {"comment_by": "64_bit", "comment_text": "3B1B is a god.", "comment_date": "2019-03-16T06:30:50Z", "likes_count": 0}, {"comment_by": "weerobot", "comment_text": "Neural Net Porn...Your the Best...", "comment_date": "2019-03-13T20:18:27Z", "likes_count": 0}, {"comment_by": "karan jathoul", "comment_text": "Thankyou. Brilliant. thanks again.", "comment_date": "2019-03-12T23:31:31Z", "likes_count": 0}, {"comment_by": "Saket Seshadri", "comment_text": "thanks a lot for these amazingly informative and intuitive videos. These videos have helped me learn a lot more than what I did in class. You&#39;re doing a great job. Thanks again", "comment_date": "2019-03-11T15:46:15Z", "likes_count": 0}, {"comment_by": "Rebel Babble", "comment_text": "You&#39;re constantly releasing impressive videos. Phenomenal job.", "comment_date": "2019-03-11T11:49:19Z", "likes_count": 0}, {"comment_by": "Vivek Pratap", "comment_text": "The best explanation and illustration on how deep learning works. Just see the 4 videos. Great job \ud83d\udc4d\ud83c\udffb", "comment_date": "2019-03-10T16:29:06Z", "likes_count": 0}, {"comment_by": "Luxcium", "comment_text": "How are you you capable to explain something so much complicated in a so simple way you are magic these videos are incredibly complex to produce I cannot understand how you manage to do that :-)", "comment_date": "2019-03-06T15:35:13Z", "likes_count": 0}, {"comment_by": "Souhardya Sen", "comment_text": "You have way less subscribers than you deserve", "comment_date": "2019-03-04T18:10:03Z", "likes_count": 0}, {"comment_by": "Math\u00e9o Lentz", "comment_text": "Tips : play the video at 0,75 speed", "comment_date": "2019-03-01T11:08:37Z", "likes_count": 0}, {"comment_by": "Math\u00e9o Lentz", "comment_text": "Excellent vid\u00e9o \u00e0 quand les sous titres en fran\u00e7ais ?", "comment_date": "2019-03-01T10:58:31Z", "likes_count": 0}, {"comment_by": "Ankit Chakraborty", "comment_text": "Thank you very much for such a nice visualization of Neural Networks. Please upload videos on vectorization along with this series. Thanks in advance.", "comment_date": "2019-03-01T10:37:14Z", "likes_count": 1}, {"comment_by": "quistis218", "comment_text": "Thank you for such well made, informative videos!", "comment_date": "2019-02-28T17:56:33Z", "likes_count": 0}, {"comment_by": "Aykut BOZKURT", "comment_text": "I finally formed an image in my mind on backprop. Tx for clear explanation.", "comment_date": "2019-02-28T16:52:54Z", "likes_count": 0}, {"comment_by": "Aditya Shrivastava", "comment_text": "It would have been better if you had explained with softmax and softmax entropy.", "comment_date": "2019-02-25T17:39:26Z", "likes_count": 0}, {"comment_by": "Otto", "comment_text": "Not that useful actually<br>It was so basic", "comment_date": "2019-02-25T00:05:57Z", "likes_count": 0}, {"comment_by": "John Frye", "comment_text": "so I was watching this MIT video <a href=\"https://www.youtube.com/watch?v=uXt8qF2Zzfo\">https://www.youtube.com/watch?v=uXt8qF2Zzfo</a> on the same subject. My only confusion is this addition of multiple layers and its effect on complexity. Winston from MIT says that this thing is not going to grow exponentially and that is where I become a bit confused. Let us say you add a third layer to your example at the end and label a node a_k^(L-2).  When computing the effect of change of activation value of this node to your cost function, there will be three in the (L-1) layer PER node in level (L).  That means you will have to perform a multiplicity of summing operations, such that for each layer, you add a factor to  the product. In this case the layers are size 3 and 2, so six additions for each node in the (L-2) layer, etc.   Clearly you can compute the partials quadratically for each level (doubly nested for loop), but then for each level you will need to begin nesting loops exponentially and adding together partials to get overall derivative wrt cost function. Perhaps I am misunderstanding what is meant by exponential however.", "comment_date": "2019-02-24T22:33:06Z", "likes_count": 0}, {"comment_by": "toiletscrubbr", "comment_text": "i fucking love you bro", "comment_date": "2019-02-21T03:54:02Z", "likes_count": 0}, {"comment_by": "Precious Jatau", "comment_text": "Great video. You made neural networks very easy to understand.", "comment_date": "2019-02-20T05:46:04Z", "likes_count": 0}, {"comment_by": "greenblock1000", "comment_text": "!gold", "comment_date": "2019-02-20T01:02:53Z", "likes_count": 0}, {"comment_by": "JunJie Yang", "comment_text": "The forth video makes me undersatd the backpropagation pretty clear. It&#39;s hard to understand when I saw Andrew Ng&#39;s class. I&#39;d like to say : good job !", "comment_date": "2019-02-15T03:51:38Z", "likes_count": 0}, {"comment_by": "josy26", "comment_text": "I think you forgot to add the activation function on <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=2m25s\">2:25</a> in order to obtain a(L) from z(L), on your tree structure", "comment_date": "2019-02-13T16:38:11Z", "likes_count": 0}, {"comment_by": "Emil Babazade", "comment_text": "I don&#39;t know why ( probably because i don&#39;t know much calculus ) but Andrew Ng&#39;s explanation seems easier to understand (but not nearly as magical).", "comment_date": "2019-02-13T14:30:29Z", "likes_count": 1}, {"comment_by": "Alex P", "comment_text": "Absolutely fantastic visualization! Thank you", "comment_date": "2019-02-12T16:59:49Z", "likes_count": 0}, {"comment_by": "Prithwish bhowmick", "comment_text": "uff...im gonna open my class 12 rd sharma (it&#39;s a great math book in India )", "comment_date": "2019-02-12T15:32:54Z", "likes_count": 0}, {"comment_by": "ADITYA GHOSH", "comment_text": "simply amazing", "comment_date": "2019-02-11T14:40:23Z", "likes_count": 0}, {"comment_by": "Carlos Xam-mar", "comment_text": "You should do more videos about this topic!", "comment_date": "2019-02-11T10:01:53Z", "likes_count": 0}, {"comment_by": "Atin Singh", "comment_text": "thank you.. great explanation!", "comment_date": "2019-02-11T01:39:27Z", "likes_count": 0}, {"comment_by": "\u00c1lvaro", "comment_text": "This is the best material i found to understand Neural Networks. Nice job dude!", "comment_date": "2019-02-10T17:54:26Z", "likes_count": 0}, {"comment_by": "Gary Lau", "comment_text": "Please make a video to explain the backpropagation calculus of CNN and LSTM! Thanks", "comment_date": "2019-02-09T19:54:15Z", "likes_count": 0}, {"comment_by": "Chris Jones", "comment_text": "d/dx (ReLu) = ?<br><br>(you play a bit fast &amp; loose with &quot;derivative of sigmoid or whatever your squishification function is&quot;)", "comment_date": "2019-02-09T15:45:36Z", "likes_count": 0}, {"comment_by": "racoon3456", "comment_text": "But why are they partial derivatives?", "comment_date": "2019-02-08T08:21:30Z", "likes_count": 0}, {"comment_by": "hazzaldo N", "comment_text": "Thank you so much for the video series. Really appreciate the effort you go into for the animation and finer details of the topic. :)<br>I have one question - At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m27s\">8:27</a>: you explained how to find the partial derivative of Cost function with regards to Activation neuron of pervious layer  ak(L -1). You mentioned that the neuron affects more than one neuron in the last layer (or outcome). In the example of the video it affect a0(L) and a1(L), which both affect the Cost. Then you said: &quot;... and you have to add those up ...&quot;. Sorry I didn&#39;t understand - add what up? Do you mean add both affected neurons in the last layer(so add the values of a0(L) and a1(L) = 0.83 + 0.63? Is this what you mean. If so, is it correct to say then the partial derivative of:  \u2202Zj(L) with regards to Activation neuron of pervious layer \u2202ak(L -1) = a0(L) + a1(L)<br>Sorry for the long-winded and badly formatted question.", "comment_date": "2019-02-06T22:23:31Z", "likes_count": 0}, {"comment_by": "John Abramo", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m36s\">4:36</a><br>Wouldn&#39;t the derivative of (a(L) - y)\u00b2 be 2a(L) - 2y + y\u00b2?", "comment_date": "2019-02-06T14:32:14Z", "likes_count": 1}, {"comment_by": "A B C", "comment_text": "I kept getting stuck thinking about gradients leading to local minima, not global minima", "comment_date": "2019-02-05T14:20:41Z", "likes_count": 0}, {"comment_by": "playmatee1", "comment_text": "this is the motivation <br>,like visualization to math make it interesting", "comment_date": "2019-02-04T21:27:45Z", "likes_count": 0}, {"comment_by": "Miguel Ressurrei\u00e7\u00e3o", "comment_text": "Is this the last video of the series??", "comment_date": "2019-02-04T19:16:23Z", "likes_count": 0}, {"comment_by": "aty14", "comment_text": "hello men<br>I don\u2019t undrestand very well what you speak then you fall me asleep very effitiency ! thx dude ^u^", "comment_date": "2019-02-01T06:24:23Z", "likes_count": 0}, {"comment_by": "zero study", "comment_text": "Well done Mr. 1brown and the 3blues. I get the Idea. Now time to do some  (virtual envs) &quot;Field Test&quot;", "comment_date": "2019-01-31T13:41:07Z", "likes_count": 0}, {"comment_by": "Suchandra Bhattacharyya", "comment_text": "This is absolutely brilliant", "comment_date": "2019-01-31T03:28:00Z", "likes_count": 0}, {"comment_by": "Imrahil", "comment_text": "This is what always annoyed me about calling them neural networks. They are not. They&#39;re administered nodal networks. In reality there is no overwatch, only emerging properties.", "comment_date": "2019-01-30T06:58:50Z", "likes_count": 0}, {"comment_by": "Valentin Mezev", "comment_text": "OMG. All the bunch of complicated materials I&#39;ve been looking at in the last one month suddenly make sense... thank you :)", "comment_date": "2019-01-29T15:23:30Z", "likes_count": 0}, {"comment_by": "Kord Campbell", "comment_text": "Absolutely brilliant! It was effective, intuitive and enjoyable.", "comment_date": "2019-01-27T16:46:12Z", "likes_count": 0}, {"comment_by": "Andr\u00e9 Rossa", "comment_text": "thank you so much", "comment_date": "2019-01-27T14:21:26Z", "likes_count": 0}, {"comment_by": "Kai Z", "comment_text": "BEST VID EVER SEEN !", "comment_date": "2019-01-23T12:23:01Z", "likes_count": 0}, {"comment_by": "Preacher of Nothing", "comment_text": "Finally someone explained it to me in a way that makes sense.<br>It&#39;s actually pretty simple, now that I get it... but then, I suppose all math is simple, once you get it.<br>Thank you very much!", "comment_date": "2019-01-22T22:55:12Z", "likes_count": 0}, {"comment_by": "DouglasJBender", "comment_text": "I don&#39;t like how there are never any actual humans in the videos.  It feels like my species is being disrespected.", "comment_date": "2019-01-22T11:29:54Z", "likes_count": 0}, {"comment_by": "Maikon Michel Almeida", "comment_text": "y (disered output) is a unique answer in all formulas or  \r<br>should each layer contain its own desired response?", "comment_date": "2019-01-18T03:47:00Z", "likes_count": 1}, {"comment_by": "Giuseppe Perfetto", "comment_text": "This is a great video. Thank you", "comment_date": "2019-01-17T19:38:43Z", "likes_count": 0}, {"comment_by": "Ishan Mishra", "comment_text": "Can someone please explain why we need to do/find  dC0/DaL-1 at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m31s\">8:31</a>? :(", "comment_date": "2019-01-17T12:29:54Z", "likes_count": 0}, {"comment_by": "Maxime Bernard", "comment_text": "this make my math classes worth it", "comment_date": "2019-01-14T14:56:58Z", "likes_count": 1}, {"comment_by": "Khanh Nguyen", "comment_text": "it&#39;s really helpful", "comment_date": "2019-01-14T07:40:23Z", "likes_count": 0}, {"comment_by": "Steven Luoma", "comment_text": "I feel like I understand what you did at a basic level, but if I were to go try to make my own neural network, I&#39;d be in a lot of trouble. I&#39;d love a followup video showing the matrix math explicitly, even if I computer program is used to do the gruntwork. I just want to see the pieces moving myself.", "comment_date": "2019-01-13T21:49:15Z", "likes_count": 1}, {"comment_by": "Vitul Bansal", "comment_text": "just an observation.<br>I was looking at the views from 1st video of this series to last(4th), it dropped drastically. <br>from 3.41M (1st Video) <br>     to 1.34M (2nd Video) <br>   to 0.873M (3rd Video)<br>   to 0.511M (4th Video), and same is the case with no. of likes", "comment_date": "2019-01-12T14:00:48Z", "likes_count": 0}, {"comment_by": "Max Krause", "comment_text": "Candy for my brainnnn!!!", "comment_date": "2019-01-12T10:56:07Z", "likes_count": 0}, {"comment_by": "Nwxxz Chen", "comment_text": "Thanks for your impressive works. Will you publish episode 5, I&#39;m curiosity to watch how you explain CNN networks.", "comment_date": "2019-01-12T09:50:09Z", "likes_count": 0}, {"comment_by": "\u5ed6\u548c\u8d77", "comment_text": "headache", "comment_date": "2019-01-12T04:03:42Z", "likes_count": 1}, {"comment_by": "Ali Janalizadeh", "comment_text": "Could you please make videos on Recurrent Neural Networks, Convolutional Neural Networks, and Capsule Networks along with their maths and calculus? I&#39;m sure you would get a good amount of views as well.", "comment_date": "2019-01-11T16:07:01Z", "likes_count": 0}, {"comment_by": "Z Chin", "comment_text": "Finally, I understand the concept of backpropagation. Thanks.", "comment_date": "2019-01-11T08:12:08Z", "likes_count": 0}, {"comment_by": "KeithBofaptos", "comment_text": "432ish days since your last video. Can we keep hoping that you will continue to create more of these flat out amazing videos Grant? I&#39;ve just found your videos. And to be honest I&#39;m a bit worried, on a scared level, that you may not produce more. I&#39;m learning to code. And have a fascination with Neural Networks. Burning through books and videos, alone. So your videos are magic for me in a way.", "comment_date": "2019-01-11T02:15:46Z", "likes_count": 0}, {"comment_by": "Kevin Spacey", "comment_text": "Ok, gonna come back tomorrow.", "comment_date": "2019-01-08T22:58:21Z", "likes_count": 0}, {"comment_by": "Jake Quinn", "comment_text": "Great series!", "comment_date": "2019-01-08T21:43:58Z", "likes_count": 0}, {"comment_by": "Vaibhav Kumar", "comment_text": "That was so beautifully explained!!! \u2764", "comment_date": "2019-01-08T13:02:13Z", "likes_count": 0}, {"comment_by": "Wiktor Migaszewski", "comment_text": "Is CNN explanation movie in the pipeline?", "comment_date": "2019-01-06T11:57:34Z", "likes_count": 0}, {"comment_by": "Tran Tan Dat", "comment_text": "too good!", "comment_date": "2019-01-04T10:55:19Z", "likes_count": 0}, {"comment_by": "Zach Allen", "comment_text": "I have an interview tomorrow during which I\u2019ll probably be asked about machine learning, thank you!!", "comment_date": "2019-01-03T02:22:18Z", "likes_count": 0}, {"comment_by": "Avana", "comment_text": "What I don&#39;t get, is how you&#39;d go about writing the formula when you&#39;re finding the derivative of a weight between two hidden layers with respect to the cost function. All that pops up in my head is a potential giant formula that would be way too much work to code or even start writing down, especially when you go back further. So, my remaining question would be, how would one find this derivative? Since each weight in the network influences the error, you&#39;d have to apply the chain rule many times and it quickly gets messy.", "comment_date": "2019-01-01T22:35:16Z", "likes_count": 0}, {"comment_by": "Brutal Games", "comment_text": "You deserve an Oscar in visual story telling! Thank you for your work!", "comment_date": "2019-01-01T19:42:54Z", "likes_count": 2}, {"comment_by": "Javier Quevedo Fern\u00e1ndez", "comment_text": "Thank you so much for this series. I wish the teachers I had during my studies could explain concepts anywhere near as clearly as you do.", "comment_date": "2018-12-29T09:20:06Z", "likes_count": 0}, {"comment_by": "the adai", "comment_text": "Really helpful!", "comment_date": "2018-12-27T19:48:57Z", "likes_count": 0}, {"comment_by": "Kanva", "comment_text": "The most perfect channel i have ever seen till now", "comment_date": "2018-12-27T04:56:19Z", "likes_count": 0}, {"comment_by": "Traian Coza", "comment_text": "Very good series thanks", "comment_date": "2018-12-26T20:24:37Z", "likes_count": 0}, {"comment_by": "Antony Mapfumo", "comment_text": "Clear and concise explanation. Thank you.", "comment_date": "2018-12-24T07:56:28Z", "likes_count": 0}, {"comment_by": "Malte Nielsen", "comment_text": "This video series on neural networks is amazing and I think it\u2019s worth watching regardless of your background or experience in ML.<br>It has it all, intuitive introductions that are actually very close to the real picture (which you also manage to describe in a clear way), and still it feels straight to the point.<br>No need to say it, but: Your graphics are really beautiful and support your teaching to the uttermost level where your audience is almost hypnotized.<br>Although I didn\u2019t learn anything new I still feel like I learned a lot.<br>Kudos to you!", "comment_date": "2018-12-19T22:35:23Z", "likes_count": 0}, {"comment_by": "Romano Wienke", "comment_text": "I really like these 4 videos. They`re pretty easy to understand and I like the visualization of  the principles. What do you think about simulated annealing to minimize the cost function in more complex applications?", "comment_date": "2018-12-19T12:43:24Z", "likes_count": 0}, {"comment_by": "ytrichardsenior", "comment_text": "Could you make a video which explains &#39;algorithmically&#39; how to implement back propagation? That is... &quot;sum all the&quot;.. &quot;reset the value of&quot; etc.? Do you know of such a video?", "comment_date": "2018-12-18T13:46:47Z", "likes_count": 0}, {"comment_by": "Alex B", "comment_text": "when considering the derivative of the cost with respect to the activation value in the (L - 1)th layer, for all the neurons in the (L - 1)th layer, is the derivative of the cost with repsect to the (j)th neuron in the (L)th layer just a constant for all (L - 1)th layer neurons? (I&#39;m asking about the last term in the chain rule at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m00s\">9:00</a>)", "comment_date": "2018-12-16T14:53:34Z", "likes_count": 0}, {"comment_by": "Emadeldeen Eldele", "comment_text": "Brilliant explanation ... the best ever!!!!!", "comment_date": "2018-12-15T19:35:48Z", "likes_count": 0}, {"comment_by": "marco", "comment_text": "i&#39;ve never cared about a channel that much in my life, like geez the amount of work you put into your videos is insane and thank you so much for that, also for teaching me neural network", "comment_date": "2018-12-14T05:00:03Z", "likes_count": 0}, {"comment_by": "K W", "comment_text": "note to self, stopped at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m39s\">8:39</a>, will delete this comment when i return", "comment_date": "2018-12-13T23:03:33Z", "likes_count": 0}, {"comment_by": "ahgoon69er", "comment_text": "so these were lowkey calc vids", "comment_date": "2018-12-13T15:22:39Z", "likes_count": 0}, {"comment_by": "Chris Chris", "comment_text": "Very nice explanation as always. Thank you :) But I have one question left:  Concerning the last formula that you can see since minute <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m32s\">9:32</a> : So am I right that I have to calculate this dC/dw(jk,l) = ....  for each (of the thousands) training data and then calculate the average of this result to get one element in the gradient descent vector? In this case to get the element w(jk,l) in the gradient decent vector? Thanks for your help :)", "comment_date": "2018-12-12T21:40:36Z", "likes_count": 1}, {"comment_by": "Snygg-Johan", "comment_text": "Please make more videos on deep learning \u2764\ufe0f", "comment_date": "2018-12-11T12:38:16Z", "likes_count": 0}, {"comment_by": "Arisu", "comment_text": "im smart now", "comment_date": "2018-12-11T10:01:53Z", "likes_count": 0}, {"comment_by": "Avi Garg", "comment_text": "the best explanation I found on the internet yet!!", "comment_date": "2018-12-08T03:46:04Z", "likes_count": 0}, {"comment_by": "Cris", "comment_text": "This is, by wide margin, the best explanation series on BP.", "comment_date": "2018-12-07T04:38:17Z", "likes_count": 0}, {"comment_by": "Jan Auracher", "comment_text": "Probably don&#39;t have to add this anymore after all the positive comments your video received already\u00a0- but I simply feel I owe this to you: After having studied BP for weeks now I finally understand the concept. Thank you for this amazing video!", "comment_date": "2018-12-05T13:52:45Z", "likes_count": 0}, {"comment_by": "Lukas", "comment_text": "Thank you so much, you&#39;ve really inspired me to do my master after bachelor, you are a wonderfull teacher, and I&#39;m on my way for an idustrial master in CS thanks to you!!!", "comment_date": "2018-12-04T19:47:30Z", "likes_count": 0}, {"comment_by": "Ricardo I", "comment_text": "Even with that wonderful explanation, this still is fucking complex. Nice work.", "comment_date": "2018-12-03T17:26:12Z", "likes_count": 0}, {"comment_by": "Soulsphere001", "comment_text": "I learned derivatives a few years back, but now it seems I&#39;ll have to relearn some of it. I do recall that something like:<br>x^2 + x + 2 = 2x + 1<br><br>so I do recall some of that knowledge. But the second we get into multiplication/division or chains, I&#39;ve forgotten all about that. I guess it&#39;s time to log back into Khan Academy.", "comment_date": "2018-12-03T01:50:49Z", "likes_count": 0}, {"comment_by": "murat can karacabey", "comment_text": "thank you man", "comment_date": "2018-12-02T19:22:15Z", "likes_count": 0}, {"comment_by": "ImGonnaShout2000", "comment_text": "Take a moment just to appreciate the animation in the top left at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=6m51s\">6:51</a>... wow.", "comment_date": "2018-12-02T15:16:45Z", "likes_count": 0}, {"comment_by": "Rey David", "comment_text": "Great! It is all clear now", "comment_date": "2018-12-02T03:09:48Z", "likes_count": 0}, {"comment_by": "kaustaubh1234", "comment_text": "Excellent", "comment_date": "2018-12-01T15:15:55Z", "likes_count": 0}, {"comment_by": "Adnan", "comment_text": "looking forward to more videos dealing with further deep learning topics.", "comment_date": "2018-11-28T13:56:33Z", "likes_count": 0}, {"comment_by": "Pinkpat_", "comment_text": "Hi @3blue1Brown, first you make understood something teacher didnt succeed to make me understand in 6months, so thank you !<br>Second, did you plan other videos on this ML (even if this one is one y/o)?", "comment_date": "2018-11-27T23:11:08Z", "likes_count": 0}, {"comment_by": "Lalit Kumar", "comment_text": "But, given a problem how someone can choose structure of network. If I have to build an train a network for recognising alphabets how many intermediate layers and of how many neurones should I choose", "comment_date": "2018-11-26T03:37:49Z", "likes_count": 0}, {"comment_by": "Chris M", "comment_text": "An improvement in this video would be to clarify that squaring the difference between the output layer and the labeled data is not the only way to calculate the cost function. Another common way to do it is to use the same cost function as logistic regression: -(y*log(h) + (1-y)*(1-log(1-h))), where h is the output of the neural network. It results in a slightly different gradient.", "comment_date": "2018-11-26T02:16:28Z", "likes_count": 0}, {"comment_by": "ajnabee", "comment_text": "This is the best explanation on The Planet.", "comment_date": "2018-11-25T13:03:33Z", "likes_count": 0}, {"comment_by": "Mirla Monta\u00f1o", "comment_text": "great series, I will be expecting more! :D", "comment_date": "2018-11-20T04:54:15Z", "likes_count": 0}, {"comment_by": "/", "comment_text": "great video series!! best ive come across thus far. so much better than the crappy medium articles that &quot;decide to write neural networks without using frameworks to deepen their understanding&quot; which all copy one another lol", "comment_date": "2018-11-19T07:47:47Z", "likes_count": 0}, {"comment_by": "Aniket Chodankar", "comment_text": "You sir are a genius. Thanks a million!!!!!!", "comment_date": "2018-11-18T05:40:35Z", "likes_count": 0}, {"comment_by": "\u8607\u5fd7\u96c4", "comment_text": "\u771f\u662f\u5927\u5e2b\u7d1a\u7684\u4f5c\u54c1\uff0c\u89e3\u91cb\u5f97\u975e\u5e38\u6e05\u695a\uff0c\u592a\u795e\u5947\u4e86! Awesome!!", "comment_date": "2018-11-18T04:55:42Z", "likes_count": 5}, {"comment_by": "Luke Fernandez", "comment_text": "10,000th like", "comment_date": "2018-11-17T16:32:40Z", "likes_count": 0}, {"comment_by": "THE TNT MINECART", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=7m43s\">7:43</a> on top of the sum function there is an  &#39;N&#39;  can anyone explain why is it there and what is segnifies.", "comment_date": "2018-11-17T14:54:36Z", "likes_count": 0}, {"comment_by": "Malik Kon\u00e9", "comment_text": "I&#39;m a bit confused about you calling these derivative, ratios.  Are they really ratios ? or is it just an false intuition taken for the history of the notation. <a href=\"https://math.stackexchange.com/questions/320228/the-notation-for-partial-derivatives#320274\">https://math.stackexchange.com/questions/320228/the-notation-for-partial-derivatives#320274</a>", "comment_date": "2018-11-16T22:51:54Z", "likes_count": 0}, {"comment_by": "IslandCave", "comment_text": "Ok, so this shows how to look for a local minimum, and represents it here visually and metaphorically as a marble thrown on to a bumpy terrain that will then roll down a hill, till it finds a local minimum. So, I would think then a way to search for the absolute minimum would be to repeat this entire process over and over, throwing many marbles onto this metaphorical bumpy terrain, and letting it roll down, and over time, the different marbles should roll down to different local minimums. This will allow you to keep finding lower and lower local minimum&#39;s, and eventually, maybe even find a local minimum where the cost function is equal to 0.", "comment_date": "2018-11-16T20:39:21Z", "likes_count": 0}, {"comment_by": "Mister P By the Sea", "comment_text": "These are some of the best videos on neural networks I have ever seen! so well explained, from the abstract level to the mathematical. Thank you so much, I&#39;m an aspiring data scientist looking to learn, you helped me so much!", "comment_date": "2018-11-16T16:59:09Z", "likes_count": 0}, {"comment_by": "Eisenhower303", "comment_text": "It helps to simplify by setting a_0 = 1 and w_0 = b_0 etc., then you don\u2019t have to combine them into a new variable like z. They do this linear regression too.", "comment_date": "2018-11-15T23:36:09Z", "likes_count": 0}, {"comment_by": "Hisham Ragheb", "comment_text": "That was excellent and better than almost all books I&#39;ve read about ML tried to explain back propagation", "comment_date": "2018-11-14T05:24:38Z", "likes_count": 0}, {"comment_by": "Andres Gomez", "comment_text": "This is so interesting. Fantastic work!", "comment_date": "2018-11-14T05:01:42Z", "likes_count": 0}, {"comment_by": "Frederic Meyer", "comment_text": "Great video series, Grant. Enjoyed watching it a lot.<br>I still have to think about chain rule formula, as I haven&#39;t seen it presented like this before, though it is clear why it is used.", "comment_date": "2018-11-13T15:34:04Z", "likes_count": 0}, {"comment_by": "Shikha Shah", "comment_text": "What beautiful explanation and animations! Please could you make video on Rnn and Lstm.", "comment_date": "2018-11-12T13:58:32Z", "likes_count": 0}, {"comment_by": "Rinze", "comment_text": "Amazing! At first I quit watching as I got lost in the math. Luckily  some of the comments advised me to go watch it another time.Thanks a lot for this great video, I understood all of it the 2nd time.", "comment_date": "2018-11-10T13:00:59Z", "likes_count": 0}, {"comment_by": "Nur Asyiqin Hamzah", "comment_text": "hi everyone , i want to ask a question. it may sound stupid, but how to get the y actual, because my database input is the feature vector of a signature image. i have no idea where should i get the y actual.", "comment_date": "2018-11-10T08:56:24Z", "likes_count": 0}, {"comment_by": "Asma Farid", "comment_text": "Excellent video, the way concepts are being presented its highly commendable", "comment_date": "2018-11-08T10:22:42Z", "likes_count": 0}, {"comment_by": "alpha omega", "comment_text": "nice exprn", "comment_date": "2018-11-07T12:57:24Z", "likes_count": 0}, {"comment_by": "Mehmet Ca", "comment_text": "Incredible, awesome, brilliant, indescribable... so thankful for that!", "comment_date": "2018-11-03T22:32:06Z", "likes_count": 0}, {"comment_by": "Computer Scientist", "comment_text": "ok, you finally lost me here", "comment_date": "2018-11-03T02:05:14Z", "likes_count": 0}, {"comment_by": "Robert Lukoshko", "comment_text": "That is amazing! So clear understanding like never before. I understood it 15 years old, Thanks a lot, waiting for new exciting videos!", "comment_date": "2018-10-31T16:19:14Z", "likes_count": 0}, {"comment_by": "Reljod Oreta", "comment_text": "thanks for helping me as I&#39;m just starting with machine learning..", "comment_date": "2018-10-31T08:56:12Z", "likes_count": 0}, {"comment_by": "Jagveer Singh", "comment_text": "Amazing resource and put together extremely well.", "comment_date": "2018-10-29T00:00:07Z", "likes_count": 0}, {"comment_by": "Samwel Walter", "comment_text": "Where&#39;s the rest of the series man!?", "comment_date": "2018-10-27T13:01:32Z", "likes_count": 0}, {"comment_by": "giannis lakafosis", "comment_text": "thank you so much", "comment_date": "2018-10-26T23:34:06Z", "likes_count": 0}, {"comment_by": "Mariagiorgia Tandoi", "comment_text": "This was simply awesome!! It&#39;d be great if you could also make a video for recurrent neural nets, especially LSTM! I&#39;d definitely love it!", "comment_date": "2018-10-26T11:21:20Z", "likes_count": 1}, {"comment_by": "Ania Kozak", "comment_text": "Why does math isn&#39;t shown like that on universities..? Great job.", "comment_date": "2018-10-25T19:25:33Z", "likes_count": 0}, {"comment_by": "Lauyea", "comment_text": "It takes me the second time to fully understand these functions, and  I have a lot of pre-work of this topic.<br>So if anybody has questions to this video, it&#39;s just fine. Watch again will be helpful to comprehend.", "comment_date": "2018-10-25T11:56:50Z", "likes_count": 1}, {"comment_by": "Sujan Dutta", "comment_text": "thank you soooo much....I am learning ML from Andrew Ng&#39;s famaous course and this video is just what I needed to grasp the mathematics...", "comment_date": "2018-10-22T14:32:49Z", "likes_count": 0}, {"comment_by": "Aydin Ahmadli", "comment_text": "u explaining things so smooth.. thank u!", "comment_date": "2018-10-21T20:37:43Z", "likes_count": 0}, {"comment_by": "mem ento", "comment_text": "Excellent series. Very simply explained. You have a gift, for sure.", "comment_date": "2018-10-21T20:14:13Z", "likes_count": 0}, {"comment_by": "mem ento", "comment_text": "why &quot;3 blue 1 brown&quot; ?", "comment_date": "2018-10-21T20:12:57Z", "likes_count": 0}, {"comment_by": "Prathmesh Thakkar", "comment_text": "Great explanation! Would you consider showing how to code this in MATLAB?", "comment_date": "2018-10-21T10:58:52Z", "likes_count": 0}, {"comment_by": "Agustin Diaz", "comment_text": "please more!<br>you&#39;re great", "comment_date": "2018-10-20T00:22:56Z", "likes_count": 0}, {"comment_by": "L G", "comment_text": "\u4f60\u5c31\u50cf\u5728\u6211\u4eec\u9762\u524d\u6253\u5f00\u4e00\u5e45\u753b\uff0c\u5e76\u544a\u8bc9\u6211\u8be5\u5982\u4f55\u6b23\u8d4f\u5b83\uff0c\u8c22\u8c22\uff01", "comment_date": "2018-10-19T08:07:31Z", "likes_count": 0}, {"comment_by": "Anti Dote", "comment_text": "FINALLY GRASPED THE IDEA OF NEURAL NETWORKS! <b>Cough</b> <b>Cough</b> What I meant to say was I am in huuge debt to you Grant (Sensei)", "comment_date": "2018-10-17T10:33:42Z", "likes_count": 0}, {"comment_by": "callmewisdom", "comment_text": "As someone who has written a thesis about this topic and struggeled to describe it in an understandable form I can&#39;t even stress enough how excelent this video ist. Also I don&#39;t even want to think about how long it must have taken to make. Great work!", "comment_date": "2018-10-16T11:13:52Z", "likes_count": 0}, {"comment_by": "Samuel Reed", "comment_text": "These videos are unbelievably well produced. Thank you so much for your effort. You&#39;ve made this topic incredibly clear and I cannot understate how much I appreciate the amount of effort you put into these. You have incredible talent as a teacher.", "comment_date": "2018-10-14T19:15:12Z", "likes_count": 15}, {"comment_by": "Roos B", "comment_text": "Best series on YouTube ever, great job!! It does not only explain the idea but really digs deep into the mechanisms. On top of that it is so well explained. Absolutely loved it, thanks for making this series!", "comment_date": "2018-10-13T21:23:59Z", "likes_count": 0}, {"comment_by": "Amirarsalan Rajabi", "comment_text": "This channel should be nominated for the best youtube channel of all time", "comment_date": "2018-10-11T19:15:31Z", "likes_count": 0}, {"comment_by": "Marco Trevisan", "comment_text": "While wrapping my head on this, I made a pdf trying to fomalize all the passages in detail, hopefully it&#39;s correct! If anyone wants to have it it&#39;s here: <a href=\"https://www.dropbox.com/s/y2bblssfdupck5f/NeuralNetworks.pdf?dl=0\">https://www.dropbox.com/s/y2bblssfdupck5f/NeuralNetworks.pdf?dl=0</a> ...DISCLAIMER: I do not claim that those equations are correct, feel free to point me to any mistakes!", "comment_date": "2018-10-11T14:14:51Z", "likes_count": 1}, {"comment_by": "Marco Trevisan", "comment_text": "I&#39;ll be almost surely wrong, but are you sure the indices in the second formula (timestamp about <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m30s\">9:30</a>) are right? Shouldn&#39;t the summation be over another dummy index, say &quot;h&quot;, since j should stay &quot;fixed&quot;? I would guess (in pseudo Latex) \\sum_h w_{hj}^{(l+1)}\\sigma&#39;(z_h^{(l+1)})\\frac{\\partial C}{\\partial a_h^{(l+1)}}", "comment_date": "2018-10-09T20:13:46Z", "likes_count": 0}, {"comment_by": "nameless0f", "comment_text": "most notations represent the bias as a (weight x input), where the weight is always 1, for a cleaner notation", "comment_date": "2018-10-09T07:17:10Z", "likes_count": 0}, {"comment_by": "itaybm2", "comment_text": "This content is amazing and very helpful, thank you very much!", "comment_date": "2018-10-08T06:30:12Z", "likes_count": 0}, {"comment_by": "Sujith Ishtar", "comment_text": "Amazing work on the video, by the way! It&#39;s obvious how much effort you put into your work, it clearly shows!", "comment_date": "2018-10-08T04:30:26Z", "likes_count": 0}, {"comment_by": "Nadezhda Dimitrova", "comment_text": "Thank you for this. The 4 videos have been spectacular. I plan on accessing that guy&#39;s book that you recommended in the 2nd episode :)", "comment_date": "2018-10-03T14:39:38Z", "likes_count": 0}, {"comment_by": "Abhishek Shetty", "comment_text": "this explanantion of Backpropagation calculus is legen......wait for it ......daryyyyyyy", "comment_date": "2018-10-02T15:12:34Z", "likes_count": 0}, {"comment_by": "Joey Ortiz", "comment_text": "I think video this shows how to build a graphical (nodes and edges) model to help with chain rule, but it doesn&#39;t really help with understanding how the backpropogation algorithm efficiently computes the gradient. You should consider a video on forward and reverse mode differentiation. Thanks as always for your amazing videos. <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\">https://en.wikipedia.org/wiki/Automatic_differentiation</a>", "comment_date": "2018-10-02T08:20:22Z", "likes_count": 0}, {"comment_by": "robin vermillion", "comment_text": "nice job! You make the  mathematics of machine learning alot easier to understand.", "comment_date": "2018-10-01T21:14:09Z", "likes_count": 0}, {"comment_by": "Tanmay N", "comment_text": "Huh you lost me at gradient vector <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=5m28s\">5:28</a>  :/", "comment_date": "2018-10-01T07:56:33Z", "likes_count": 0}, {"comment_by": "Zakaria Bahbaz", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m10s\">8:10</a> is where i stop understanding what&#39;s going on. Are you saying to take the derivative of the cost function with respect to the previous neuron multiple ways and add?<br>EDIT:<br>Just came back and watched it the next day and the realization of what&#39;s going on hit me like a truck", "comment_date": "2018-09-30T05:39:54Z", "likes_count": 0}, {"comment_by": "TCC123", "comment_text": "I saw a 6.5 minute ad (related to AI) in order to support you.<br>Keep up!", "comment_date": "2018-09-27T14:11:31Z", "likes_count": 3}, {"comment_by": "Pankaj Pundir", "comment_text": "the best explanation for the back propagation. Got a clear concept of it.", "comment_date": "2018-09-22T16:54:53Z", "likes_count": 0}, {"comment_by": "Maxim Lopin", "comment_text": "In a 3 layer network, after doing the chain rule i get a bunch of matrices, and how to a multiply them out? they have different dimensions since input layer is 10x1,  hidden 15x1 and output 10x1, and i guess it has to to with transpose matrices and i need to multiply some of the matrices with same dimensions element wise and some of them with dot product after transposing one of them, but how do i know what exactly to do, and what&#39;s the intuition about it? why is transposing one of matrices is not like cheating? it&#39;s like &quot;dimensions don&#39;t match so i will just flip the matrix and it all works now just fine&quot;? thanks for any help.", "comment_date": "2018-09-21T07:45:23Z", "likes_count": 0}, {"comment_by": "Paolo", "comment_text": "Literally the only youtube channel, that makes studying 2 hours of math, go by in a blink of an eye &lt;3 Thank you so much :D", "comment_date": "2018-09-19T22:19:34Z", "likes_count": 14}, {"comment_by": "danny iskandar", "comment_text": "take time to digest oh making a pathway to my graycells ..probably 3 more times.", "comment_date": "2018-09-19T01:16:29Z", "likes_count": 0}, {"comment_by": "Juan Pablo Carrillo", "comment_text": "How do you avoid getting stuck in a local minimum? Would you need non linear optimization?", "comment_date": "2018-09-18T04:24:24Z", "likes_count": 0}, {"comment_by": "Sachin Kalsi", "comment_text": "Yes, this is THE BEST explanation I have found for BP so far. Sir, can you please do some videos on RNN &amp; How BP works in RNN ? Pleaseeeee sir", "comment_date": "2018-09-18T02:57:08Z", "likes_count": 0}, {"comment_by": "Aman", "comment_text": "This video is a gem", "comment_date": "2018-09-17T02:28:23Z", "likes_count": 0}, {"comment_by": "RIZAL", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=7m20s\">7:20</a> why use square not absolute?", "comment_date": "2018-09-16T10:04:07Z", "likes_count": 0}, {"comment_by": "Santiago Dom\u00ednguez Collado", "comment_text": "i come from Andrew Ng&#39;s course, thanks for your videos, they helped me even more than videos of Coursera.", "comment_date": "2018-09-12T18:08:37Z", "likes_count": 0}, {"comment_by": "Abhishek Nigam", "comment_text": "Amazing series! sooo good", "comment_date": "2018-09-09T17:20:55Z", "likes_count": 0}, {"comment_by": "sbuvaneish", "comment_text": "Great Job :)", "comment_date": "2018-09-08T21:41:13Z", "likes_count": 0}, {"comment_by": "K. Ali Pardhan", "comment_text": "thanks :)", "comment_date": "2018-09-08T19:06:58Z", "likes_count": 0}, {"comment_by": "kumar Prateek", "comment_text": "You are the real deal man!!!\ud83d\udc4d\ud83d\udc4d", "comment_date": "2018-09-08T17:20:45Z", "likes_count": 0}, {"comment_by": "Bri Schnurz", "comment_text": "Is ist possible to provide german subtitels in Part 2 and 4 ?", "comment_date": "2018-09-05T16:42:55Z", "likes_count": 0}, {"comment_by": "reza mohajer", "comment_text": "That was totally comprehensive and perfect to me.<br>Thanks guys..", "comment_date": "2018-09-05T10:56:57Z", "likes_count": 1}, {"comment_by": "Bevel", "comment_text": "Thanks, man! This actually makes sense to me! One question though, what do you subtract the weight that you need to nudge by? Or better yet, how much do you nudge the weight. Do you subtract it by the derivative of that weight with respect to the neural network&#39;s cost? Thanks!", "comment_date": "2018-08-30T21:47:50Z", "likes_count": 0}, {"comment_by": "Hans G", "comment_text": "I watched this at 0.75 speed and took lots of notes.", "comment_date": "2018-08-30T11:54:10Z", "likes_count": 0}, {"comment_by": "Davide Riva", "comment_text": "First of all, I really appreciate this series of videos about Neural Networks.<br>I wanted to contribute with a small donation (1/5 euros) but doing so trough Patreon is a bit tricky and I can&#39;t afford a monthly donation. Consider adding a &quot;paypal donation&quot; link :)<br>Sorry for my English but I&#39;m not native", "comment_date": "2018-08-28T11:50:28Z", "likes_count": 0}, {"comment_by": "SinthTeck", "comment_text": "You. Are. Awesome.<br>My professor&#39;s notes were a mess, and I really couldn&#39;t visualize the meaning of all those derivatives! You made it crystal clear! Thanks a lot!", "comment_date": "2018-08-27T15:34:45Z", "likes_count": 0}, {"comment_by": "Ashish Gupta", "comment_text": "beautiful WHAAT AN EXPLANATION !!!!! Hats off", "comment_date": "2018-08-26T13:51:38Z", "likes_count": 0}, {"comment_by": "Craiu", "comment_text": "I am sitting here watching this video and am just like.... no way... wow.. amazing! :D", "comment_date": "2018-08-25T20:03:45Z", "likes_count": 0}, {"comment_by": "Pyralia", "comment_text": "Wow I&#39;m a weird person, I barely understood this series until I watched this last video =) I guess I understand calculus notation stuff better than pictures lmao<br>However, I&#39;m still wondering how exactly each change is applied. I know that some changes in each weight and bias is big or small according to the gradient of the cost function, but that gives a ratio of one change to the next. I&#39;m assuming it&#39;s a method like Newton&#39;s method for finding roots?", "comment_date": "2018-08-25T06:47:39Z", "likes_count": 0}, {"comment_by": "Adam Zahran", "comment_text": "I keep coming back to this video and every time I watch it I want to press the like button again but unfortunately you can only like once :(", "comment_date": "2018-08-24T14:12:27Z", "likes_count": 0}, {"comment_by": "Thomas", "comment_text": "clearly the best video out there to understand BP", "comment_date": "2018-08-23T23:21:24Z", "likes_count": 0}, {"comment_by": "Nicolas B", "comment_text": "Thanks for these 4 videos! Could you advise how to move forward once this is done, understood, and we have read <a href=\"http://neuralnetworksanddeeplearning.com/\">http://neuralnetworksanddeeplearning.com/</a> ? You spoke briefly about conventional NN and LSTM ?", "comment_date": "2018-08-23T15:02:52Z", "likes_count": 0}, {"comment_by": "D. K.", "comment_text": "So we&#39;ve computed the best direction to step. How much do we step? Some small amount, then look at the next layer back, until we&#39;re done? Then start again?", "comment_date": "2018-08-23T13:42:03Z", "likes_count": 0}, {"comment_by": "Derek Sands", "comment_text": "Can you do a video explaining the step by step math behind hash algorithms? Like MD5 or SHA1. Also, an RSA or any  symmetric key video would be really cool to see.", "comment_date": "2018-08-23T04:08:37Z", "likes_count": 0}, {"comment_by": "Rob", "comment_text": "But how do you update the weights and biases? How do you use the calculated ratios?", "comment_date": "2018-08-18T15:14:14Z", "likes_count": 0}, {"comment_by": "Vinita Baniwal", "comment_text": "Awesome explanation , great !!", "comment_date": "2018-08-16T10:55:49Z", "likes_count": 0}, {"comment_by": "phuc nguyen van", "comment_text": "please make video for convolution neural network. Great chanel", "comment_date": "2018-08-15T08:33:31Z", "likes_count": 0}, {"comment_by": "Yunus Emre \u00c7atal\u00e7am", "comment_text": "Masterpiece", "comment_date": "2018-08-10T19:53:14Z", "likes_count": 0}, {"comment_by": "Josip Pu\u0161i\u0107", "comment_text": "10Q very much!", "comment_date": "2018-08-09T13:05:56Z", "likes_count": 0}, {"comment_by": "Shulong Liu", "comment_text": "very impressive", "comment_date": "2018-08-02T16:43:50Z", "likes_count": 0}, {"comment_by": "niintyhma", "comment_text": "Great video! I was inspired by this so i coded a simple neural network from scratch with one hidden layer to recognize ones and zeros :) The math was explained very intuitively but at the same time rigorously. I had to actually use different learning rates for different layers to get results so remember kids: learning rate matters! These videos are great entry point to concepts of deep learning. I always thought there would be more difficult math involved but basic matrix algebra and derivation skills is all you need to get started.", "comment_date": "2018-07-30T22:33:42Z", "likes_count": 0}, {"comment_by": "arthurparis1", "comment_text": "Is it possible to use the Lagrangien method to minimize the cost function?", "comment_date": "2018-07-30T08:58:37Z", "likes_count": 0}, {"comment_by": "santiago calvo", "comment_text": "So im new to neural networks and also pretty ignorant towards math, but i would love to learn how to use neural network mostly out of curiosity, where would you say i should start?", "comment_date": "2018-07-30T05:06:24Z", "likes_count": 0}, {"comment_by": "Vectozavr", "comment_text": "That is the reason for learning the math! To understand such a beautiful things! That is awesome! Thank&#39;s a lot!!!", "comment_date": "2018-07-29T15:56:37Z", "likes_count": 304}, {"comment_by": "Brock", "comment_text": "Flamingtons.", "comment_date": "2018-07-28T22:01:51Z", "likes_count": 0}, {"comment_by": "Lukas Maier", "comment_text": "The video helped me very much! One question: At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=7m30s\">7:30</a> the sum goes from j=0 to n(L-1). Why n(L-1)? Shouldn&#39;t the length of the sum be dependent of the number of neurons in the Lth layer?", "comment_date": "2018-07-28T21:14:09Z", "likes_count": 0}, {"comment_by": "ya boi", "comment_text": "that actually made sense wow", "comment_date": "2018-07-27T19:26:31Z", "likes_count": 0}, {"comment_by": "Adolphe NDAGIJIMANA", "comment_text": "Best video, very helpful. Could you add another showing the matrix multiplication involved? Thanks.", "comment_date": "2018-07-27T17:55:12Z", "likes_count": 0}, {"comment_by": "Marc Normandin", "comment_text": "This is presented well and very easy to understand.", "comment_date": "2018-07-26T23:22:16Z", "likes_count": 0}, {"comment_by": "thatwillbe", "comment_text": "Love these visual explanations!", "comment_date": "2018-07-26T13:44:19Z", "likes_count": 0}, {"comment_by": "pritesh pandey", "comment_text": "This is the best explanation of backpropagation I came across. Thanks a lot.", "comment_date": "2018-07-24T18:45:47Z", "likes_count": 0}, {"comment_by": "Niko Yochum", "comment_text": "Is this series going to be continued?", "comment_date": "2018-07-22T03:57:17Z", "likes_count": 0}, {"comment_by": "Moartem S", "comment_text": "So backpropagation expresses all the derivitives by chain rule and calculates the components recursivly. Basicly it&#39;s a clever way of calculating a sum.<br><br>Very insightful video.", "comment_date": "2018-07-20T21:29:58Z", "likes_count": 0}, {"comment_by": "Sandeep Karkhanis", "comment_text": "any chance you would consider explaining Convolutional NN,. RNN / LSTM, Capsule Networks in your exemplary style? pretty please", "comment_date": "2018-07-19T21:46:29Z", "likes_count": 0}, {"comment_by": "Charan Reddy", "comment_text": "Im an atheist but learning neural networks makes me want to believe in GOD", "comment_date": "2018-07-19T10:13:59Z", "likes_count": 0}, {"comment_by": "Vlad-Florian Ochiule\u021b", "comment_text": "There is something that I didn&#39;t understand. For the last layer you can calculate the cost, but it seems that the cost must be calculated for the previous layers as well. How do we know the &quot;y&quot;(necessary to calculate the cost) for those previous layers?", "comment_date": "2018-07-19T08:01:48Z", "likes_count": 0}, {"comment_by": "RocketSpecialist", "comment_text": "This is a very good video HOWEVER each output of the hidden layers are affecting the outputs and the part to backpropagate between hidden layer and input layer is more tricky since now the output of a hidden layer is also affacting all output layers. Would love to see a video follow up on that part", "comment_date": "2018-07-17T14:06:23Z", "likes_count": 1}, {"comment_by": "Pranay Singh", "comment_text": "we need more videos on CNN sir, please. This channel is extremely wondefull", "comment_date": "2018-07-17T07:52:53Z", "likes_count": 0}, {"comment_by": "Johan Ohyee", "comment_text": "Hi! At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m19s\">4:19</a>, shouldn&#39;t the derivative of the activation be 2 ( 1 - y ), and not be 2 ( a^L - y )? I&#39;m confused...", "comment_date": "2018-07-15T22:21:38Z", "likes_count": 1}, {"comment_by": "Ayushman Dash", "comment_text": "You are the best. My favorite channel on YouTube.", "comment_date": "2018-07-15T19:10:14Z", "likes_count": 0}, {"comment_by": "Aneesh Prasobhan", "comment_text": "Please make a video on Convolutional Neural Networks. Thanks", "comment_date": "2018-07-12T01:13:04Z", "likes_count": 0}, {"comment_by": "Jonathan Lastrilla", "comment_text": "just wow", "comment_date": "2018-07-10T06:30:10Z", "likes_count": 0}, {"comment_by": "Jacob Holyfield", "comment_text": "That&#39;s cool I learned a lot from this!", "comment_date": "2018-07-10T05:43:41Z", "likes_count": 0}, {"comment_by": "Maharshi Chakraborty", "comment_text": "Do a series for CNNs also, PLEASE! Thank you so much for this, it was the best explaination video for ANNs on the internet!", "comment_date": "2018-07-09T10:59:08Z", "likes_count": 0}, {"comment_by": "Ravisankar Varadarajan", "comment_text": "Thanks for the video. It helped me to understand the mechanisms underneath Neural Networks", "comment_date": "2018-07-08T18:45:07Z", "likes_count": 0}, {"comment_by": "Noor Alkhadhar", "comment_text": "i love your videos, thanks for all the time you put into them. you&#39;ve turned another &quot;math just isn&#39;t my thing&quot; person into a &quot;fuck yeah math!&quot; person. will you ever continue this series? i REALLY HOPE YOU DO. pls.", "comment_date": "2018-07-06T23:25:17Z", "likes_count": 0}, {"comment_by": "Long Nguyen-Vu", "comment_text": "I have a small question on the formula at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m34s\">9:34</a> , the expanded formula of the derivative of cost function C with respect to aj( l +1) at the bottom<br>It should be equal to 2(aj(l+1) - yj) instead of 2(aj(l) - yj) <br><br>It is symmetric to the formula at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=6m28s\">6:28</a><br><br>Please correct me if I was wrong. Thanks so much!", "comment_date": "2018-07-03T06:57:38Z", "likes_count": 0}, {"comment_by": "Mohammad Reza", "comment_text": "This 10 min video took me 45 minutes to watch", "comment_date": "2018-07-01T22:53:56Z", "likes_count": 1}, {"comment_by": "gsrini", "comment_text": "Which tool do you use to make these cool animations?", "comment_date": "2018-06-29T16:06:01Z", "likes_count": 0}, {"comment_by": "Thomas Bingel", "comment_text": "Very recommendable", "comment_date": "2018-06-27T04:42:18Z", "likes_count": 0}, {"comment_by": "Rebecca Humbarger", "comment_text": "Love these videos.  Do the pi guys have names?", "comment_date": "2018-06-25T18:14:20Z", "likes_count": 0}, {"comment_by": "jesse dampare", "comment_text": "Beautiful . Now the real fun is coding all this!", "comment_date": "2018-06-25T14:01:17Z", "likes_count": 0}, {"comment_by": "Vasilis Prantzos", "comment_text": "Your communication skills are amazing to say the least...", "comment_date": "2018-06-24T18:05:22Z", "likes_count": 0}, {"comment_by": "Spencer Kraisler", "comment_text": "Do calculate the dError dWeight, does it matter what pathway you take to get to the weight from the output layer?", "comment_date": "2018-06-24T08:01:36Z", "likes_count": 0}, {"comment_by": "bigun89", "comment_text": "I understand everything so far, including this video.  But it screams for a part 4 - and climbing through example code, pointing out what each section is doing in reference to everything talked about so far.  This video series (so far) has taught me so much about ANN.  Thank you!", "comment_date": "2018-06-22T18:27:24Z", "likes_count": 0}, {"comment_by": "castortoutnu", "comment_text": "Amazing video !<br>Too bad that how to adjust the weights according to the cost is not explained :/<br>Or maybe (probably to be honest) I&#39;m just not seeing the elephant in the room.", "comment_date": "2018-06-21T22:32:07Z", "likes_count": 0}, {"comment_by": "Christophe Poulin", "comment_text": "Excellent explanation for a not so trivial subject... Thanks!", "comment_date": "2018-06-21T18:16:34Z", "likes_count": 0}, {"comment_by": "Thoughts, Ideas, and what?", "comment_text": "I&#39;ll keep this short<br><br>1) Thanks!<br>2) In other videos I see that Co= E( Target - Calculated Result) ^2.\u00a0<br>-- In this video I see its the reverse, E( CalRes - Target)^2.<br><br>3) What &quot;Y&quot; do you use for the a(L-1) iteration?<br>-- For a(L)-subJ you use Y-subJ.<br>-- How do you determine Y-subK for a(L-1)-subK layer so we can adjust the W(L-1) weights?", "comment_date": "2018-06-20T02:02:25Z", "likes_count": 0}, {"comment_by": "Albert Borr\u00e0s Pons", "comment_text": "Just amazing, fantastic job!", "comment_date": "2018-06-19T14:48:11Z", "likes_count": 0}, {"comment_by": "k slm", "comment_text": "\u201cThe definition of genius is taking the complex and making it simple.\u201d\r - Albert Einstein<br><br>You are genius.", "comment_date": "2018-06-18T20:57:48Z", "likes_count": 2469}, {"comment_by": "Akshay", "comment_text": "Thank You!", "comment_date": "2018-06-18T15:22:10Z", "likes_count": 0}, {"comment_by": "Ilya Pukhov", "comment_text": "Although this video is very good, I still struggled without a numerical example, and here is one of the Stanford lectuers that helped me understand  (libk to exact time in the video) <a href=\"https://youtu.be/d14TUNcbn1k?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&amp;t=661\">https://youtu.be/d14TUNcbn1k?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&amp;t=661</a>", "comment_date": "2018-06-17T09:48:28Z", "likes_count": 0}, {"comment_by": "Shimon Doodkin", "comment_text": "while trying to understand this. I think it would be more understandable if taught from outputs to inputs. like you have those outputs: w of layer L btw the notation is L(last layer), L-1. ah and if there are multiple layers the notation is j connects to k. back to w(j,k) of layer L. for this, you need to calculate an average of a list fo derivates lets calculate a derivative. for this derivative, you need the difference of input and this the difference of change of w value", "comment_date": "2018-06-16T20:00:01Z", "likes_count": 0}, {"comment_by": "\u0412\u0441\u0435\u0432\u043e\u043b\u043e\u0434 \u0421\u0435\u0432\u043e\u0441\u0442\u044c\u044f\u043d\u043e\u0432", "comment_text": "Where do I have to donate to get lection about other types of neural network ?! It was the clearest lection about neural networks that I&#39;ve heard in last 10 years at all!", "comment_date": "2018-06-15T11:36:41Z", "likes_count": 0}, {"comment_by": "XXTominhoXX", "comment_text": "error at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m58s\">4:58</a>. Correct is: dC_0/da^(L) = -2(a-y).  Wrong in video: dC_0/da^(L) = 2(a-y)", "comment_date": "2018-06-13T23:30:01Z", "likes_count": 1}, {"comment_by": "Shaunak Baradkar", "comment_text": "Awesome man!! Please do more :) Will surely support you once I get a job ;)", "comment_date": "2018-06-13T19:15:48Z", "likes_count": 0}, {"comment_by": "Andrew Lazy", "comment_text": "That is beautiful, but what doesn&#39;t seem to be comprehensible is How you determine the y (which the target value) for a neuron in a hidden layer while doing back prop. :(((", "comment_date": "2018-06-04T15:30:46Z", "likes_count": 0}, {"comment_by": "Daniel Krau\u00dfer", "comment_text": "Amazing videos! Looking forward to the next video in this series! :)", "comment_date": "2018-06-03T15:19:44Z", "likes_count": 0}, {"comment_by": "Israel Miller", "comment_text": "Incredible", "comment_date": "2018-06-02T02:36:50Z", "likes_count": 0}, {"comment_by": "Siddhartha Kumar Singh", "comment_text": "woahh....that was realllly amazingg.", "comment_date": "2018-05-30T12:19:59Z", "likes_count": 0}, {"comment_by": "Johnny Vaughan", "comment_text": "Looks like someone likes infinitesimals more than limits! (I do too! \ud83d\ude06)", "comment_date": "2018-05-28T17:24:30Z", "likes_count": 0}, {"comment_by": "Suvir Misra", "comment_text": "brilliant as usual. Have you done one on LSTM? How do you make these exciting visuals?", "comment_date": "2018-05-25T22:22:46Z", "likes_count": 0}, {"comment_by": "Mike g", "comment_text": "Very well done.", "comment_date": "2018-05-25T11:12:12Z", "likes_count": 0}, {"comment_by": "Mary Angelica", "comment_text": "These videos are awesome.  I&#39;m currently working on teaching myself mathematical underpinnings of machine learning, coming from a pure math background, but I have less of a computational one, and it gave me a good start of the general picture behind what is going on.  Thank you so much!", "comment_date": "2018-05-25T00:44:16Z", "likes_count": 0}, {"comment_by": "cowcannon", "comment_text": "Neural networks have layers, ogres have layers<br>Shrek is an AI confirmed", "comment_date": "2018-05-24T21:48:41Z", "likes_count": 669}, {"comment_by": "John Anselmo", "comment_text": "Hey!!! I&#39;m just wondering if you&#39;re going to be doing more machine learning videos. I love them  a lot, I found it hard to find videos and books that explained it in an understandable way.", "comment_date": "2018-05-24T01:58:53Z", "likes_count": 0}, {"comment_by": "Archsage Rob", "comment_text": "I came across your channel by chance, and Im so happy I did. I took a course on machine learning back in college, and while I did float by, I never really had a strong understanding of the mechanisms behind the math. You&#39;ve broken everything down incredibly well!", "comment_date": "2018-05-23T12:43:36Z", "likes_count": 0}, {"comment_by": "Johnny Han", "comment_text": "Any plans on making new videos regarding deep learning? there are many mathematical/probabilistic/statistic concepts that can be only found in formulas and confusing diagrams... like HMM,MCMC,MCTC,", "comment_date": "2018-05-22T11:26:16Z", "likes_count": 0}, {"comment_by": "michael scheinfeild", "comment_text": "really great chain rule", "comment_date": "2018-05-20T08:12:43Z", "likes_count": 0}, {"comment_by": "Ralph Dias", "comment_text": "I really enjoyed this playlist. thanks a ton!", "comment_date": "2018-05-18T16:42:22Z", "likes_count": 0}, {"comment_by": "hamid khb", "comment_text": "you teach better that 90% of every prof that i ever had. keep the project going. is there any where to donate?", "comment_date": "2018-05-18T12:51:13Z", "likes_count": 0}, {"comment_by": "cria", "comment_text": "I see that there is a negative exp formula for viewers of this series, but please take on consideration that we&#39;re the best of your viewers and keep doing more videos about neural networks.", "comment_date": "2018-05-15T11:15:05Z", "likes_count": 0}, {"comment_by": "Khariton Gorbunov", "comment_text": "pls make more<br>sooo good", "comment_date": "2018-05-14T16:08:56Z", "likes_count": 0}, {"comment_by": "EMov", "comment_text": "those series are motivating me to live, they are my daily joy doses &lt;3 Thanks a lot!", "comment_date": "2018-05-14T12:42:13Z", "likes_count": 0}, {"comment_by": "Alexander Cameron", "comment_text": "Awesome video", "comment_date": "2018-05-14T12:05:17Z", "likes_count": 0}, {"comment_by": "Julien Raffaud", "comment_text": "that vocal fry", "comment_date": "2018-05-11T23:16:46Z", "likes_count": 0}, {"comment_by": "Haave", "comment_text": "Well.. I hate math and refuse to understand it. I think that I get how this is working, you explained it really great. Now I know what is intended to happen with all this magical math stuff, so thank you a lot for those videos you made.", "comment_date": "2018-05-10T20:12:42Z", "likes_count": 0}, {"comment_by": "Ashish", "comment_text": "make py program of nueral networks .. plz", "comment_date": "2018-05-10T14:30:09Z", "likes_count": 0}, {"comment_by": "zinwa lin", "comment_text": "great channel.", "comment_date": "2018-05-09T10:20:57Z", "likes_count": 0}, {"comment_by": "Vasishta Polisetty", "comment_text": "I am a medstudent who left math back in class 10. The fact that I could at least remotely follow till the end of this video series  just shows how you have tremendous pedagogic skill. Great work!", "comment_date": "2018-05-06T14:14:48Z", "likes_count": 0}, {"comment_by": "Hepichack", "comment_text": "That was fucking awesome. Maths with visualization. Hope some day all teachers in the world adopt that teaching way.", "comment_date": "2018-05-05T17:42:01Z", "likes_count": 0}, {"comment_by": "1 2", "comment_text": "Why does the Sum for the Cost go to n^(L-1) and not just n^L? @<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=7m25s\">7:25</a>", "comment_date": "2018-05-03T00:43:16Z", "likes_count": 0}, {"comment_by": "Prashant Singh", "comment_text": "Great use of chain rule of differentiation.", "comment_date": "2018-05-02T16:32:26Z", "likes_count": 0}, {"comment_by": "Hugh March", "comment_text": "watched this about 10 times ;) couldn&#39;t keep track", "comment_date": "2018-05-01T22:37:35Z", "likes_count": 0}, {"comment_by": "lazeeboiii", "comment_text": "This was a great explanation.  Follow up question: So this video explains how to find the gradient for each weight and bias in the neural network.  So then is the update rule for any of those weights or biases, say w:  w = w - gradient * learning rate.", "comment_date": "2018-04-30T19:13:01Z", "likes_count": 0}, {"comment_by": "Yunfei Chen", "comment_text": "a little bit more complicated??? With 16 different networks and 4 different layers you will end up with 13000 terms in your equation and that would be a pain to derive just saying????", "comment_date": "2018-04-29T22:14:43Z", "likes_count": 0}, {"comment_by": "Juan Pablo Villaseca", "comment_text": "My firstborn shall be named 3blue1brown in your honor", "comment_date": "2018-04-26T01:44:34Z", "likes_count": 0}, {"comment_by": "AkaVibez", "comment_text": "I really appreciate the work you put in your videos. The plots and animations look amazing. It is great to see people on this platform focusing on giving knowledge to others instead of distributing random nonsens. Please keep it up, i think your work is really important and matters a lot. Thank you", "comment_date": "2018-04-24T07:01:14Z", "likes_count": 0}, {"comment_by": "L B", "comment_text": "I used this model to implement my neural net. what should be my range of weights and biases. and should my biases be initialized randomly as well. My range is for both [-1,1].", "comment_date": "2018-04-22T13:43:34Z", "likes_count": 0}, {"comment_by": "Jinming Tang", "comment_text": "I benefit a lot from your short series of deep learning. Now when I go further in this field, I also hope you can provide further courses in this topic. For example, except for the gradient descend, what is the details of other optimizer as Adam, Adagrad? How do they calculate the loss, especially in complex neural network as GoogLeNet? Thank you for your excellent work!", "comment_date": "2018-04-21T14:55:30Z", "likes_count": 0}, {"comment_by": "Brahma Reddy", "comment_text": "THE best explanation, waiting for videos on RNN and LSTMs", "comment_date": "2018-04-21T02:06:50Z", "likes_count": 0}, {"comment_by": "sanyam jain", "comment_text": "what does 3Blue1Brown mean?", "comment_date": "2018-04-20T22:17:31Z", "likes_count": 0}, {"comment_by": "TheGothGaming", "comment_text": "I love you", "comment_date": "2018-04-17T13:36:31Z", "likes_count": 0}, {"comment_by": "TheGrimravager", "comment_text": "ah beautiful, I was trying to work this out on my own in the afternoon, but I lost concentration, so this video is great to help me finish the job", "comment_date": "2018-04-16T20:08:21Z", "likes_count": 0}, {"comment_by": "hemant yadav", "comment_text": "at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=1m54s\">1:54</a> the subscript of w and b should be (L-1).", "comment_date": "2018-04-13T02:02:23Z", "likes_count": 0}, {"comment_by": "warby579", "comment_text": "Fantastic video, but please use more vibrant colours to differentiate elements, it&#39;s quite difficult for colourblind people to see the differences between more muted colours like those you&#39;re using.", "comment_date": "2018-04-12T22:26:52Z", "likes_count": 0}, {"comment_by": "BloodManticore24", "comment_text": "Dear 3Blue1Brown, I really love your videos. Could you please also add a growing Neural Gas tutorial and Genetic Algorithms on the future?", "comment_date": "2018-04-10T15:25:55Z", "likes_count": 0}, {"comment_by": "Denis S.", "comment_text": "Great job, thank you!", "comment_date": "2018-04-10T11:58:10Z", "likes_count": 0}, {"comment_by": "Emma Nguyen", "comment_text": "One suggestion. Don&#39;t use k twice, e.g. for a number of training examples and indices of activation neuron in one layer. It will kill your eyes and burn your mind for a not necessary cause...- from a math major", "comment_date": "2018-04-10T09:38:36Z", "likes_count": 0}, {"comment_by": "snippletrap", "comment_text": "Please continue this series. I want to see how we get features like loops and edges.", "comment_date": "2018-04-08T21:02:19Z", "likes_count": 2}, {"comment_by": "Andrea Baro", "comment_text": "Great videos! Can we hope you would eventually make new ones for transfer functions other than sigmoid, for convolutional neural networks, and for long short term memory? thanks!", "comment_date": "2018-04-07T14:20:49Z", "likes_count": 0}, {"comment_by": "shivakumar doddamani", "comment_text": "All 4 videos good for revision to get more intuition but not to learn from scratch.. at least for me", "comment_date": "2018-04-07T11:42:26Z", "likes_count": 0}, {"comment_by": "akhilesh pandey", "comment_text": "I literally said to myself.. &#39;This is how you explain things!!&#39;<br>I can not thank you enough!!", "comment_date": "2018-04-06T23:35:58Z", "likes_count": 0}, {"comment_by": "LadyHangaku", "comment_text": "Can I marry you? I finally understood the meaning of derivation. Oh my, thank you so much! :D", "comment_date": "2018-04-06T15:41:55Z", "likes_count": 0}, {"comment_by": "Joe Wong", "comment_text": "WHAT AN AMAZING CHANNEL!!!! THANKS", "comment_date": "2018-04-05T20:22:14Z", "likes_count": 0}, {"comment_by": "ElMokhtar MOHAMED MOUSSA", "comment_text": "i love you", "comment_date": "2018-04-05T13:08:08Z", "likes_count": 0}, {"comment_by": "Gryk", "comment_text": "Really cleared up those back. prop. update rules. Thanks for the great visualizations.", "comment_date": "2018-04-03T14:00:40Z", "likes_count": 0}, {"comment_by": "Pier Lim", "comment_text": "Thank you, this really helped me to understand the calculus!", "comment_date": "2018-04-03T06:45:24Z", "likes_count": 0}, {"comment_by": "Akhilez", "comment_text": "AMAZING!!", "comment_date": "2018-04-02T19:12:48Z", "likes_count": 0}, {"comment_by": "Torr-Net", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m04s\">4:04</a> How I do this on the multi neurons example?<br>Thx in advance :)", "comment_date": "2018-04-01T08:57:12Z", "likes_count": 0}, {"comment_by": "Johanng2", "comment_text": "This is so amazing. The way you create intuitive understanding of incomprehensible things like derivatives is just genius.", "comment_date": "2018-04-01T06:27:20Z", "likes_count": 0}, {"comment_by": "Marc Oliv\u00e9", "comment_text": "Hello, very nice explanation. Does it make sense to use these neural networks that you explained for time-series data forecasting? Thanks.", "comment_date": "2018-03-31T22:32:38Z", "likes_count": 0}, {"comment_by": "Subrat Pati", "comment_text": "Have become a fan of your channel......Nobody can make this look so simple", "comment_date": "2018-03-29T02:54:29Z", "likes_count": 0}, {"comment_by": "AJTech", "comment_text": "Thank you so much! Helped me a lot to understand papers on this topic", "comment_date": "2018-03-24T22:26:04Z", "likes_count": 0}, {"comment_by": "Neelesh Gupta", "comment_text": "you are god....RESPECT !", "comment_date": "2018-03-24T13:29:03Z", "likes_count": 0}, {"comment_by": "Steph Fong", "comment_text": "how about do one for particle swarm algorithm?", "comment_date": "2018-03-23T03:19:12Z", "likes_count": 0}, {"comment_by": "jampk24", "comment_text": "I spent 3 minutes staring at the C0 typo in the derivative. Very helpful video series though. Thanks.", "comment_date": "2018-03-22T19:08:17Z", "likes_count": 1}, {"comment_by": "Swapnil Borse", "comment_text": "U have earned my respect <i>/\\</i>", "comment_date": "2018-03-22T06:13:17Z", "likes_count": 0}, {"comment_by": "Soumyodeep Dey", "comment_text": "Great video. Can you tell me what software you use to make the presentation?", "comment_date": "2018-03-21T08:38:43Z", "likes_count": 0}, {"comment_by": "\u738b\u752f", "comment_text": "So basically there is no &quot;backpropagation&quot; on my perspective. it&#39;s just that one weight/bias, no matter where it is inside the network, its contribution will always finally be carried to the output layer. Thank you 3blue1brown this is awesome!", "comment_date": "2018-03-21T06:05:11Z", "likes_count": 0}, {"comment_by": "Anil Panda", "comment_text": "What an excellent way to explain. Thanks for making this video. You&#39;ve helped me understand a complex concept by breaking it down to small chunks.", "comment_date": "2018-03-20T13:09:56Z", "likes_count": 0}, {"comment_by": "catrinaisahuman", "comment_text": "How does dz/dw = a? I&#39;m new to derivatives", "comment_date": "2018-03-20T05:45:36Z", "likes_count": 0}, {"comment_by": "shreyas shubham", "comment_text": "Lmaooo best!", "comment_date": "2018-03-17T20:04:53Z", "likes_count": 0}, {"comment_by": "Maharshi Sharma", "comment_text": "What&#39;s the procedure to get u make videos on more machine learning content? Because this was ridiculously amazing", "comment_date": "2018-03-17T14:46:48Z", "likes_count": 0}, {"comment_by": "Solomon Tesema", "comment_text": "I paused the video just to tell you how genius you are! Man you are genius! And your team too, if you have a team!!! Bravo!", "comment_date": "2018-03-16T12:17:43Z", "likes_count": 0}, {"comment_by": "\u738b\u4e8c", "comment_text": "awesome", "comment_date": "2018-03-14T00:47:50Z", "likes_count": 0}, {"comment_by": "Selase Kwawu", "comment_text": "This is how 21st teaching should look like. It feels like your work should be made a &quot;human right&quot;. Thank you.", "comment_date": "2018-03-08T17:06:41Z", "likes_count": 282}, {"comment_by": "SuckYourBone", "comment_text": "understanding math ho wto:break into <a href=\"http://parts.works/\">parts.works</a> every time.", "comment_date": "2018-03-04T22:11:19Z", "likes_count": 0}, {"comment_by": "amihart", "comment_text": "Why do we calculate the whole thing recursively and not all at once? I mean surely you could just represent the entire network as one big equation and take the derivative of that to computer the gradients rather than having to calculate each layer individually... or am I misunderstanding something.", "comment_date": "2018-03-04T20:04:53Z", "likes_count": 0}, {"comment_by": "Andrei Cristian Sohan", "comment_text": "Thank you for making this series! I managed to understand the concept and the steps necessary for this iterative process from watching your videos, with just very basic prior knowledge (mainly terminology and nothing else). Your videos are very well planned and I am amazed how you are able to document, plan and implement them in only 3 weeks. All your (almost) 1 million subscribers are well deserved. Thank you!", "comment_date": "2018-03-01T19:42:25Z", "likes_count": 0}, {"comment_by": "sureshkm", "comment_text": "I&#39;m just at 3.59s, but i could not resist myself to comment on it, you are so awesome! very well explained so far!", "comment_date": "2018-03-01T19:12:35Z", "likes_count": 0}, {"comment_by": "G\u00f6rkem Seven Vids", "comment_text": "What i&#39;m going to do with this dcost/dw values now? also how to calculate dc/da(L-1)?", "comment_date": "2018-02-28T15:39:38Z", "likes_count": 0}, {"comment_by": "Henry Parker", "comment_text": "I could kiss you right now. Im a physics/comp sci major who just took a machine learning class and I want to do research into using neural networks to solve the complicated differential equations that arise in physics, but I hit a wall when trying to dig deeper into the theory of neural networks. Your video has served as a great introduction to the notation that I needed to understand some of the papers I&#39;m looking through. You&#39;re an invaluable resource please never stop.", "comment_date": "2018-02-28T06:25:25Z", "likes_count": 0}, {"comment_by": "Kastriot Gashi", "comment_text": "Awesome is not enough for this channel!", "comment_date": "2018-02-27T20:29:16Z", "likes_count": 0}, {"comment_by": "imago", "comment_text": "about 1 million watched the first video, 200k watched this. That makes me top 20%!<br><br>I have a very high need for justifying my actions", "comment_date": "2018-02-26T11:10:01Z", "likes_count": 1}, {"comment_by": "Ibrahim alshubaily", "comment_text": "Thanks a million!", "comment_date": "2018-02-24T05:59:12Z", "likes_count": 0}, {"comment_by": "debbabi fares", "comment_text": "Thanks for the videos , but I only know MATLAB, could same one help", "comment_date": "2018-02-24T00:14:12Z", "likes_count": 0}, {"comment_by": "Saiphes", "comment_text": "I was confused at the difference between a &quot;bias&quot; and an &quot;activation&quot;. Thanks.", "comment_date": "2018-02-23T16:02:28Z", "likes_count": 0}, {"comment_by": "IslandRai", "comment_text": "This was incredibly helpful :&#39; )", "comment_date": "2018-02-22T23:42:18Z", "likes_count": 0}, {"comment_by": "iJonny10", "comment_text": "Can you do a video about Differentials? E.g. how can one substitute stuff like d^2/dx^2 or why is dx/dy = /(dy/dx)? Normally I understand the most of those little tricks, but it would help to get an intuition about what those Differentials actually are...", "comment_date": "2018-02-22T20:34:30Z", "likes_count": 0}, {"comment_by": "DK Bhardwaj", "comment_text": "Instead of so much complexity &quot;HUMAN NEURAL NETWORK&quot; stores the conception of numerals in 3D omnidirectional memory. So no matter how numerals are represented human brain neurons can recognize numerals = as discrete quantum. Problem is Mathematical model must be SIMPLEST WHOLE NUMERALS rather than constructed pixels, not complex as numerals are the easiest to learn for human brain. Even 6 month child starts recognizing numerals. But above tensor approach is too complex for such a simple problem.", "comment_date": "2018-02-22T17:39:20Z", "likes_count": 0}, {"comment_by": "sjh7132", "comment_text": "Is there any advantage to averaging the gradients over the training set, then making adjustments to the weights vs making micro adjustments to the weights after each training example?  It seems that both take approximately the same number of computations, but the averaging requires more storage.", "comment_date": "2018-02-22T05:35:15Z", "likes_count": 0}, {"comment_by": "Keith Siopes", "comment_text": "Dude, excellent work! One question: if I am working with continuous data (output and input) that can take on negative values or values greater than 1, would I just leave off the final activation function for the output layer? If not, please advise. Thank you!", "comment_date": "2018-02-20T23:52:52Z", "likes_count": 0}, {"comment_by": "Technolus", "comment_text": "Does relu have any advantage over the sigmoid?", "comment_date": "2018-02-20T19:06:03Z", "likes_count": 0}, {"comment_by": "Bro Lee", "comment_text": "1.1m people watched the first one, only 0.17m of them make it through.", "comment_date": "2018-02-19T23:10:04Z", "likes_count": 0}, {"comment_by": "bees toes", "comment_text": "im curious if ur going to make more videos about ML. please make more", "comment_date": "2018-02-17T08:53:45Z", "likes_count": 0}, {"comment_by": "Thomas Aribart", "comment_text": "Hello 3Blue1Brown ! I just finished Andrew Ng&#39;s course on machine learning on the coursera platform. An awesome course but I looked at his calculus for back propagation and he gets something different : Everything is similar when he calculates the delta vectors (dC/da) but then he seems to &quot;forget&quot; the sigmoid derivative term when implementing the gradient.<br>To me you seem to be right, but I did the exercises with his formulas and it worked quite well, so I&#39;m kinda confused :/ Thanks anyway for your awesome content !", "comment_date": "2018-02-15T19:51:37Z", "likes_count": 0}, {"comment_by": "Tristan T", "comment_text": "I would love it if you could do a video about convolutional neural networks.", "comment_date": "2018-02-15T12:48:27Z", "likes_count": 0}, {"comment_by": "sathya nukala", "comment_text": "You are really great", "comment_date": "2018-02-14T18:00:43Z", "likes_count": 0}, {"comment_by": "Atul Gupta", "comment_text": "Nice one to develop basic intuition behind backpropagation.", "comment_date": "2018-02-13T13:47:37Z", "likes_count": 0}, {"comment_by": "AJAY G", "comment_text": "&#39;w&#39; and &#39;a&#39; fall into the category of product rule during derivation, i don&#39;t know if that would make a difference or not.", "comment_date": "2018-02-11T18:34:46Z", "likes_count": 0}, {"comment_by": "AJAY G", "comment_text": "hey you&#39;ve done a small mistake when applying product rule in partial derivation of &#39;z&#39; with respect to &#39;w&#39; at the 5th minute", "comment_date": "2018-02-11T18:33:36Z", "likes_count": 0}, {"comment_by": "Venkatesh JK", "comment_text": "Well this is bloody brilliant, chuck all books and hook onto one such thing!", "comment_date": "2018-02-11T05:47:56Z", "likes_count": 0}, {"comment_by": "Jon Snow", "comment_text": "Very informative video! Enjoy every second of it!", "comment_date": "2018-02-10T11:42:48Z", "likes_count": 0}, {"comment_by": "Yah", "comment_text": "You are awesome, making such complicated subjects so clear. I could never thank you enough for all your great videos.", "comment_date": "2018-02-09T02:48:25Z", "likes_count": 0}, {"comment_by": "Stanley Anderson", "comment_text": "Thanks for this whole series. I was intimidated by this computer science perspective, but you made me feel so much better about the subject.", "comment_date": "2018-02-08T22:52:22Z", "likes_count": 0}, {"comment_by": "dkwroot", "comment_text": "At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m26s\">8:26</a> , he deleted the equation for the derivative of the cost with respect to the weights and replaced it with an equation for the previous activation layer. This confused me badly since I figured we we no longer using the derivative with respect to the weights for determining the gradient descent. This isn&#39;t the case though, he just wanted to show us how to determine the derivative of the previous layers activation so that we can continue the back propagation to the next hidden layer. It looks like we use the derivative of the cost function with respect to the weights that he gave us earlier for the multi-neuron too.<br><br>I really wish he just put the formula for the cost with respect to weights to the side instead of changing it, it confused me badly!", "comment_date": "2018-02-06T06:48:30Z", "likes_count": 0}, {"comment_by": "Maxiedubaka", "comment_text": "I am a 7th grader and I get this...\u200b", "comment_date": "2018-02-06T03:18:13Z", "likes_count": 0}, {"comment_by": "sjh7132", "comment_text": "If you were to watch what happens as these networks learn, do they tend to tune the strongest connections first, then when those are tuned work on the weaker ones?  It seems the ones that cause the largest slope get the most attention.", "comment_date": "2018-02-06T00:31:56Z", "likes_count": 0}, {"comment_by": "Nelson Elias Salinas", "comment_text": "Man, thank you so much, I have never had a course on machine learning, and now my PhD is on that and together with the coding train (from where I ended up in your channel) this has been so helpful. Reading books and such is useful, but this kind of videos are just awesome. Again thank you very much.", "comment_date": "2018-02-05T12:44:50Z", "likes_count": 0}, {"comment_by": "Alex Z", "comment_text": "In the end in the yellow square shouldn&#39;t it be (l-1) instead of (l+1) ?", "comment_date": "2018-02-04T19:07:28Z", "likes_count": 0}, {"comment_by": "Lp Xiv", "comment_text": "now i can visualize part of the massive computation behind the network.  Hope you can make more on neural network and deep learning.", "comment_date": "2018-02-04T13:26:30Z", "likes_count": 0}, {"comment_by": "Vid_sh_itsme", "comment_text": "Is this just the gradient descent in backpropagation???<br>Do other training algorithm (like levenberg Marquette, braysen regularization etc.) also work the same way??", "comment_date": "2018-01-31T18:47:05Z", "likes_count": 0}, {"comment_by": "Altun Hasanli", "comment_text": "You are the best! &lt;3", "comment_date": "2018-01-30T21:49:39Z", "likes_count": 0}, {"comment_by": "Ruslan Nikolaev", "comment_text": "Best explanation of gradient descent and chain rule i&#39;ve ever seen", "comment_date": "2018-01-30T01:15:07Z", "likes_count": 0}, {"comment_by": "Thomas Np", "comment_text": "The clearest explanatation that I never seen and heard so far. Congratulations for your work ! :)", "comment_date": "2018-01-29T10:16:17Z", "likes_count": 1}, {"comment_by": "Eniotna Yssaneb", "comment_text": "i didn&#39;t understand how do we exploit this result ?<br>one element remains missed", "comment_date": "2018-01-28T23:40:25Z", "likes_count": 0}, {"comment_by": "m s", "comment_text": "Just glancing at this video, this is how i imagine people see when viewing my code", "comment_date": "2018-01-28T21:54:15Z", "likes_count": 0}, {"comment_by": "\u7cbe\u9748\u9f20", "comment_text": "Thank you very much for the high-quality content.  I mean my passion on subjects related to maths is definitely ignited (and therefore makes me more willing to put efforts in maths).  Thank you very much!", "comment_date": "2018-01-28T04:20:41Z", "likes_count": 0}, {"comment_by": "Edward Antonian", "comment_text": "This is beautiful", "comment_date": "2018-01-27T21:59:56Z", "likes_count": 0}, {"comment_by": "Lucas Janos Martin", "comment_text": "When you find the cost you do the operation (a-y)^2 and you said the derivative to this is 2(a-y),<br>The function (a-y)^2 is the same as (y-a)^2 so why are the two derivatives to these different: ( 2a-2y and 2y-2a)?", "comment_date": "2018-01-27T19:58:28Z", "likes_count": 0}, {"comment_by": "Manni vtply", "comment_text": "Probably the most intuitive explanation on backprop that I&#39;ve seen so far. I can&#39;t thank you enough! haha", "comment_date": "2018-01-26T11:00:20Z", "likes_count": 0}, {"comment_by": "Werbung Schrott", "comment_text": "youre videos are great! you should monetarize them more heavily, you can easily fit 1 or 2 advertisments in the middle of the vid! keep on going, you rock!!", "comment_date": "2018-01-26T10:39:44Z", "likes_count": 0}, {"comment_by": "Adrian Gray", "comment_text": "so if I got it right the cost of previous neuron is the resulting gradient?", "comment_date": "2018-01-25T09:58:10Z", "likes_count": 0}, {"comment_by": "Martinius", "comment_text": "I can&#39;t say I understood all of it, but man you make some great videos!", "comment_date": "2018-01-24T23:32:01Z", "likes_count": 0}, {"comment_by": "exitudio", "comment_text": "I love you", "comment_date": "2018-01-23T05:08:55Z", "likes_count": 0}, {"comment_by": "anjopag31", "comment_text": "MIND<br>BLOWN", "comment_date": "2018-01-22T00:40:33Z", "likes_count": 0}, {"comment_by": "Xiaohu Zhu", "comment_text": "curious about the tools made this amazing visualization happen.. d3js?", "comment_date": "2018-01-20T09:01:27Z", "likes_count": 0}, {"comment_by": "Or Yedidia", "comment_text": "when calculating the change in cost in respect to the bias, which weight and previous activation do you consider when calculating z(L) on a multi neuron network?<br><br>I am trying to code a network for myself to learn, please help.", "comment_date": "2018-01-19T13:45:38Z", "likes_count": 0}, {"comment_by": "pablo richard", "comment_text": "Thank you so much for your explanations, you&#39;re channel is awesome !", "comment_date": "2018-01-19T09:53:46Z", "likes_count": 0}, {"comment_by": "Max H", "comment_text": "I&#39;m sorry but these are just the best nn videos available online.  A lot of effort seems to have been put into them.  Thanks man, appreciate it", "comment_date": "2018-01-18T13:09:20Z", "likes_count": 0}, {"comment_by": "Mark Perrin", "comment_text": "Well this series may have decided what I&#39;ll do for my Masters/Doctorate, it is actually that interesting!", "comment_date": "2018-01-18T06:08:32Z", "likes_count": 0}, {"comment_by": "Jonathan", "comment_text": "The thing is though, you never mention how you would minimise the weigh or the bias, you simply state it can be done using the derivatives. A few problems still remains, local minimums for example.", "comment_date": "2018-01-17T22:19:42Z", "likes_count": 0}, {"comment_by": "Ursula Major", "comment_text": "really love your videos on this subject; are you working on a chapter 4?", "comment_date": "2018-01-17T04:11:23Z", "likes_count": 0}, {"comment_by": "Arka\u00efd", "comment_text": "Those animations are candy for eyes... Your work is awesome. It will be incredibly useful for a science project I&#39;m currently working on", "comment_date": "2018-01-16T21:10:33Z", "likes_count": 0}, {"comment_by": "Dodi", "comment_text": "Goddammit! My jaw litterally dropped down for two good minutes for the intuition you gave me. I&#39;m getting the BP and there is no doubt that it&#39;s only because of you (...and me desiring to learn, of course). Thank you very much for all your work. Seriously.", "comment_date": "2018-01-15T21:32:24Z", "likes_count": 0}, {"comment_by": "Mugi", "comment_text": "i lost at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=5m15s\">5:15</a>", "comment_date": "2018-01-13T14:41:25Z", "likes_count": 0}, {"comment_by": "Miten Solanki", "comment_text": "which software you use to make these videos ?", "comment_date": "2018-01-12T14:00:35Z", "likes_count": 0}, {"comment_by": "Ostap Okhrin", "comment_text": "Brilliant !!! Thanks for these videos!", "comment_date": "2018-01-12T09:29:42Z", "likes_count": 0}, {"comment_by": "Dominik Kallusky", "comment_text": "Looks like the final formula given @<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m30s\">9:30</a> is the the derivative of the cost C0 for a single training sample only, right? To get the <i>total</i> cost C with respect to w(j,k) shouldn&#39;t it be an average across all the cost functions for all training samples?", "comment_date": "2018-01-12T00:34:55Z", "likes_count": 0}, {"comment_by": "Vincent Thacker", "comment_text": "This content is wonderful!  Looking forward to the rest of the series!", "comment_date": "2018-01-10T16:33:54Z", "likes_count": 0}, {"comment_by": "Ali Nawaz", "comment_text": "I love the visualisations that you use! How do you do those? Teach me maestro!", "comment_date": "2018-01-10T15:10:51Z", "likes_count": 0}, {"comment_by": "Anshul Kanakia", "comment_text": "I can&#39;t tell you how long I&#39;ve been trying to visualize all this in my head to get a solid mental picture of backpropagation... Well, I guess I can - it was the duration of a flight from Frankfurt to Seattle (about 9 hours) and it involved one terribly lit backside of an airplane menu and a shitty pencil. I am so grateful for the work you put into animating this algorithm. It has literally brought tears to my eyes and a smile on my face. Thank you.", "comment_date": "2018-01-09T21:30:44Z", "likes_count": 4}, {"comment_by": "green_mamba", "comment_text": "Excellent series of videos once more. 3Blue1Brown your explanations are awesome always. Now I got a grasp on what the fundamentals behind all this hype of neural networks is : ) By the way, I&#39;ve watch several of your videos and learned really a lot from them. I wish I&#39;d had a teacher like you back in my college days, that would have saved me a lot of headaches ! I&#39;ve always been also amazed by the graphics you use, they are just as if they ware inside of my head. I know it must take a lot of talent to develop this videos, but just out of curiosity, what software do you use to produce them (if it is ok to know)? Is it some custom software or something?", "comment_date": "2018-01-08T00:51:49Z", "likes_count": 0}, {"comment_by": "damcism", "comment_text": "Great work! I am reading a book about deep learning, but with these great visual demonstrations of the concepts that are being explained, it makes much easier to understand it all.", "comment_date": "2018-01-06T17:26:25Z", "likes_count": 0}, {"comment_by": "Manoel Machado", "comment_text": "This explanation is so clear. For me which have all the grads of calculus courses this was very easy to understand! Thanks for all your analogies during all the series of Deep Learning.", "comment_date": "2018-01-06T08:37:41Z", "likes_count": 0}, {"comment_by": "Cem Sazara", "comment_text": "Best explanation about backpropagation so far ! :)", "comment_date": "2018-01-05T23:57:34Z", "likes_count": 0}, {"comment_by": "Shashank Bansal", "comment_text": "This series is insanely good especially the animations and explanations. They are very intuitive. Thanks and well done.", "comment_date": "2018-01-05T02:47:53Z", "likes_count": 0}, {"comment_by": "lincong wang", "comment_text": "hello", "comment_date": "2018-01-05T01:53:43Z", "likes_count": 0}, {"comment_by": "Josue", "comment_text": "Great. Great. Super Great video. You got a new subscriber.", "comment_date": "2018-01-04T10:48:13Z", "likes_count": 0}, {"comment_by": "RTheThinker", "comment_text": "this series cleared a lot for me but one thing I still don&#39;t understand: a{L-1} * SigPrime( z{L}) * 2(a{L} - y) results in a vector. How could this update the weight matrix?", "comment_date": "2018-01-03T15:49:11Z", "likes_count": 0}, {"comment_by": "Clayton Harting", "comment_text": "This was a fantastic video series! I would love it if you made more on different forms of machine learning, such as K-Means, recommender systems, and anomaly detection. Thank you!", "comment_date": "2018-01-02T22:45:46Z", "likes_count": 0}, {"comment_by": "Joren Boulanger", "comment_text": "Thank you so much for this.", "comment_date": "2018-01-01T12:34:23Z", "likes_count": 0}, {"comment_by": "\u00f8", "comment_text": "For those who haven&#39;t taken multivariable calculus, a concept allowed me to better understand backpropagation is the &quot;total derivative,&quot; which given by the &quot;multivariable chain rule&quot;. This is specifically useful when you figure out \ud835\udf15C/\ud835\udf15a, but since you cannot nudge the activation directly, you must change the weights, biases, and activations of the layers before it. How you change these 3 things is very elegantly found using the chain rule/total derivative.<br><br>Grant has done a series on multivariable calculus with Khan Academy. Here is the video introducing the multivariable chain rule:\u00a0<a href=\"https://www.youtube.com/watch?v=NO3AqAaAE6o\">https://www.youtube.com/watch?v=NO3AqAaAE6o</a>, and here is the intuition behind it:\u00a0<a href=\"https://www.youtube.com/watch?v=hFvBZf-Jx28\">https://www.youtube.com/watch?v=hFvBZf-Jx28</a>. (Here is the same thing on the Khan Academy website:\u00a0<a href=\"https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/multivariable-chain-rule/v/multivariable-chain-rule)\">https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/multivariable-chain-rule/v/multivariable-chain-rule)</a>.", "comment_date": "2017-12-30T21:58:48Z", "likes_count": 0}, {"comment_by": "Saif B", "comment_text": "Thank you very much, your videos are amazing visually and content wise. Well done.", "comment_date": "2017-12-30T17:14:14Z", "likes_count": 1}, {"comment_by": "Stefan", "comment_text": "If u are having problems understanding this, go and watch coding trains video on gradient decent. I feel like that makes every thing more intuitive", "comment_date": "2017-12-30T16:46:55Z", "likes_count": 0}, {"comment_by": "Roy B", "comment_text": "this programming is sick", "comment_date": "2017-12-30T15:40:52Z", "likes_count": 0}, {"comment_by": "Aniket Tomar", "comment_text": "@3Blue1Brown It&#39;s been almost 2 months...<br>Is this the end of the series..?<br>I&#39;m sad", "comment_date": "2017-12-30T06:11:27Z", "likes_count": 0}, {"comment_by": "Bryan Chu", "comment_text": "Ok, that&#39;s it. I&#39;m gonna try to program this insanity.", "comment_date": "2017-12-28T17:24:36Z", "likes_count": 0}, {"comment_by": "Dennis Aprilla Christie", "comment_text": "BEST. EXPLANATION. EVER.", "comment_date": "2017-12-27T11:18:34Z", "likes_count": 2}, {"comment_by": "Shikha Shah", "comment_text": "Excellent explanation! I wish you could upload more videos on machine learning.", "comment_date": "2017-12-24T04:02:23Z", "likes_count": 1}, {"comment_by": "Carlos Quintero", "comment_text": "I&#39;m sure you&#39;ve heard it before but you are amazing. This makes me see how the calc and alg II i&#39;ve done can be applied irl.", "comment_date": "2017-12-23T07:24:36Z", "likes_count": 1}, {"comment_by": "Robert Shane", "comment_text": "How long would it take to implement this?", "comment_date": "2017-12-23T01:28:30Z", "likes_count": 0}, {"comment_by": "Desam1000", "comment_text": "I understand all the formulas, but I still don&#39;t understand how these formulas are applied by the algorithm.<br>If C0 is to be minimized, a lot of different weights and biases to be changed to do that. It seems to me like solving an equation with thousands of free variables.", "comment_date": "2017-12-21T20:05:12Z", "likes_count": 0}, {"comment_by": "Samuel Urban", "comment_text": "I just completed a multi-variable calculus class (and we reviewed matrix calculations, etc.) And I was able to follow along! Good job sir! (Now onto linear algebra...)", "comment_date": "2017-12-21T19:36:46Z", "likes_count": 0}, {"comment_by": "Stellus Ctf", "comment_text": "Excellent &#39;To the power of Infinity&#39; Sir ... in my whole life I NEVER seen an training video at this quality (content , presentation &amp; the thought process went into this!) you should be given  International &quot;Trainer of the century&quot; , I wish I had a professor like you", "comment_date": "2017-12-21T04:07:52Z", "likes_count": 0}, {"comment_by": "A. Rashad", "comment_text": "How do you know all of this. . .are you . . .A WITCH?", "comment_date": "2017-12-21T02:47:19Z", "likes_count": 0}, {"comment_by": "shenphysics", "comment_text": "really good and exciting. I can&#39;t wait for more uploads. Keep the good work.", "comment_date": "2017-12-21T00:20:44Z", "likes_count": 0}, {"comment_by": "kdawghomie", "comment_text": "Listen, I&#39;m sure you get a lot of comments on your videos so you might never see this. But you should know that you&#39;re awesome. Like, really fucking awesome. <br><br>It&#39;s been a decade since I learned calculus, and I&#39;m randomly relearning it for FUN right now, all because of you. Calculus. Fun? What?!!!", "comment_date": "2017-12-18T20:56:12Z", "likes_count": 0}, {"comment_by": "anjopag31", "comment_text": "Where should I learn the calculus required to understand this?", "comment_date": "2017-12-18T19:45:22Z", "likes_count": 0}, {"comment_by": "test", "comment_text": "To see how this works with actual numbers, which is what everybody is looking for in the end.<br>Computational graphs are a great way to see how everything fits together.<br><a href=\"https://www.youtube.com/watch?v=d14TUNcbn1k\">https://www.youtube.com/watch?v=d14TUNcbn1k</a>", "comment_date": "2017-12-18T13:39:15Z", "likes_count": 0}, {"comment_by": "dAvrilthebear", "comment_text": "Thank you for the video!<br>At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m24s\">8:24</a> how do you find the derivative of z(j) with respect to all the w(jk) ? Is it multivariable calculus?", "comment_date": "2017-12-17T23:47:15Z", "likes_count": 0}, {"comment_by": "Can Metan", "comment_text": "This sounds quite difficult I must say. Have to look at other resources to understand the calculus behind this.", "comment_date": "2017-12-17T10:08:03Z", "likes_count": 0}, {"comment_by": "Brian Emerson", "comment_text": "Whenever I look into any programming stuff, I come to the conclusion that I should do all the math by hand first, then program it all. If I&#39;m going to teach the computer how to do it hundreds of times, I should make sure I can do it myself.", "comment_date": "2017-12-17T09:13:25Z", "likes_count": 0}, {"comment_by": "Hanhyojoo", "comment_text": "any more series videos?", "comment_date": "2017-12-16T14:26:50Z", "likes_count": 0}, {"comment_by": "Ashwin mirskar", "comment_text": "Amazing series!! Helped me understand the exact math of back propagation better than anything else out there on youtube :) Thank You @3Blue1Brown", "comment_date": "2017-12-16T13:22:59Z", "likes_count": 0}, {"comment_by": "munkieluva", "comment_text": "This is truly one of the most helpful, clear, and well produced videos I have ever seen. Thank you.", "comment_date": "2017-12-15T21:11:06Z", "likes_count": 0}, {"comment_by": "Randy Sterbentz", "comment_text": "Where were you when my final project for a sophomore-level physics programming class was to create a neural network to recognize numbers!?  I still have slight hiccups in understanding three years later, but working through this has helped.", "comment_date": "2017-12-14T23:55:37Z", "likes_count": 0}, {"comment_by": "Ravi kumar", "comment_text": "these videos were awesome...!!  but where can we get more such videos on neural network to study more  ?", "comment_date": "2017-12-14T08:15:09Z", "likes_count": 0}, {"comment_by": "nukedkaltak", "comment_text": "This must have been the most gracious presentation of the chain rule in existence", "comment_date": "2017-12-13T19:14:40Z", "likes_count": 1}, {"comment_by": "Alexey Cherkaev", "comment_text": "Interesting how things are connected: watching this video made me realize that back propagation is called \u201creverse derivatives\u201d in automatic differentiation. It also shows clearly how efficient it is for cases \u201cmany inputs\u201d - \u201cfew outputs\u201d compared to forward derivatives!", "comment_date": "2017-12-13T14:43:13Z", "likes_count": 0}, {"comment_by": "Isaac Teg", "comment_text": "OH MY GOD IT FINALLY ALL MAKES SENSE. I have been trying to understand the back-prop algorithm for over a year thank you so much.", "comment_date": "2017-12-13T04:00:37Z", "likes_count": 0}, {"comment_by": "Vishnu Viswanath", "comment_text": "how come I didn&#39;t come across this channel before. This is amazing!", "comment_date": "2017-12-13T00:43:15Z", "likes_count": 0}, {"comment_by": "Xelod93", "comment_text": "I been looking for something like this to make myself things more clear, and i found the answer here on this channel, Thank you very very much for this! Got my sub, I&#39;ll follow on those awesome videos and explanations! Keep going!", "comment_date": "2017-12-11T20:38:34Z", "likes_count": 0}, {"comment_by": "--", "comment_text": "There is an error in formulas at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m35s\">9:35</a>. j is used in two different roles - dC/da(j)(l) = Sum(j = 0 to nl-1) {...}", "comment_date": "2017-12-10T21:38:36Z", "likes_count": 0}, {"comment_by": "Nikhil Sasi Rajan", "comment_text": "I hope more videos on the topic are on the way!", "comment_date": "2017-12-10T15:53:41Z", "likes_count": 0}, {"comment_by": "Brittny Freeman", "comment_text": "OMG...I feel like a F*ing Boss after both watching and understanding this!", "comment_date": "2017-12-10T01:01:15Z", "likes_count": 0}, {"comment_by": "Sam", "comment_text": "Great video, but i&#39;m still far away from understanding what exactly is and how to build a neural network on my own.<br>I wanna imagine it right but have to face both, translation from english to german and remembering themore complicated math-stuff correctly. So can someone help me pls? (I&#39;ll even share my cookie for ya D:)", "comment_date": "2017-12-09T19:33:46Z", "likes_count": 0}, {"comment_by": "x90slide", "comment_text": "I wonder why this didnt appear in my subscriptions.<br><br>Awesome stuff, keep it up!", "comment_date": "2017-12-09T16:17:01Z", "likes_count": 0}, {"comment_by": "x90slide", "comment_text": "I wonder why this didnt appear in my subscriptions.<br><br>Awesome stuff, keep it up!", "comment_date": "2017-12-09T16:16:53Z", "likes_count": 0}, {"comment_by": "FINX", "comment_text": "Hi, is there a way to describe infinity without using the infinity symbol?<br>(Mabey 0*0 or 0^0)", "comment_date": "2017-12-08T09:33:15Z", "likes_count": 0}, {"comment_by": "Junaid", "comment_text": "Calculus was easy easy to understand!", "comment_date": "2017-12-08T05:16:53Z", "likes_count": 0}, {"comment_by": "raunaque patra", "comment_text": "will there be a video on Backpropagation in CNN .Its very difficult to find a good resource which derives and explains the formulas in CNN back prop", "comment_date": "2017-12-07T21:12:17Z", "likes_count": 0}, {"comment_by": "BlauerTeeTec", "comment_text": "It would be fantastic if you could do a series like &quot;The essence of discrete mathematics&quot; It&#39;s such a fundamental concept of almost everything math is about, but still so hard to wrap your mind around. I could use some of your beautiful visualisations :D", "comment_date": "2017-12-07T18:42:30Z", "likes_count": 0}, {"comment_by": "Math Guy Dan", "comment_text": "Wait a second, why does this sound like a least squares problem???", "comment_date": "2017-12-07T07:24:54Z", "likes_count": 0}, {"comment_by": "Joel Dixton", "comment_text": "1) The background music is unobtrusive and pleasant<br>2) The content has had work put into and is comprehensive, with nice visuals<br>3) The person making the video doesn&#39;t think he&#39;s funny (looking at you siraj)<br><br>Never considered patreon before but you&#39;re tempting me tbh. Thanks very much for this.", "comment_date": "2017-12-05T22:06:21Z", "likes_count": 0}, {"comment_by": "Kaleb Bruwer", "comment_text": "I understand some of this, but it&#39;s becoming clear to me that I need to learn a lot more calculus. I&#39;m working through that book you recommended and got derailed the moment he mentioned partial derivatives, not knowing what those are means all of this doesn&#39;t quite make sense to me.", "comment_date": "2017-12-05T17:22:23Z", "likes_count": 0}, {"comment_by": "Jill Valentine", "comment_text": "will u go through kernel method? it is a very important topic in machine learing <br>it can convert data form non-linear to linear", "comment_date": "2017-12-05T08:07:36Z", "likes_count": 0}, {"comment_by": "Sam Kirkiles", "comment_text": "Thank you so much for this video", "comment_date": "2017-12-03T19:29:07Z", "likes_count": 0}, {"comment_by": "FuckWind", "comment_text": "I LOVE YOU. THANK YOU SO MUCH. THIS HELPED ME SO MUCH AND JUST IN TIME FOR MY MACHINE LEARNING COURSE ON COLLEGE. THANKS.", "comment_date": "2017-12-03T16:25:11Z", "likes_count": 0}, {"comment_by": "Logan", "comment_text": "Since you love calculus , i thought you should do a video on pid (Proportional Integral Derivative) controllers", "comment_date": "2017-12-03T15:02:20Z", "likes_count": 0}, {"comment_by": "Julio Franco", "comment_text": "It would be nice a video with the explanation of the Fourier Transform! :D thanks for everything!", "comment_date": "2017-12-03T01:31:37Z", "likes_count": 1}, {"comment_by": "Rohan Anand", "comment_text": "The best explanantion!", "comment_date": "2017-12-02T15:55:05Z", "likes_count": 0}, {"comment_by": "Ju-young Lee", "comment_text": "Thank you sooooo much", "comment_date": "2017-12-01T14:17:33Z", "likes_count": 0}, {"comment_by": "pandaman", "comment_text": "uh hello hi uhm when ru gonna have more good stuff? i finished your product on the same day i got it. can\u2019t get enough. need more m(e || a)th...", "comment_date": "2017-12-01T05:33:53Z", "likes_count": 0}, {"comment_by": "Annemarie en Pieter Bresters - van Bragt", "comment_text": "machene learning is great, I have made programes that: see the difference between musical words and mathematical words, mimics the sin function and I&#39;ve tried to learn a program to do &quot;four in a row&quot; (but it wasn&#39;t very succesful)", "comment_date": "2017-11-30T17:53:09Z", "likes_count": 0}, {"comment_by": "Tyler Matthew Harris", "comment_text": "Are you still alive?", "comment_date": "2017-11-29T22:50:11Z", "likes_count": 0}, {"comment_by": "TheIppo1000", "comment_text": "This is awesome! <br>When will the next video of this series be out?", "comment_date": "2017-11-29T12:23:29Z", "likes_count": 0}, {"comment_by": "Winston Liu", "comment_text": "what is the difference between \u03c3(x) and \u03c3&#39;(x)", "comment_date": "2017-11-29T01:43:24Z", "likes_count": 0}, {"comment_by": "Aditya Dua", "comment_text": "Have you ever considered writing books on similar topics ?<br>I think you will be do a brilliant job. <br>Most of the authors out there lack the art of teaching through an intuitive sense, so their content seems really dry.<br>But from watching your videos, i think you could be a really good author of maths/computer science books.", "comment_date": "2017-11-28T07:39:48Z", "likes_count": 0}, {"comment_by": "Zack Kelvington", "comment_text": "I encountered a problem on my pre-calc homework that had the answer, x=logbase3 (-3). The answer is nonreal and after falling in love with your videos. I knew you could help me. Thanks.", "comment_date": "2017-11-28T03:38:01Z", "likes_count": 0}, {"comment_by": "Idan", "comment_text": "Can you please make some videos about Graph Theory ? I need your help, my master.", "comment_date": "2017-11-27T19:12:57Z", "likes_count": 0}, {"comment_by": "MD Saiful Islam", "comment_text": "Your videos are great specially the ones on calculus. Can you make a video explaining the differentiation under integral sign method? \ud83d\ude09", "comment_date": "2017-11-26T20:28:38Z", "likes_count": 0}, {"comment_by": "CO8izm", "comment_text": "I love your videos and your effort. You indeed make mathematics more joyful and intuitive for us students. Thank you for that. I would really love to see some similar material on numerical methods (Fourier transforms, convolution for example). Thank you again", "comment_date": "2017-11-26T12:07:56Z", "likes_count": 0}, {"comment_by": "Draktharr", "comment_text": "How do you choose the initial randomization of the weight, bias and neuron values before training your data?", "comment_date": "2017-11-24T18:51:47Z", "likes_count": 0}, {"comment_by": "Abu Sayed", "comment_text": "Your videos are really Awesome!! And please make some videos on visual Tensor algebra including general relativity etc.", "comment_date": "2017-11-24T17:19:53Z", "likes_count": 0}, {"comment_by": "Chainerlt", "comment_text": "Thank you!", "comment_date": "2017-11-23T22:36:50Z", "likes_count": 0}, {"comment_by": "Snakein Sun", "comment_text": "1. THANK YOU so much for things you are doing<br>2. When next video will come out describing backpropagation in details? Cant wait :]", "comment_date": "2017-11-23T07:36:53Z", "likes_count": 0}, {"comment_by": "Guilherme Ximenes", "comment_text": "Thank you. I really appreciate the way you transmit your knowledge. I&#39;m glad I found your channel. =)", "comment_date": "2017-11-22T20:33:01Z", "likes_count": 0}, {"comment_by": "SneakPeek", "comment_text": "Amazing Video! Thanks a lot and keep making this awesome videos:)", "comment_date": "2017-11-22T04:03:25Z", "likes_count": 0}, {"comment_by": "Vadim Omelchenko", "comment_text": "What is missing is the recurrent formula for the error function. This is the heart of backprop.", "comment_date": "2017-11-21T15:28:49Z", "likes_count": 0}, {"comment_by": "columbus8myhw", "comment_text": "How far along is the next video in this series?", "comment_date": "2017-11-21T12:07:12Z", "likes_count": 0}, {"comment_by": "Ben", "comment_text": "where the fuck where you last year in cal 3", "comment_date": "2017-11-21T04:01:33Z", "likes_count": 0}, {"comment_by": "Yongjun Lee", "comment_text": "I really enjoy all of your videos. (probably seen all of them lol). I always like to visualize the math concepts I&#39;m learning, and you do a phenomenal job at that. It would be amazing if you did a video on vector field, divergence,  curl, green, stokes&#39; theorem and make a connection with diagonalization. That would be a cool thing to visualize.", "comment_date": "2017-11-20T02:52:45Z", "likes_count": 0}, {"comment_by": "Brock Lynch", "comment_text": "Why is the derivative of (a(L) - y)^2 with respect to a(L)  not simply a(L), since y is a constant? I think I see it though, the y must not be affected by the square, so it just travels through.", "comment_date": "2017-11-19T23:31:55Z", "likes_count": 0}, {"comment_by": "Tyler Matthew Harris", "comment_text": "Hey Grant. If you inventing strong AI right now please don&#39;t kill or enslave us. We like you. Lol", "comment_date": "2017-11-19T19:05:30Z", "likes_count": 0}, {"comment_by": "Madhi Varman", "comment_text": "i don&#39;t know why 15 people disliked this video ... Is there any video channel explain the concept better than this channel ?  If so Suggest me i&#39;ll check them..", "comment_date": "2017-11-19T16:59:29Z", "likes_count": 0}, {"comment_by": "prince2phore", "comment_text": "you&#39;re a genius teacher! don&#39;t know what you use for the visuals but that&#39;s awesome and so well in sync with your already clear explanations, genius :)", "comment_date": "2017-11-18T16:56:07Z", "likes_count": 0}, {"comment_by": "Hoa Nguyen", "comment_text": "so exciting chanel ! thanks", "comment_date": "2017-11-18T12:11:39Z", "likes_count": 0}, {"comment_by": "Tyler Matthew Harris", "comment_text": "Next video this week?", "comment_date": "2017-11-17T19:05:23Z", "likes_count": 0}, {"comment_by": "Steeve Pierre-Louis", "comment_text": "The way you broke that down is actually ridiculously well done. You&#39;re ridiculous 3b1b!", "comment_date": "2017-11-17T08:01:47Z", "likes_count": 1}, {"comment_by": "Jan Hynek", "comment_text": "First time I became Patreon. Thanks a lot!", "comment_date": "2017-11-17T00:29:47Z", "likes_count": 0}, {"comment_by": "ademonicpeanut", "comment_text": "Hey 3B1B. Wouldn&#39;t the derivative of the ReLU at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m41s\">4:41</a> be non continuous?", "comment_date": "2017-11-16T19:27:04Z", "likes_count": 0}, {"comment_by": "Benjamin Frank", "comment_text": "Cette chaine est vraiment g\u00e9niale !", "comment_date": "2017-11-15T21:30:13Z", "likes_count": 0}, {"comment_by": "ganondorfchampin", "comment_text": "Rel or whatever it&#39;s called is a nondifferentiable function, so how does that work?", "comment_date": "2017-11-15T08:45:33Z", "likes_count": 0}, {"comment_by": "Simeon", "comment_text": "Keep making these videos!!! There&#39;s so awesome and helpfull :)", "comment_date": "2017-11-15T01:52:13Z", "likes_count": 0}, {"comment_by": "ParadoxCircuit", "comment_text": "Damn I was going so strong for the first three videos, but this one has me thinking I&#39;m going to have to rewatch it 3 or 4 times to actually grasp the significance.", "comment_date": "2017-11-15T01:41:54Z", "likes_count": 2}, {"comment_by": "Haifeng Yang", "comment_text": "When it comes to minimize a complicated multi-dimensional function, Monte Carlo is the first thing coming into my mind. Is it used in neural network? How do you avoid local minima?", "comment_date": "2017-11-14T18:03:43Z", "likes_count": 0}, {"comment_by": "Manuel Strondl", "comment_text": "I might swap from mechanical engineering to mathematics because of your videos... Not sure if I should thank or hate you for that :S", "comment_date": "2017-11-14T15:16:40Z", "likes_count": 0}, {"comment_by": "Ritwik Mishra", "comment_text": "One doubt<br>At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m27s\">9:27</a> shouldn&#39;t the last derivative (in yellow box top RHS) be<br>dC/daj(l) = ... OR Summation i=0 to N { 2*(aj(L) - yj) } where N is number of training examples<br>Maybe the narrator forgot to write subscript 0 in front of C", "comment_date": "2017-11-14T08:15:23Z", "likes_count": 0}, {"comment_by": "Klaes", "comment_text": "How have you not built an android yet?", "comment_date": "2017-11-14T07:31:55Z", "likes_count": 0}, {"comment_by": "Discusser", "comment_text": "wait what are you talking about again", "comment_date": "2017-11-13T18:56:29Z", "likes_count": 0}, {"comment_by": "Sapient Pearwood", "comment_text": "I know this is a math channel, and not a machine learning or numerical methods channel, but more of these kinds of videos would be awesome. It is hard to find content like this, where you build up a complex topic from scratch until it seems easily accessible and not at all scary.<br><br>+3Blue1Brown Could you do a series on numerical methods? Topics could include stuff like: numerical stencils (central difference, Crank-Nicholson, etc.), iterative methods (jacobi, multi-grid, etc.), krylov subspace methods (bi-conjugate gradient, etc.), maybe even eigen-function expansions and spectral methods. For me, these topics gave insights into how PDEs worked that I never got from studying the math by itself (e.g. disspersion vs. dissipation error, that kind of thing). I think it might work on this channel, and it might be interesting to do a series on how calculus is typically done in scientific computing. No pressure though. Just keep making videos, we&#39;ll all keep watching.", "comment_date": "2017-11-13T16:36:56Z", "likes_count": 0}, {"comment_by": "dmv321", "comment_text": "Why is the partial of C with respect to aj(l) the same as the summation with (l+1) layer at 9m31s? Hard to explain in words but not understanding why it&#39;s l+1 and not l-1.", "comment_date": "2017-11-13T07:35:35Z", "likes_count": 0}, {"comment_by": "Kcireh", "comment_text": "can you teach how use this your application? I wanna use these animation on my explanation on my collage", "comment_date": "2017-11-11T13:23:48Z", "likes_count": 0}, {"comment_by": "Arpit Dubey", "comment_text": "Your videos are without doubt THE best resource on the internet when it comes to building an intuitive understanding of neural networks. Thank you!", "comment_date": "2017-11-11T13:13:58Z", "likes_count": 1}, {"comment_by": "Defy", "comment_text": "That feeling when you understand completely without even pausing one time :) I would love to have you as my advanced analysis teacher, your explanations are better than great !", "comment_date": "2017-11-11T13:09:06Z", "likes_count": 9}, {"comment_by": "Young Nam", "comment_text": "Thanks 3B1B. I&#39;m understanding machine learning mush better, and following your video while note taking was the easiest method for learning.<br><br>I&#39;m a little confused about what the change in C_0 with respect to change in a_(L-1, k) for the k-th activation in L-1 layer (I just changed to this notation because I feel more comfortable writing like this in one line text). That&#39;s <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=8m40s\">8:40</a> part of the video I guess. It doesn&#39;t make intuitive sense for me as to why you need the summation of impact of a_(L-1, k) on a_(L, 0~n), say without any multiplier or something.<br><br>Trying to understand the meaning of `dC_0/da_(L-1, k)` I thought of a Neural Network where there are only two layers, input and output layer, and input layer having 1 neuron and output layer having 2 neurons.<br><br>Does it ever make sense for a_(L-1, k) to be an activation (or neuron?) in an input layer? If so, I think it makes to add the &#39;impact&#39; all up especially when the weights are all same &#39;direction&#39; or sign because if so summing them all up would result in greater number, and this would mean changing the input has the biggest impact in this scenario.<br><br>If not, I&#39;m still confused what `dC_0/da_(L-1, k)` is and why it has the summation.", "comment_date": "2017-11-11T12:50:46Z", "likes_count": 6}, {"comment_by": "chandramouli kamanchi", "comment_text": "These four videos on neural networks are fantastic, beautifully explained.", "comment_date": "2017-11-11T11:48:44Z", "likes_count": 1}, {"comment_by": "Alexandru Gheorghe", "comment_text": "Excellent lesson! Thank you very much.", "comment_date": "2017-11-11T11:45:49Z", "likes_count": 3}, {"comment_by": "Existenceisillusion", "comment_text": "I never heard that symbol called &quot;del&quot; before, I&#39;ve only heard it called &quot;partial&quot;. I always heard a different symbol called &quot;del&quot;, for example &quot;del cross E vector equals negative partial B vector by partial  <i>t</i> &quot;.", "comment_date": "2017-11-10T20:13:34Z", "likes_count": 0}, {"comment_by": "no name", "comment_text": "thank you so much u inspired me deeply for this field", "comment_date": "2017-11-10T12:44:25Z", "likes_count": 0}, {"comment_by": "Kaixo", "comment_text": "Is it weird that I do not understand anything if I only started with Neural Networks 5 days ago? I&#39;m trying to understand it, but it is so much information at once, I cannot keep track of everything...", "comment_date": "2017-11-10T08:02:15Z", "likes_count": 0}, {"comment_by": "Hyperion", "comment_text": "You used the mean squared error here but I&#39;ve typically seen the cross entropy loss used for classification tasks. Is it because you wanted to simplify the cost function or because it isn&#39;t really that much of a difference in terms of accuracy?", "comment_date": "2017-11-10T06:31:39Z", "likes_count": 0}, {"comment_by": "Bahaa Ezz", "comment_text": "Thanks a million .. it is the nicest explanation of BP I ever watched.", "comment_date": "2017-11-09T20:48:01Z", "likes_count": 1}, {"comment_by": "gina stubinski", "comment_text": "Thank you!! I love this channel. So simple and easy to understand.", "comment_date": "2017-11-09T10:46:46Z", "likes_count": 0}, {"comment_by": "Gran Luo", "comment_text": "This is really one of the greatest channel I&#39;ve ever seen. I&#39;ve learned learned neural network since the beginning of this year and I found a lot of video, including Andrew Ng and Udacity. They do have very great explanations on how to derive formulas, but this is my first time to see how to visualize a formula!!! That is fantastic! I am doing data visualization for quite a time and this is my first time to see formula visualization!! Still can&#39;t believe it, what an amazing channel!!", "comment_date": "2017-11-09T10:41:26Z", "likes_count": 1}, {"comment_by": "Ara Balaki", "comment_text": "<b>DOPE</b>", "comment_date": "2017-11-09T10:21:59Z", "likes_count": 0}, {"comment_by": "Joseph Ghantous", "comment_text": "Can&#39;t be explained any better, keep up the great work!", "comment_date": "2017-11-09T09:28:58Z", "likes_count": 1}, {"comment_by": "Ashish Rao", "comment_text": "So once we get dC/dW(L), am I right in thinking that dC/dW(L-1) = [dC/dZ(L)]*[dZ(L)/da(L-1)]*[da(L-1)/dZ(L-1)]*[dZ(L-1)/dW(L-1)] ?", "comment_date": "2017-11-09T04:48:36Z", "likes_count": 0}, {"comment_by": "Johnson Yuen", "comment_text": "You have a gift for teaching. Please become a professor. This is explained so much better than the hand-wavey math most professors use, with no explanation of the intuition", "comment_date": "2017-11-09T02:59:07Z", "likes_count": 1}, {"comment_by": "02vLxcZF", "comment_text": "Great video series! One question: In the second video of this series, you mentioned at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=16m20s\">16:20</a> that newer neural networks would be better at chunking images into parts that look intelligent to humans\u00a0(e.g. several curved parts for a zero, the top of a nine, etc.). I was somehow expecting you to cover this in the following videos but that does not seem to be the case. How are we from that type of neural network?", "comment_date": "2017-11-08T20:21:45Z", "likes_count": 0}, {"comment_by": "Bork", "comment_text": "You absolute legend", "comment_date": "2017-11-08T15:28:06Z", "likes_count": 2}, {"comment_by": "thfreakinacage", "comment_text": "My god! A basic machine learning video series that actually makes sense to completely beginners!<br>Subscribed, and waiting in great anticipation for the next one! :D", "comment_date": "2017-11-08T10:23:17Z", "likes_count": 44}, {"comment_by": "Brian Matthews", "comment_text": "The point where you addressed the concern that the example you were using was too simple, having only 1 edge, was spot on as you were leading me down this merry garden path. I appreciate how much you watch your own videos and predict where the watcher would mentally say, &quot;but what about...&quot;", "comment_date": "2017-11-08T09:13:54Z", "likes_count": 52}, {"comment_by": "Sam Golden", "comment_text": "I truly can&#39;t thank you enough for these series.", "comment_date": "2017-11-07T23:20:16Z", "likes_count": 1}, {"comment_by": "Nish", "comment_text": "Thank you very much...", "comment_date": "2017-11-07T08:14:18Z", "likes_count": 1}, {"comment_by": "BritishPrivateer", "comment_text": "Nice eye", "comment_date": "2017-11-07T06:59:33Z", "likes_count": 0}, {"comment_by": "Ruoyu Qian", "comment_text": "I finally get what part does partial derivative actually have in machine learning, thank you very much, please make more video about other popular ML algos, such as logistic regression, random forest, K-means, and SVM!!", "comment_date": "2017-11-07T05:30:29Z", "likes_count": 1}, {"comment_by": "Emad Gohari", "comment_text": "Thank you man! you are THE BEST!", "comment_date": "2017-11-07T04:24:58Z", "likes_count": 1}, {"comment_by": "Tobias Arvidsson", "comment_text": "Very good explanation and videos! I did some neural network back in computational physics course and I struggled to fully grasp it beyond just the math, but you explain it in such a good way.  Few years ago, but the memories come back and look even more clear when having seen your video.", "comment_date": "2017-11-07T00:42:10Z", "likes_count": 1}, {"comment_by": "thought thinking thought", "comment_text": "I really hope you see this, but you should do a video on chaos theory", "comment_date": "2017-11-06T22:38:24Z", "likes_count": 0}, {"comment_by": "Fran\u00e7ois Ozenne", "comment_text": "Great video as always :)<br>Do some of you know good videos on lebesgues integrales and distributions ?", "comment_date": "2017-11-06T22:17:32Z", "likes_count": 0}, {"comment_by": "Max Pure", "comment_text": "Please consider making a video about Partial differential equations, iv been through dozens of videos about it and still fail to clearly understand it. i know if there is someone who can make it clear, it is you.", "comment_date": "2017-11-06T19:09:22Z", "likes_count": 1}, {"comment_by": "Jay Soaring", "comment_text": "If there is an award for educational video series on advanced scientific matters. Please give this award to 3b1b. Love it!", "comment_date": "2017-11-06T18:14:26Z", "likes_count": 8}, {"comment_by": "Micah Sheller", "comment_text": "Beautiful work! Reminds me of my late father who was a math professor: he had the same gentle, happy style, and believed heartily in making math a safe place for everyone to learn and have fun. Gonna make me tear up :)", "comment_date": "2017-11-06T17:41:03Z", "likes_count": 4}, {"comment_by": "Sergey Antopolskiy", "comment_text": "this is sooooo useful. thank you so much for breaking it down so great. I feel like I have a full picture in my head now.", "comment_date": "2017-11-06T17:30:08Z", "likes_count": 3}, {"comment_by": "brixomatic", "comment_text": "With all the calculus and intuition laid out, I would be thankful for one detailed backprop walkthrough on a simple example network diagram, say 2 input neurons, 2 hidden and 2 output neurons: Start with all relu nodes, random weights and one desired output. Show the error for each output. Show how each individual weight change in the output layer is computed. Then do the backprop step. Show how each weight change in the hidden layer is computed. Then change the weights in all layers and compute the new output layer result with the same image. This would be an exampe that should be easily programmed, that everyone should be able to follow and if one programmed this, they could trace if the program was correct, by comparing if the values would come out equal to the example.<br>I&#39;ve been watching a lot of videos, but most of them do fail to really lay out a good and easy example that one can follow. Most of them are either too abstract, incomplete or deal with matrices that are not so quick to map to the actual graph in my mind. All those indices are hard to track. Showing how the numbers channge and sum in the graph is easier to grasp.", "comment_date": "2017-11-06T13:36:16Z", "likes_count": 0}, {"comment_by": "Jabrils", "comment_text": "youre a deity Grant", "comment_date": "2017-11-06T12:27:51Z", "likes_count": 753}, {"comment_by": "Ita Cirovic Donev", "comment_text": "Excellent explanation !!!", "comment_date": "2017-11-06T11:20:00Z", "likes_count": 0}, {"comment_by": "tk", "comment_text": "You are the epitome of the power of the internet as a tool for good. Thank you so much for your hard work!", "comment_date": "2017-11-06T11:17:29Z", "likes_count": 0}, {"comment_by": "bejoscha", "comment_text": "I love the \u201eratio\u201c of views and likes between Chapter 3 and appendix of Chapter 3. Reminds me of the old \u201efact\u201c for presentations where each formula supposedly halves the amount of people still listening...", "comment_date": "2017-11-06T10:41:06Z", "likes_count": 1}, {"comment_by": "Georg Hanslmaier", "comment_text": "Wow and i thought excel is the most difficult pc-thing...", "comment_date": "2017-11-06T06:30:00Z", "likes_count": 0}, {"comment_by": "Timur Sultanov", "comment_text": "Wasn&#39;t all that hard... But i did have a course of tensor analysis back in university... So this is much simpler than that...", "comment_date": "2017-11-06T01:06:26Z", "likes_count": 1}, {"comment_by": "hi", "comment_text": "So I see everyone here is a fan of the &quot;Rick and Morty&quot; television series. What are the happenings of your days fellow fans?", "comment_date": "2017-11-05T23:30:58Z", "likes_count": 0}, {"comment_by": "Ehsan.1.M", "comment_text": "Thank you so much, Grant. I finally learned back prop, and I have become a patron. I wish I could do more.", "comment_date": "2017-11-05T21:49:00Z", "likes_count": 6}, {"comment_by": "Nicolas Schmid", "comment_text": "Could you do a video with a concrete exemple of code in python that shoes what operations the computer has to do? I think it would be very interesting", "comment_date": "2017-11-05T19:46:00Z", "likes_count": 0}, {"comment_by": "Jeff Hotes", "comment_text": "I love your content. I just watched SethBling use this principle for his Mario Kart tensor flow demonstration. Keep it up, y&#39;all are great.", "comment_date": "2017-11-05T18:43:10Z", "likes_count": 1}, {"comment_by": "Henrique Rigitano", "comment_text": "is it posible to post a video on bayesian networks? please!!!", "comment_date": "2017-11-05T17:17:03Z", "likes_count": 1}, {"comment_by": "Reckless Roges", "comment_text": "W_jk_  <b>are you serious?</b>  ;-) (Internet slang has rotted my brain.)", "comment_date": "2017-11-05T16:35:36Z", "likes_count": 0}, {"comment_by": "Kim Svensson", "comment_text": "You videos have such informative and well explained content with an amazingly calm and including tone! I&#39;m a fan", "comment_date": "2017-11-05T13:42:12Z", "likes_count": 3}, {"comment_by": "Petch85", "comment_text": "love these videos:<br>How do one &quot;calculate&quot; hvor many layers and nodes one need for at given problem?<br>fx if i need to approximate y=x^2 for x between 0 and 100, I suspect that would be way easier than recognize a face in a 10 MP color photo. But have due I estimate the optimal size of my network?<br>Keep up the good work.", "comment_date": "2017-11-05T13:35:00Z", "likes_count": 0}, {"comment_by": "G\u00e1bor Gyebn\u00e1r", "comment_text": "Awesome material. :) Please make a video about convolutional neural networks, too.", "comment_date": "2017-11-05T13:13:55Z", "likes_count": 80}, {"comment_by": "Kartik chincholikar", "comment_text": "Amazing video.Christopher Olah&#39;s Blog has an equally good but easier to follow explanation of Computational <a href=\"http://graphs.do/\">graphs.Do</a> check it out.", "comment_date": "2017-11-05T12:50:52Z", "likes_count": 0}, {"comment_by": "thatdude_93", "comment_text": "could you do some videos on differential geometry? like what you did with you &quot;essence of...&quot; series", "comment_date": "2017-11-05T12:43:03Z", "likes_count": 1}, {"comment_by": "Masum Hasan", "comment_text": "How do you make the awesome visuals?", "comment_date": "2017-11-05T08:39:22Z", "likes_count": 0}, {"comment_by": "Mk Km", "comment_text": "My brain turned into a gas in the process of watching the video, but at least I understand it now.<br>                                     -Average comment below 3B1B video", "comment_date": "2017-11-05T07:46:06Z", "likes_count": 0}, {"comment_by": "erik shure", "comment_text": "Nice!", "comment_date": "2017-11-05T05:25:23Z", "likes_count": 0}, {"comment_by": "Matija Tecer", "comment_text": "Great video, great channel! It would be awesome if you could do videos on Riemann surfaces and spheres", "comment_date": "2017-11-05T03:49:24Z", "likes_count": 0}, {"comment_by": "Sanjib Narzary", "comment_text": "I liked all of your series", "comment_date": "2017-11-05T01:31:08Z", "likes_count": 0}, {"comment_by": "s0mthingz", "comment_text": "This makes so much sense to me now! Thanks a lot!", "comment_date": "2017-11-05T01:06:30Z", "likes_count": 0}, {"comment_by": "logan H", "comment_text": "What the fuck... super disappointed 10 min video... i want 25-30 mins at least... very low energy", "comment_date": "2017-11-05T00:06:11Z", "likes_count": 0}, {"comment_by": "Denis Khryashchev", "comment_text": "Why are we interested in calculating the derivatives of the cost functions with respect to the activations at L-1? In the gradient of C we only have derivatives with respect to the weights and biases and we cannot change the activations directly, right?. I am a bit confused.", "comment_date": "2017-11-04T23:18:07Z", "likes_count": 1}, {"comment_by": "Philippe Carphin", "comment_text": "This series is insanely good.  As a teacher, I feel like Salieri watching Mozart play and being like &quot;It&#39;s so beautiful, how is he so good!&quot;", "comment_date": "2017-11-04T21:25:17Z", "likes_count": 350}, {"comment_by": "30svich", "comment_text": "3Blue1Brown I dont get why is it partial derivative and not a total derivative, can you explain?", "comment_date": "2017-11-04T21:08:59Z", "likes_count": 0}, {"comment_by": "TiagoTiago", "comment_text": "When you mention another video in a video, it would probably be a good idea to add a link to the description and to the little &quot;i&quot; thing in the corner.", "comment_date": "2017-11-04T20:12:26Z", "likes_count": 0}, {"comment_by": "Julio Cesar", "comment_text": "Great, thanks. FInally everything lay down clearly.", "comment_date": "2017-11-04T19:18:36Z", "likes_count": 0}, {"comment_by": "Soham Shah", "comment_text": "hi, I might be wrong here but according to what i have learnt in the basics of calculus tells me that at <a href=\"https://youtu.be/tIeHLnjs5U8?t=323\">https://youtu.be/tIeHLnjs5U8?t=323</a> (time=<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=5m15s\">5:15</a>) it should be avg= 1/n-1(multiplied by the area under the curve).<br>tell me if i am wrong.<br>Also are ther emore episode planned?", "comment_date": "2017-11-04T17:29:57Z", "likes_count": 0}, {"comment_by": "Syam Ahmad", "comment_text": "but when is the Differential Equations comes out????????????????????????????????", "comment_date": "2017-11-04T17:13:48Z", "likes_count": 0}, {"comment_by": "Nicolas Schmid", "comment_text": "Please keep on doing videos about that topic, you\u2018ve realy opened my eyes on an amazing univer which is the neural network and deep learning", "comment_date": "2017-11-04T15:45:05Z", "likes_count": 2}, {"comment_by": "Lucas Kook", "comment_text": "Absolutely outstanding content! Thank you for your great work!", "comment_date": "2017-11-04T14:59:33Z", "likes_count": 0}, {"comment_by": "Raunak Dutta", "comment_text": "Could you make a video on Kalman filters? Most channels only tell what it does, you are the one who could actually bring out the Math without too much of notations. It would be awesome", "comment_date": "2017-11-04T14:00:49Z", "likes_count": 0}, {"comment_by": "Love Your Own Tribe", "comment_text": "Excellent video as usual.", "comment_date": "2017-11-04T13:59:18Z", "likes_count": 0}, {"comment_by": "Erioch", "comment_text": "Honestly, this is one of the best (If not the best) channel on Mathematics/Science education I have seen. Intuitive but not oversimplified. Thank you so much that for offering your spectacular work and you help so many people understand these concepts.", "comment_date": "2017-11-04T13:42:44Z", "likes_count": 4}, {"comment_by": "crimfan", "comment_text": "You know you&#39;re in index hell when you have to bust out the superscripts!", "comment_date": "2017-11-04T13:38:02Z", "likes_count": 0}, {"comment_by": "Natthaphong Phuntusil", "comment_text": "Best explanation ever!!", "comment_date": "2017-11-04T13:20:35Z", "likes_count": 1}, {"comment_by": "Yarflam", "comment_text": "Good video. I think you have lost a lot people with the formulas. I suppose the next video explain the update of the neural network. :D Do you know an other way than the backpropagation ?", "comment_date": "2017-11-04T13:00:33Z", "likes_count": 0}, {"comment_by": "MrShawnengineer", "comment_text": "Very clear presentations. Thank you.", "comment_date": "2017-11-04T12:48:18Z", "likes_count": 1}, {"comment_by": "Annemarie en Pieter Bresters - van Bragt", "comment_text": "this must have taken so much time, I was able to just understand it now (after using my whole evening yesterday)", "comment_date": "2017-11-04T11:19:24Z", "likes_count": 3}, {"comment_by": "Now842", "comment_text": "Very very good video. Thank you very much! With you I finally really understood NNs", "comment_date": "2017-11-04T10:25:23Z", "likes_count": 1}, {"comment_by": "Sean Demers", "comment_text": "What function is he saying at <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=2m00s\">2:00</a>? He says Sigmoid and then something that I don&#39;t know how to spell to look up", "comment_date": "2017-11-04T09:49:38Z", "likes_count": 4}, {"comment_by": "Plat Crab", "comment_text": "2 vids in one day hell yes", "comment_date": "2017-11-04T06:19:49Z", "likes_count": 1}, {"comment_by": "Manpreet Matharu", "comment_text": "this is freaking amazing .....", "comment_date": "2017-11-04T05:10:10Z", "likes_count": 1}, {"comment_by": "Eudaimonian", "comment_text": "Did you take Andrew Ng&#39;s course in machine learning while you were at Stanford?", "comment_date": "2017-11-04T04:52:55Z", "likes_count": 0}, {"comment_by": "J M", "comment_text": "I can&#39;t believe I subscribed to you 2 years ago!  (with a different account) And I&#39;ve never regretted it :D", "comment_date": "2017-11-04T04:39:26Z", "likes_count": 2}, {"comment_by": "Jamie Peters", "comment_text": "You truly have a gift for teaching in a clear and concise manor! Thanks for the great vid!", "comment_date": "2017-11-04T03:45:09Z", "likes_count": 2}, {"comment_by": "FAS Ligand", "comment_text": "How do do you deal with local minima? My only idea would be, when the gradient gets sufficiently small (one minimum is confidently found) to choose a weight at random and roughly (perhaps with bigger steps) check if it still converges to the same value. <br><br>Or maybe do that before the backpropagation process. Instead of choosing the first weight by random, make a rough estimation by &quot;sampling&quot; the loss function at different arguments and choosing the lowest weight.<br><br>Other than just luck, I don&#39;t see how local minima could be avoided", "comment_date": "2017-11-04T02:55:07Z", "likes_count": 0}, {"comment_by": "B Mac", "comment_text": "Does anyone know why we had to use a non-linear function (ie. sigmoid) for normalising the z layer?", "comment_date": "2017-11-04T02:55:02Z", "likes_count": 0}, {"comment_by": "FriedIcecreamIsAReality", "comment_text": "Awesome videos, man. I am a big fan. I also listen to the ben ben and blue podcasts.  <br><br>I am a math undergraduate and have some programming skills. I have been interested in artificial intelligence, but didn&#39;t know where to start. These videos is exactly what i needed, hopefully i can use neural networks for some simple projects of solving games. I like to make game solvers (sudoku, mastermind,...). <br><br>Thank you. This is not relevant, but I&#39;m from Colombia, just so you know you have great outreach.", "comment_date": "2017-11-04T02:32:55Z", "likes_count": 1}, {"comment_by": "Merthan E.", "comment_text": "<a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=1m30s\">1:30</a> why is the cost the difference squared?", "comment_date": "2017-11-04T02:28:35Z", "likes_count": 0}, {"comment_by": "Zenytram Searom", "comment_text": "WOW i thought that the math behind it would be waaayy more complicated. i know that this video has a lot information to digest but it&#39;s not complicated.", "comment_date": "2017-11-04T02:14:14Z", "likes_count": 4}, {"comment_by": "Sergio Korochinsky", "comment_text": "I just unsubscribed to this channel, so I can have the pleasure of subscribing again.", "comment_date": "2017-11-04T02:03:59Z", "likes_count": 263}, {"comment_by": "Spam Email", "comment_text": "Do a series on trigonometry", "comment_date": "2017-11-04T01:00:01Z", "likes_count": 1}, {"comment_by": "Tommy Butler", "comment_text": "More!!!", "comment_date": "2017-11-04T00:41:06Z", "likes_count": 1}, {"comment_by": "Neuro Morphing", "comment_text": "This guy can really teach 10^2 better than all those NN-gurus all together ---&gt; Fantastic job !", "comment_date": "2017-11-04T00:30:19Z", "likes_count": 0}, {"comment_by": "Oscar Friberg", "comment_text": "This appendix video feels more intuitive than the main video. I&#39;m strange.", "comment_date": "2017-11-04T00:27:30Z", "likes_count": 0}, {"comment_by": "Patrick Hodson", "comment_text": "\ud83e\udd43 I may or may not be sober while watching this, hehehe", "comment_date": "2017-11-04T00:22:57Z", "likes_count": 0}, {"comment_by": "SgtRevan", "comment_text": "Why does the bias have a separate gradient equation?  The way I&#39;m learning it currently is to include the bias as a row of weights in each layer, and when doing the dot product it automatically sums those biases into the output (this also means each training example needs a 1 appended to it).  But when we backprop, we just use the entire data vector/matrix, thus sending the bias backwards with the same equation as the weights...hopefully that makes sense?  Do they yield the same result?", "comment_date": "2017-11-03T23:54:09Z", "likes_count": 0}, {"comment_by": "Martijn Riemers", "comment_text": "Can you shed some light on why certain activation functions are better than others and why the difference between the desired output and the current output is always squared? Wouldn\u2019t cubing put more of a penalty on larger errors than squaring making the network learn faster but potentially generalize less?", "comment_date": "2017-11-03T23:47:27Z", "likes_count": 0}, {"comment_by": "Ian Beach", "comment_text": "I mean I haven&#39;t enjoyed my calc 3 course, but of all my math courses it has probably opened my eyes the most to things I didn&#39;t understand before. If I had watched this series a year ago I might have gotten some of it, but now I can correlate examples I learned in class to examples here. Optimization by the path of gradient descent? The chain rule you  used for partials? Those were all examples I learned in class. Even if I didn&#39;t understand these things though, I&#39;m sure I could figure it out though. This channel is amazing, and I look forward to seeing what you cover in the future.", "comment_date": "2017-11-03T23:40:38Z", "likes_count": 0}, {"comment_by": "Singularity", "comment_text": "Brain.exe has failed successfully.", "comment_date": "2017-11-03T23:39:32Z", "likes_count": 0}, {"comment_by": "kang Chih Lun", "comment_text": "This is the best and clearest explanation in all BP course I could find !  \u6c92\u6709\u4e4b\u4e00\uff01", "comment_date": "2017-11-03T23:19:42Z", "likes_count": 300}, {"comment_by": "Aurora Long", "comment_text": "One of the 0s isn&#39;t subscripted", "comment_date": "2017-11-03T23:03:48Z", "likes_count": 0}, {"comment_by": "Nitin", "comment_text": "Netflix can show these rather than some other questionable material.", "comment_date": "2017-11-03T22:54:27Z", "likes_count": 255}, {"comment_by": "Peter Oliphant", "comment_text": "I seriously have a hard time explaining how much I appreciate this video. I am far and away a symbolic thinker, as opposed to a geometric one, and while I love all of your videos and how intuitive you make the concepts, it&#39;s sometimes hard for me to think about the geometry. I am much more comfortable working with symbols and that&#39;s why I treasure videos like this. Thank you :)", "comment_date": "2017-11-03T22:52:16Z", "likes_count": 7}, {"comment_by": "Abhishek Kumar", "comment_text": "I always feel, if u have a mentor who can break complex things into simple stuff so beautifully, even a dumb guy can grasp the concept. Keep doing the good stuff. Admirer++", "comment_date": "2017-11-03T22:21:46Z", "likes_count": 8}, {"comment_by": "Florent Raeymaeckers", "comment_text": "I&#39;m surprised i understood o_o", "comment_date": "2017-11-03T22:06:31Z", "likes_count": 0}, {"comment_by": "empmachine", "comment_text": "i don&#39;t know how you keep raising the bar. this is perfect, thank you!", "comment_date": "2017-11-03T21:45:36Z", "likes_count": 1}, {"comment_by": "Erik Mingjun Ma", "comment_text": "Do you have any reccomendations on good litterature regarding this subject beyond the scope which you are able to cover in these videos?", "comment_date": "2017-11-03T21:36:47Z", "likes_count": 0}, {"comment_by": "joshinils", "comment_text": "And how would one actually use this? I dont understand how this would be applied to an actual network, how one would compute how to change the weights and biases for a given example", "comment_date": "2017-11-03T21:35:01Z", "likes_count": 0}, {"comment_by": "Vladyslav Korenyak", "comment_text": "Wow, I&#39;m really surprised by the simple idea behind such a complex thing. I heard that machine learning it&#39;s very heavy in math, but now I think it&#39;s more heavy in actually programming the math and don&#39;t get lost. <br><br>I wonder if our brain does something similar. A baby has all this data that comes through the senses and a weight function related to your hormones level, which respond to your survival and well being. And I just get lost in my own idea, too much variables :(", "comment_date": "2017-11-03T21:07:25Z", "likes_count": 1}, {"comment_by": "J\u00e9r\u00f4me Bruzaud", "comment_text": "Awesome video as always. I&#39;m actually a bit sad it marks the end of your video series on deep learning :(. Give us more ! :D", "comment_date": "2017-11-03T20:53:28Z", "likes_count": 3}, {"comment_by": "Alecto Perez", "comment_text": "3Blue1Brown, I know there are some mean comments, but the number of people who liked your video is literally more than 100x the portion that disliked it. That ratio is unheard of for practically any other youtuber. Be proud of yourself!", "comment_date": "2017-11-03T20:06:26Z", "likes_count": 1}, {"comment_by": "Merijn Vogel", "comment_text": "After seeing a few pieces of books, descriptions on the internet about back propagation, with this video I finally reached some kind of enlightenment (especially at about <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=4m50s\">4:50</a> into this video). Thank you so much for that!<br><br>Just as a hobby, I was trying to implement a neural network from scratch in java: plain objects for neurons, connections and layers.  I wanted to really visualize how a neural network WORKS. (Visualize either as computer code, but maybe I even want to create some visual for it...) This will certainly help me on my way!", "comment_date": "2017-11-03T20:04:03Z", "likes_count": 3}, {"comment_by": "dampfmaschine2000", "comment_text": "Well, this is just a brilliant video, it&#39;s hard to teach this stuff any clearer I think!<br><br>However, there is one thing that I didn&#39;t really understand.<br>At <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=9m06s\">9:06</a>, when you change the activations in the layer L-1, why didn&#39;t this change the activations in the layer L? You cannot get a gradient of the cost-function when the<br>activation in the last layer doesn&#39;t change, can you?", "comment_date": "2017-11-03T19:48:10Z", "likes_count": 0}, {"comment_by": "Jan Negrey", "comment_text": "I&#39;m going to be frank. I love your videos, the&#39;re simple and understandable. But with Neural Network I feel like a moron. Still, thank you for those videos, and I hope that in a couple of weeks there will be another one.", "comment_date": "2017-11-03T19:41:39Z", "likes_count": 2}, {"comment_by": "Daniel Astillero", "comment_text": "ReLU is linear", "comment_date": "2017-11-03T18:49:10Z", "likes_count": 0}, {"comment_by": "iau", "comment_text": "Is it wrong to use a programming-like notation for vectors and matrices (a[L, n], or for example a[3, 5] for the activation of neuron 5 of layer 3) inside mathematical notation, instead of using subscripts and superscripts?<br><br>I ask because I usually get confused thinking superscript indices are exponents and subscript indices are variations or alternatives, instead of the next one in a sequence.<br><br>I ask because I find the notation x[t] as the t-th element of sequence x much more intuitive than x_t.", "comment_date": "2017-11-03T18:28:07Z", "likes_count": 37}, {"comment_by": "staffehn", "comment_text": "Maybe that\u2019s also because I\u2019m a maths student, but honestly I don\u2019t remember much anymore from the video part one, because the matter presented WITH the formulas makes understanding and intuition soo much easier as opposed to when you just try to grasp a summary/overview of the resulting necessary steps of computation without any argument behind them. The \u201cintuition\u201d video just showed so many plainly arbitrary details that there was no way to really remember anything.", "comment_date": "2017-11-03T18:17:47Z", "likes_count": 0}, {"comment_by": "hiqwertyhi", "comment_text": "It&#39;s not that no-one else makes top-notch math/cs videos, it&#39;s that this guy makes it CLICK.", "comment_date": "2017-11-03T18:10:56Z", "likes_count": 845}, {"comment_by": "Chris Wilson", "comment_text": "I cannot tell you how much I appreciate these videos. I don&#39;t have a strong math background (english undergrad) but I&#39;m teaching myself data science. It REALLY helps to have the equations explained rather than just presented and to tie the components of the equation back to intuitions. Thank you thank you thank you.", "comment_date": "2017-11-03T18:07:43Z", "likes_count": 69}, {"comment_by": "\u6c88\u715c\u8a73", "comment_text": "This is very informative. Thank you for doing this!<br><br><a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=3m42s\">3:42</a><br><br>The \u03b4C0 on top should be \u03b4C&lt;subscript&gt;0&lt;/subscript&gt;", "comment_date": "2017-11-03T18:02:07Z", "likes_count": 0}, {"comment_by": "flabort", "comment_text": "So this covers a Neural Network with a multiple choice output, but it only outputs one result, and requires an input. What are the differences between this and a &quot;Recursive Neural Network&quot; like @RoboRosewater on twitter, which outputs text strings based on its training data, but with no input to get an output? Like, afaik, the only thing @RoboRosewater looks at when determining what character to print onto it&#39;s text string is the character(s) it outputted last.", "comment_date": "2017-11-03T17:43:11Z", "likes_count": 0}, {"comment_by": "Abstruct", "comment_text": "This stuff is an amazing supplication to Andrew Ng&#39;s courses, it gives a lot more intuition and visual understandings of the formulas.", "comment_date": "2017-11-03T17:37:14Z", "likes_count": 52}, {"comment_by": "WarmWeatherGuy", "comment_text": "Will there be a video about using a genetic algorithm to find the values of all the weights etc.?", "comment_date": "2017-11-03T17:36:43Z", "likes_count": 0}, {"comment_by": "Rahul", "comment_text": "Nothing&#39;s better than 3B1B releasing two videos at once. Yay!", "comment_date": "2017-11-03T17:08:33Z", "likes_count": 2}, {"comment_by": "Owen de Heer", "comment_text": "i read the book a while ago and it was difficult to understand the fundamentals at first, but now you&#39;ve made it so much more intuitive! i did a school project where i wanted to use gradient descent to make a program learn to solve the rubik&#39;s cube but i stopped working on it but after watching this i revisited it and found a mistake i made and now i will try to finally finish it!", "comment_date": "2017-11-03T17:02:09Z", "likes_count": 1}, {"comment_by": "LordDecapo", "comment_text": "This is bugging me, 1st part of this is &quot;chapter 3&quot; and this is &quot;appendix to PART 3&quot;.. any way to change this to  &quot;appendix, of chapter 3&quot;?<br>Content is great, even better then the patreon prerelease :)", "comment_date": "2017-11-03T16:37:54Z", "likes_count": 0}, {"comment_by": "Boris Milenski", "comment_text": "I&#39;m so glad I took an extra course in Math  which gave me the basics of calculus and pointed me to this channel. IT MAKES SENSE!", "comment_date": "2017-11-03T16:27:54Z", "likes_count": 2}, {"comment_by": "Suharsh Tyagi", "comment_text": "This is the longest 10 minute video I have ever watched. Literally took me half an hour, but the feeling of the idea behind this completely settling in , makes it totally worth it!", "comment_date": "2017-11-03T16:23:25Z", "likes_count": 347}, {"comment_by": "Merthan E.", "comment_text": "I don&#39;t understand anything but I still like this.", "comment_date": "2017-11-03T16:19:46Z", "likes_count": 0}, {"comment_by": "Aman Deep", "comment_text": "Awesome video...please do a series on vector calculus...vector calculus is a very foggy concept and also it will provide good material for your animations...once i&#39;ll get job...i&#39;ll definitely support your channel", "comment_date": "2017-11-03T16:18:51Z", "likes_count": 0}, {"comment_by": "Alex Larionov", "comment_text": "What an art of math, Jesus Christ", "comment_date": "2017-11-03T16:13:19Z", "likes_count": 82}, {"comment_by": "Jordan Wheeler", "comment_text": "I&#39;m in my second year of my Maths  degree and understanding this video has given me the final push to go out and start researching AI more deeply. it&#39;s been a long interest of mine and over the years I&#39;ve been thinking a lot about it and had some abstract thoughts/ideas that I can now look into Mathematically! You sir have inspired me more than any Mathematician. Don&#39;t EVER stop what you&#39;re doing here!", "comment_date": "2017-11-03T16:10:00Z", "likes_count": 1}, {"comment_by": "Tyler Matthew Harris", "comment_text": "Could we use absolute value instead of squares?", "comment_date": "2017-11-03T16:01:26Z", "likes_count": 0}, {"comment_by": "David Edward", "comment_text": "Early on, you said something about this being different from how the chain rule is usually presented in calculus texts. Howso? I don&#39;t understand.", "comment_date": "2017-11-03T15:58:28Z", "likes_count": 0}, {"comment_by": "ma271", "comment_text": "Thank you! Machine learning is so interesting", "comment_date": "2017-11-03T15:41:07Z", "likes_count": 1}, {"comment_by": "Aravind Kannan", "comment_text": "This is by far the best video I have ever seen in Neural Networks. Thanks for this! :)", "comment_date": "2017-11-03T15:39:27Z", "likes_count": 26}, {"comment_by": "Antonio Bernardo", "comment_text": "this is easily the best channel in youtube today! once I get a job i will more than glad to support you!", "comment_date": "2017-11-03T15:31:44Z", "likes_count": 529}, {"comment_by": "Sebastiaan Craens", "comment_text": "As an hobbyist programmer, i can&#39;t thank you enough! Once i finish my studies i will donate to you. :)", "comment_date": "2017-11-03T15:23:57Z", "likes_count": 6}, {"comment_by": "notbobbobby", "comment_text": "Right now, I am so thankful for having taken vector calculus and several numerical approximation courses. This was an AWESOME video to watch. Thanks! :)", "comment_date": "2017-11-03T15:23:20Z", "likes_count": 8}, {"comment_by": "Vadim Borisov", "comment_text": "thank you for the appendix!!!!!!!!!", "comment_date": "2017-11-03T15:13:48Z", "likes_count": 3}, {"comment_by": "Poseidon", "comment_text": "This was such a good explanation!! Finally I understand this!", "comment_date": "2017-11-03T15:06:08Z", "likes_count": 0}, {"comment_by": "Lag Duck", "comment_text": "*looking at thumbnail<br>oh sh~, Im never going to understand that complex stuff. probably should watch it anyway<br>*ten minutes later<br>whoa! that&#39;s actually quite clear now!", "comment_date": "2017-11-03T14:58:56Z", "likes_count": 194}, {"comment_by": "Saptarshi Mitra", "comment_text": "Amazing man..... I say 3gold1platinum", "comment_date": "2017-11-03T14:54:25Z", "likes_count": 625}, {"comment_by": "poundcake2000", "comment_text": "Grant, I&#39;ve been thinking about these videos and I have a couple questions.... last time you fed a nonsense, scrambled image through the network, and it confidently identified it as a 5.  My question is two fold:<br><br>1) Would any value be added to the network by adding a final neuron that indicates &quot;no number&quot; or &quot;not anything identifiable&quot; or &quot;throw this image in the trash can&quot;?  Maybe that way the network would sort those trash-type images into a pile for, perhaps, a human to look at.<br><br>2) Is there any benefit to training your network to get the right answer, but be less confident about it?  You keep mentioning that we want the network to identify a &quot;2&quot; as a &quot;2&quot; and the goal is a final neuron to light up the &quot;2&quot; at 100% (and everything else at 0%).  Does this extreme goal perhaps cause really confident misclassifications sometimes (besides the junk image from last video)?  Would it be in any way beneficial to train the network to see a &quot;2&quot;, but perhaps to only be 90% confident about it?  Maybe it would <b>think</b> it saw a 2, but it&#39;s not totally sure - like a young child learning numbers.  (but, being 90% a 2, is still a 2.)  Would this help at all with the junk images from the previous question / video?  Also, maybe since we train the network to not be so confident all the time, could we then put any image that falls below, say, 75% confidence into another bin for humans to verify?<br><br>Also, on BB&amp;B you mentioned that it can sometimes feel like too much pressure when people say &quot;yeah, good video, I can&#39;t wait for the next one!&quot;  Well, I&#39;ll instead say, thanks for this series, and I&#39;ll patiently await the next video as you take your time to make it a high quality production like you always do - whenever that happens, I&#39;m good.  No pressure!", "comment_date": "2017-11-03T14:51:52Z", "likes_count": 107}, {"comment_by": "RapsyJigo", "comment_text": "Why do we take the derivative of those functions?", "comment_date": "2017-11-03T14:50:48Z", "likes_count": 0}, {"comment_by": "13thxenos", "comment_text": "Nicely done video. <br>I knew I learned backpropagation before, but it was hard, and I didn&#39;t use it manually ( I used frameworks like TensorFlow which uses computational graphs and backpropagate automatically) so I&#39;ve forgotten how it actually worked.<br>But this video is a great resource for newcomers to ANNs and people like me that have forgotten the theory behind it all. Thank you.", "comment_date": "2017-11-03T14:45:03Z", "likes_count": 14}, {"comment_by": "Loto974", "comment_text": "Please be my new maths teacher.", "comment_date": "2017-11-03T14:45:03Z", "likes_count": 0}, {"comment_by": "Shian", "comment_text": "How many chapters will there be in total?", "comment_date": "2017-11-03T14:31:00Z", "likes_count": 5}, {"comment_by": "Starry Starry", "comment_text": "I&#39;m understanding literally nothing, why am I watching this", "comment_date": "2017-11-03T14:27:59Z", "likes_count": 119}, {"comment_by": "Patel Nirmal", "comment_text": "Awesome channel", "comment_date": "2017-11-03T14:21:06Z", "likes_count": 290}, {"comment_by": "Jake Roosenbloom", "comment_text": "Please properly Capitalize the Title", "comment_date": "2017-11-03T14:14:41Z", "likes_count": 1}, {"comment_by": "MrFujinko", "comment_text": "5", "comment_date": "2017-11-03T14:13:16Z", "likes_count": 0}, {"comment_by": "noneuclidean", "comment_text": "-e^i\u03c0 th", "comment_date": "2017-11-03T14:12:54Z", "likes_count": 39}, {"comment_by": "Delta Beta", "comment_text": "<a href=\"https://m.youtube.com/watch?v=lebz80SnL3I\">https://m.youtube.com/watch?v=lebz80SnL3I</a>", "comment_date": "2017-11-03T14:11:06Z", "likes_count": 0}, {"comment_by": "Want - Diverse Content", "comment_text": "1", "comment_date": "2017-11-03T14:08:58Z", "likes_count": 0}, {"comment_by": "2000tmaster", "comment_text": "first", "comment_date": "2017-11-03T14:08:53Z", "likes_count": 2}]